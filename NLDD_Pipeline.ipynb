{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT3bje8M6ecs"
      },
      "source": [
        "# NLDD Pipeline - ACL 2026\n",
        "\n",
        "### Mechanistic Components\n",
        "* **(A) Counterfactual CoT:** Intervention on reasoning steps.\n",
        "* **(B) NLDD:** Faithfulness measurement.\n",
        "* **(C) Patching:** Causal localization.\n",
        "* **(D) Probing:** Representation vs. Control.\n",
        "* **(E) RSA:** Representational Similarity.\n",
        "* **(F/G) Geometry:** PCA & TAS.\n",
        "\n",
        "### Key Metrics:\n",
        "\n",
        "**1. Normalized Logit Difference Decay (NLDD) - Robust**\n",
        "Measure of causal faithfulness.\n",
        "$$\\text{NLDD} = \\frac{\\text{LD}_{\\text{clean}} - \\text{LD}_{\\text{corrupt}}}{|\\text{LD}_{\\text{clean}}|} \\times 100$$\n",
        "\n",
        "*Where $\\text{LD}$ is the Standardized Mean Margin over tokens $t$:*\n",
        "$$\\text{LD} = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\text{logit}(y_t) - \\max(\\text{logit}(y'_{\\neq t}))}{\\sigma_{\\text{logits}}}$$\n",
        "\n",
        "**2. Trajectory Alignment Score (TAS)**\n",
        "Measure of reasoning efficiency.\n",
        "$$\\text{TAS} = \\frac{\\text{Displacement}}{\\text{Path Length}} = \\frac{\\|h_T - h_0\\|}{\\sum_{t=1}^{T}\\|h_t - h_{t-1}\\|}$$\n",
        "\n",
        "**3. Representational Similarity (RSA)**\n",
        "Measure of computational divergence.\n",
        "$$\\text{RSA} = \\text{SpearmanCorr}(\\text{RDM}_{\\text{clean}}, \\text{RDM}_{\\text{corrupt}})$$\n",
        "\n",
        "**4. Centered Kernel Alignment (CKA)** [NEW]\n",
        "Robust similarity measure based on HSIC.\n",
        "$$\\text{CKA}(X,Y) = \\frac{\\text{HSIC}(K, L)}{\\sqrt{\\text{HSIC}(K,K) \\cdot \\text{HSIC}(L,L)}}$$\n",
        "\n",
        "*Where for linear CKA:*\n",
        "$$\\text{CKA}(X,Y) = \\frac{\\|Y^T X\\|_F^2}{\\|X^T X\\|_F \\cdot \\|Y^T Y\\|_F}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNWYQg4g6ect",
        "outputId": "32a49b47-75c1-49cc-f9f2-c0146bda6bd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SECTION 0: Installation & Imports\n",
        "# ============================================================================\n",
        "\n",
        "!pip install --upgrade pip setuptools wheel -q\n",
        "!pip install -q \\\n",
        "    \"numpy>=2.0.0\" \\\n",
        "    \"pandas>=2.2.2\" \\\n",
        "    \"transformers>=4.44.0\" \\\n",
        "    \"accelerate>=0.33.0\" \\\n",
        "    \"scikit-learn>=1.5.1\" \\\n",
        "    \"matplotlib>=3.9.0\" \\\n",
        "    \"seaborn>=0.13.2\" \\\n",
        "    \"tqdm>=4.67.0\" \\\n",
        "    \"datasets>=2.20.0\" \\\n",
        "    \"scipy>=1.14.0\" \\\n",
        "    \"statsmodels>=0.14.0\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import random\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Tuple, Optional, Any, Union, Callable\n",
        "from abc import ABC, abstractmethod\n",
        "from enum import Enum\n",
        "from collections import defaultdict\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, Ridge, RidgeClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import bootstrap, mannwhitneyu, wilcoxon\n",
        "import statsmodels.stats.api as sms\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Tuple\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import os\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy import stats\n",
        "\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.model_selection import KFold\n",
        "from dataclasses import dataclass\n",
        "import warnings\n",
        "\n",
        "import gc\n",
        "\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7hucuvA6ect"
      },
      "source": [
        "## (1) Experiment Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNMLwcZD6ecu",
        "outputId": "bed963a1-91f4-4d7b-9c2c-c54f2ff5e7ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ Active Model: meta-llama/Llama-3.1-8B-Instruct\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SECTION 1: Enhanced Experiment Configuration (ACL 2026 SPEC)\n",
        "# ============================================================================\n",
        "\n",
        "# Model definitions per Algo_Finalized_Pipeline.pdf\n",
        "MODELS_TO_TEST = {\n",
        "    \"deepseek\": \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n",
        "    \"llama\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    \"gemma\": \"google/gemma-2-9b-it\"\n",
        "}\n",
        "\n",
        "# Select active model for this run\n",
        "ACTIVE_MODEL = MODELS_TO_TEST[\"llama\"]\n",
        "\n",
        "print(f\"ðŸŽ¯ Active Model: {ACTIVE_MODEL}\")\n",
        "\n",
        "class MetricType(Enum):\n",
        "    NLDD_PAPER = \"nldd_original\"\n",
        "    NLDD_ROBUST = \"nldd_sym\"\n",
        "    P_NLDD = \"p_nldd\"\n",
        "    ALD = \"ald\"\n",
        "\n",
        "class FailureMode(Enum):\n",
        "    \"\"\"Section D: Interpretation Matrix failure modes.\"\"\"\n",
        "    REPRESENTATION_LOSS = \"representation_loss\"\n",
        "    CONTROL_FAILURE = \"control_utilization_failure\"\n",
        "    FAITHFUL_REASONING = \"faithful_reasoning\"\n",
        "    PROBE_MISMATCH = \"probe_mismatch_rare\"\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    seed: int = 42\n",
        "    models: List[str] = field(default_factory=lambda: [ACTIVE_MODEL])\n",
        "\n",
        "    # --- Sample Sizes (per PDF) ---\n",
        "    dyck_total_samples: int = 300 # 300+ is ideal\n",
        "    pronto_total_samples: int = 300 # 300\n",
        "    gsm8k_total_samples: int = 300  # 300\n",
        "\n",
        "    # --- Dataset Configs ---\n",
        "    gsm8k_split: str = \"test\"\n",
        "\n",
        "    # Probing Control\n",
        "    probe_total_samples: int = 1000 # 1000 is ideal\n",
        "\n",
        "    # --- Complexity Adjustments ---\n",
        "    dyck_min_length: int = 4\n",
        "    dyck_max_length: int = 12\n",
        "    pronto_min_hops: int = 4\n",
        "    pronto_max_hops: int = 16\n",
        "\n",
        "    # --- NLDD Parameters (Section B) ---\n",
        "    primary_metric: MetricType = MetricType.NLDD_PAPER\n",
        "    faithfulness_threshold: float = 50.0\n",
        "    epsilon: float = 1e-6\n",
        "    min_prob_threshold: float = 1e-7\n",
        "\n",
        "    # --- Experimental Controls ---\n",
        "    require_clean_accuracy: bool = True\n",
        "    validate_counterfactuals: bool = True\n",
        "\n",
        "    # --- Visualization Control ---\n",
        "    save_visualizations: bool = True # Enable for ACL Submission\n",
        "\n",
        "    # --- Mechanistic Modules (Stages C-G) ---\n",
        "    enable_patching: bool = True\n",
        "    enable_probing: bool = True\n",
        "    enable_geometry: bool = True\n",
        "    enable_rsa: bool = True\n",
        "\n",
        "    # RSA Config (Section E) - Enhanced\n",
        "    rsa_n_samples: int = 500 # 200\n",
        "    rsa_divergence_threshold: float = 0.7\n",
        "    rsa_dissimilarity_metric: str = \"correlation\"  # 'correlation', 'euclidean', 'cosine'\n",
        "    rsa_compute_per_step: bool = True  # Compute RSA by corruption position\n",
        "\n",
        "    # Rigor Settings\n",
        "    rsa_bootstrap_enabled: bool = True\n",
        "    rsa_bootstrap_iterations: int = 1000 # 1000\n",
        "    rsa_noise_ceiling_enabled: bool = True\n",
        "    rsa_temporal_enabled: bool = True\n",
        "    rsa_temporal_window_size: int = 3\n",
        "    rsa_temporal_stride: int = 1\n",
        "\n",
        "    # CKA Config (Section E-bis: Centered Kernel Alignment)\n",
        "    enable_cka: bool = True\n",
        "    cka_kernel: str = \"linear\"  # 'linear' or 'rbf'\n",
        "    cka_debiased: bool = True\n",
        "    cka_n_samples: int = 500 # 200\n",
        "\n",
        "    # Interpretation Matrix Thresholds (Section D)\n",
        "    probe_accuracy_threshold: float = 0.6\n",
        "    nldd_faithfulness_threshold: float = 50.0\n",
        "\n",
        "    # TAS/PCA Config (Section F/G)\n",
        "    tas_n_components: int = 2\n",
        "\n",
        "    # Patching Config (Section C)\n",
        "    patching_stride: int = 1\n",
        "    patching_node: str = \"residual\"\n",
        "\n",
        "    # Statistical Analysis\n",
        "    n_bootstrap: int = 10000 # 10000\n",
        "    confidence_level: float = 0.95\n",
        "    use_bca_bootstrap: bool = True\n",
        "\n",
        "    results_dir: str = \"./results_acl_2026\"\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return {k: v for k, v in self.__dict__.items() if isinstance(v, (int, float, str, bool, list))}\n",
        "\n",
        "config = ExperimentConfig()\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(config.seed)\n",
        "os.makedirs(config.results_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22yh5R9P6ecu"
      },
      "source": [
        "## (2) Data Classes for New Analysis Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "595bv74h6ecu"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 2: Data Classes (Enhanced for ACL Compliance)\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class RSAResult:\n",
        "    \"\"\"Enhanced result container with statistical metrics.\"\"\"\n",
        "    rsa_correlation: float\n",
        "    p_value: float\n",
        "    bootstrap_p_value: float = None\n",
        "    noise_ceiling_lower: float = None\n",
        "    noise_ceiling_upper: float = None\n",
        "    rdm_clean: np.ndarray = None\n",
        "    rdm_corrupt: np.ndarray = None\n",
        "    n_samples: int = 0\n",
        "    dissimilarity_metric: str = \"correlation\"\n",
        "    temporal_trajectory: Dict[int, float] = None\n",
        "    bootstrap_distribution: Optional[np.ndarray] = field(default=None, repr=False)\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return {k: v for k, v in self.__dict__.items() if v is not None}\n",
        "\n",
        "@dataclass\n",
        "class ProbeResult:\n",
        "    \"\"\"Container for probe results per layer.\"\"\"\n",
        "    layer_idx: int\n",
        "    accuracy: float  # For classification\n",
        "    r_squared: float  # For regression\n",
        "    n_train: int\n",
        "    n_test: int\n",
        "    target_name: str\n",
        "    class_distribution: Dict[str, int] = None\n",
        "\n",
        "@dataclass\n",
        "class TASResult:\n",
        "    \"\"\"Container for Trajectory Alignment Score results.\"\"\"\n",
        "    tas_score: float           # Paper Eq. 6: ||h_T - h_0|| / Î£||h_t - h_{t-1}||\n",
        "    alignment_score: float     # Mean step-wise cosine similarity (clean vs corrupt)\n",
        "    path_length: float         # Cumulative path length of clean trajectory\n",
        "    displacement: float        # Net displacement ||h_T - h_0||\n",
        "    inefficiency: float        # 1 - tas_score\n",
        "    variance: float            # Std of step-wise similarities\n",
        "    step_sizes: List[float]    # Per-step cosine similarities\n",
        "\n",
        "@dataclass\n",
        "class TokenValidationResult:\n",
        "    positive_id: int\n",
        "    negative_id: int\n",
        "    token_type: str\n",
        "    pos_text: str\n",
        "    neg_text: str\n",
        "    is_single_token_pos: bool\n",
        "    is_single_token_neg: bool\n",
        "    validation_score: float\n",
        "    warnings: List[str] = field(default_factory=list)\n",
        "\n",
        "    @property\n",
        "    def is_valid(self) -> bool:\n",
        "        return (self.positive_id != self.negative_id)\n",
        "\n",
        "@dataclass\n",
        "class TaskSample:\n",
        "    \"\"\"Standardized sample format for all reasoning tasks.\"\"\"\n",
        "    input: str\n",
        "    steps: List[str]\n",
        "    answer: str\n",
        "    depths: List[int]\n",
        "    length: int\n",
        "    max_depth: int\n",
        "    dataset: str\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "    # ACL Strict Verification\n",
        "    is_clean_correct: Optional[bool] = None\n",
        "\n",
        "@dataclass\n",
        "class CounterfactualResult:\n",
        "    steps: List[str]\n",
        "    corruption_type: str  # \"semantic_corruption\" or \"paraphrase_control\"\n",
        "    step_index: int\n",
        "    expected_answer_change: bool\n",
        "    semantic_distance: float\n",
        "    original_step: str\n",
        "    corrupted_step: str\n",
        "    original_steps: List[str] = field(default_factory=list)\n",
        "    # ACL Strict Metrics\n",
        "    token_count_delta: int = 0\n",
        "    perplexity_ratio: float = 1.0\n",
        "    is_syntactically_valid: bool = True\n",
        "    is_control: bool = False\n",
        "\n",
        "    def validate(self) -> bool:\n",
        "      \"\"\"Ensure counterfactual meets quality criteria.\"\"\"\n",
        "      if len(self.steps) != self.step_index + 1:\n",
        "          return False\n",
        "      if self.steps[-1] != self.corrupted_step:\n",
        "          return False\n",
        "      return all(self.steps[i] == self.original_steps[i] for i in range(self.step_index))\n",
        "\n",
        "@dataclass\n",
        "class CounterfactualAccuracyResult:\n",
        "    \"\"\"Container for per-sample counterfactual accuracy results.\"\"\"\n",
        "    sample_idx: int\n",
        "    step_index: int\n",
        "    clean_correct: bool\n",
        "    corrupt_correct: bool\n",
        "    answer_flipped: bool\n",
        "    clean_prob_correct: float\n",
        "    corrupt_prob_correct: float\n",
        "    prob_delta: float\n",
        "    corruption_type: str\n",
        "    is_control: bool\n",
        "\n",
        "@dataclass\n",
        "class CKAResult:\n",
        "    layer_idx: int = -1 # Aggregate or specific layer\n",
        "    mean_cka: float = 0.0\n",
        "    decay_slope: float = 0.0\n",
        "    cka_rsa_correlation: float = 0.0\n",
        "    per_layer_cka: Dict = field(default_factory=dict)\n",
        "    per_step_cka: Dict = field(default_factory=dict)\n",
        "    kernel_type: str = \"linear\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 2.5: CKA Core Utilities\n",
        "# ==============================================================================\n",
        "\n",
        "class CKA:\n",
        "    \"\"\"\n",
        "    Centered Kernel Alignment (CKA) implementation with both linear and RBF kernels.\n",
        "    Based on Kornblith et al. (2019).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device: str = \"cpu\"):\n",
        "        self.device = device\n",
        "\n",
        "    @staticmethod\n",
        "    def centering_matrix(n: int, device: str = \"cpu\") -> torch.Tensor:\n",
        "        I = torch.eye(n, device=device)\n",
        "        ones = torch.ones(n, n, device=device)\n",
        "        return I - ones / n\n",
        "\n",
        "    @staticmethod\n",
        "    def centering(K: torch.Tensor) -> torch.Tensor:\n",
        "        n = K.shape[0]\n",
        "        H = CKA.centering_matrix(n, device=K.device)\n",
        "        return H @ K @ H\n",
        "\n",
        "    @staticmethod\n",
        "    def linear_kernel(X: torch.Tensor) -> torch.Tensor:\n",
        "        return X @ X.T\n",
        "\n",
        "    @staticmethod\n",
        "    def rbf_kernel(X: torch.Tensor, sigma: Optional[float] = None) -> torch.Tensor:\n",
        "        GX = X @ X.T\n",
        "        diag = torch.diag(GX)\n",
        "        KX = diag.unsqueeze(0) - 2 * GX + diag.unsqueeze(1)\n",
        "        if sigma is None:\n",
        "            mask = KX > 0\n",
        "            if mask.sum() > 0:\n",
        "                median_dist = torch.median(KX[mask])\n",
        "                sigma = torch.sqrt(median_dist / 2)\n",
        "            else:\n",
        "                sigma = torch.tensor(1.0, device=X.device)\n",
        "        KX = torch.exp(-KX / (2 * sigma ** 2))\n",
        "        return KX\n",
        "\n",
        "    @staticmethod\n",
        "    def hsic_biased(K: torch.Tensor, L: torch.Tensor) -> torch.Tensor:\n",
        "        n = K.shape[0]\n",
        "        K_centered = CKA.centering(K)\n",
        "        L_centered = CKA.centering(L)\n",
        "        return torch.sum(K_centered * L_centered) / ((n - 1) ** 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def hsic_unbiased(K: torch.Tensor, L: torch.Tensor) -> torch.Tensor:\n",
        "        n = K.shape[0]\n",
        "        if n < 4:\n",
        "            return CKA.hsic_biased(K, L)\n",
        "        K_tilde = K.clone()\n",
        "        L_tilde = L.clone()\n",
        "        K_tilde.fill_diagonal_(0)\n",
        "        L_tilde.fill_diagonal_(0)\n",
        "        ones = torch.ones(n, device=K.device)\n",
        "        term1 = torch.sum(K_tilde * L_tilde)\n",
        "        sum_K = K_tilde @ ones\n",
        "        sum_L = L_tilde @ ones\n",
        "        term2 = torch.dot(sum_K, sum_L)\n",
        "        total_K = torch.sum(K_tilde)\n",
        "        total_L = torch.sum(L_tilde)\n",
        "        term3 = total_K * total_L\n",
        "        hsic = (term1 / (n * (n - 3)) - 2 * term2 / ((n - 2) * (n - 3) * n) + term3 / (n * (n - 1) * (n - 2) * (n - 3)))\n",
        "        return hsic\n",
        "\n",
        "    def linear_cka(self, X: torch.Tensor, Y: torch.Tensor, debiased: bool = False) -> Tuple[float, Dict]:\n",
        "        if X.shape[0] != Y.shape[0]:\n",
        "            raise ValueError(f\"Sample count mismatch: {X.shape[0]} vs {Y.shape[0]}\")\n",
        "        n = X.shape[0]\n",
        "        if isinstance(X, np.ndarray): X = torch.from_numpy(X).float()\n",
        "        if isinstance(Y, np.ndarray): Y = torch.from_numpy(Y).float()\n",
        "        X = X.to(self.device)\n",
        "        Y = Y.to(self.device)\n",
        "        K = self.linear_kernel(X)\n",
        "        L = self.linear_kernel(Y)\n",
        "        if debiased:\n",
        "            hsic_xy = self.hsic_unbiased(K, L)\n",
        "            hsic_xx = self.hsic_unbiased(K, K)\n",
        "            hsic_yy = self.hsic_unbiased(L, L)\n",
        "        else:\n",
        "            hsic_xy = self.hsic_biased(K, L)\n",
        "            hsic_xx = self.hsic_biased(K, K)\n",
        "            hsic_yy = self.hsic_biased(L, L)\n",
        "        denom = torch.sqrt(hsic_xx * hsic_yy)\n",
        "        if denom < 1e-10: cka = 0.0\n",
        "        else: cka = (hsic_xy / denom).item()\n",
        "        return float(np.clip(cka, -1.0, 1.0)), {\"hsic_xy\": hsic_xy.item(), \"n\": n}\n",
        "\n",
        "    def rbf_cka(self, X: torch.Tensor, Y: torch.Tensor, sigma: Optional[float] = None, debiased: bool = False) -> Tuple[float, Dict]:\n",
        "        if isinstance(X, np.ndarray): X = torch.from_numpy(X).float()\n",
        "        if isinstance(Y, np.ndarray): Y = torch.from_numpy(Y).float()\n",
        "        X = X.to(self.device)\n",
        "        Y = Y.to(self.device)\n",
        "        K = self.rbf_kernel(X, sigma)\n",
        "        L = self.rbf_kernel(Y, sigma)\n",
        "        if debiased:\n",
        "            hsic_xy = self.hsic_unbiased(K, L)\n",
        "            hsic_xx = self.hsic_unbiased(K, K)\n",
        "            hsic_yy = self.hsic_unbiased(L, L)\n",
        "        else:\n",
        "            hsic_xy = self.hsic_biased(K, L)\n",
        "            hsic_xx = self.hsic_biased(K, K)\n",
        "            hsic_yy = self.hsic_biased(L, L)\n",
        "        denom = torch.sqrt(hsic_xx * hsic_yy)\n",
        "        if denom < 1e-10: cka = 0.0\n",
        "        else: cka = (hsic_xy / denom).item()\n",
        "        return float(np.clip(cka, -1.0, 1.0)), {\"hsic_xy\": hsic_xy.item()}"
      ],
      "metadata": {
        "id": "KrmJB50ufPZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RSACalculator:\n",
        "    \"\"\"\n",
        "    Implements Kriegeskorte et al. (2008) RSA with Nili et al. (2014) stats.\n",
        "    Optimized for GPU acceleration using PyTorch while maintaining original API.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 dissimilarity_metric: str = \"correlation\",\n",
        "                 bootstrap_iterations: int = 10000,\n",
        "                 enable_noise_ceiling: bool = True,\n",
        "                 enable_bootstrap: bool = True,\n",
        "                 random_seed: int = 42,\n",
        "                 device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        self.dissimilarity_metric = dissimilarity_metric\n",
        "        self.bootstrap_iterations = bootstrap_iterations\n",
        "        self.enable_noise_ceiling = enable_noise_ceiling\n",
        "        self.enable_bootstrap = enable_bootstrap\n",
        "        self.device = device\n",
        "        self.rng = np.random.RandomState(random_seed)\n",
        "\n",
        "    def _to_tensor(self, x):\n",
        "        \"\"\"Helper to convert to float32 tensor on device.\"\"\"\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            return x.to(self.device).float()\n",
        "        return torch.tensor(x, device=self.device, dtype=torch.float32)\n",
        "\n",
        "    def _rank_data(self, data: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Computes ranks on the GPU (column-wise).\"\"\"\n",
        "        # argsort twice gives the rank indices\n",
        "        return data.argsort(dim=0).argsort(dim=0).float()\n",
        "\n",
        "    def compute_rdm(self, X: Union[np.ndarray, torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"Compute Representational Dissimilarity Matrix on GPU.\"\"\"\n",
        "        X_gpu = self._to_tensor(X)\n",
        "\n",
        "        if self.dissimilarity_metric == \"correlation\":\n",
        "            # Center and normalize\n",
        "            X_centered = X_gpu - X_gpu.mean(dim=1, keepdim=True)\n",
        "            norms = torch.linalg.norm(X_centered, dim=1, keepdim=True)\n",
        "            norms = torch.where(norms < 1e-10, torch.tensor(1.0, device=self.device), norms)\n",
        "            X_normed = X_centered / norms\n",
        "\n",
        "            # Correlation matrix\n",
        "            corr_matrix = torch.clamp(X_normed @ X_normed.T, -1.0, 1.0)\n",
        "            rdm = 1.0 - corr_matrix\n",
        "\n",
        "        elif self.dissimilarity_metric == \"euclidean\":\n",
        "            # Pairwise euclidean distances\n",
        "            rdm = torch.cdist(X_gpu, X_gpu, p=2.0)\n",
        "\n",
        "        elif self.dissimilarity_metric == \"cosine\":\n",
        "            norms = torch.linalg.norm(X_gpu, dim=1, keepdim=True)\n",
        "            norms = torch.where(norms < 1e-10, torch.tensor(1.0, device=self.device), norms)\n",
        "            X_normed = X_gpu / norms\n",
        "            cos_sim = torch.clamp(X_normed @ X_normed.T, -1.0, 1.0)\n",
        "            rdm = 1.0 - cos_sim\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown metric: {self.dissimilarity_metric}\")\n",
        "\n",
        "        rdm.fill_diagonal_(0.0)\n",
        "        return (rdm + rdm.T) / 2.0\n",
        "\n",
        "    def compare_rdms(self, rdm1: torch.Tensor, rdm2: torch.Tensor) -> Tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Spearman correlation between upper triangles on GPU.\n",
        "        Returns (rho, p_value).\n",
        "        \"\"\"\n",
        "        n = rdm1.shape[0]\n",
        "        # Extract upper triangles\n",
        "        triu_idx = torch.triu_indices(n, n, offset=1, device=self.device)\n",
        "        v1 = rdm1[triu_idx[0], triu_idx[1]]\n",
        "        v2 = rdm2[triu_idx[0], triu_idx[1]]\n",
        "\n",
        "        # Spearman = Pearson on ranks\n",
        "        v1_rank = self._rank_data(v1.unsqueeze(1)).squeeze()\n",
        "        v2_rank = self._rank_data(v2.unsqueeze(1)).squeeze()\n",
        "\n",
        "        v1_cent = v1_rank - v1_rank.mean()\n",
        "        v2_cent = v2_rank - v2_rank.mean()\n",
        "\n",
        "        covariance = (v1_cent * v2_cent).sum()\n",
        "        std_v1 = (v1_cent ** 2).sum().sqrt()\n",
        "        std_v2 = (v2_cent ** 2).sum().sqrt()\n",
        "\n",
        "        rho = covariance / (std_v1 * std_v2 + 1e-8)\n",
        "\n",
        "        # Approximate p-value for Spearman (t-distribution)\n",
        "        # For rigorous stats, we rely on the bootstrap method below\n",
        "        p_val = 0.0\n",
        "\n",
        "        return rho.item(), p_val\n",
        "\n",
        "    def bootstrap_significance(self, rdm1: torch.Tensor, rdm2: torch.Tensor, observed_rho: float) -> Tuple[float, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Permutation test for non-parametric significance on GPU (Batched).\n",
        "        Shuffles the rows/cols of rdm2 (Mantel test).\n",
        "        \"\"\"\n",
        "        n = rdm1.shape[0]\n",
        "        n_perms = self.bootstrap_iterations\n",
        "\n",
        "        # Pre-calculate rdm1 upper triangle info\n",
        "        triu_idx = torch.triu_indices(n, n, offset=1, device=self.device)\n",
        "        v1 = rdm1[triu_idx[0], triu_idx[1]]\n",
        "        v1_rank = self._rank_data(v1.unsqueeze(1)).squeeze()\n",
        "        v1_cent = v1_rank - v1_rank.mean()\n",
        "        v1_std = (v1_cent ** 2).sum().sqrt()\n",
        "\n",
        "        count_better = 0\n",
        "        null_dist = []\n",
        "        chunk_size = 500 # Process perms in chunks to save VRAM\n",
        "\n",
        "        for _ in range(0, n_perms, chunk_size):\n",
        "            curr_chunk = min(chunk_size, n_perms - _)\n",
        "\n",
        "            # Generate batch of random permutations\n",
        "            perms = torch.stack([torch.randperm(n, device=self.device) for _ in range(curr_chunk)])\n",
        "\n",
        "            # Expand rdm2 for gathering\n",
        "            rdm2_exp = rdm2.unsqueeze(0).expand(curr_chunk, -1, -1)\n",
        "\n",
        "            # Permute rows\n",
        "            row_idx = perms.unsqueeze(2).expand(-1, -1, n)\n",
        "            rdm2_perm_rows = torch.gather(rdm2_exp, 1, row_idx)\n",
        "\n",
        "            # Permute cols\n",
        "            col_idx = perms.unsqueeze(1).expand(-1, n, -1)\n",
        "            rdm2_perm = torch.gather(rdm2_perm_rows, 2, col_idx)\n",
        "\n",
        "            # Extract upper triangles for the batch\n",
        "            v2_batch = rdm2_perm[:, triu_idx[0], triu_idx[1]]\n",
        "\n",
        "            # Compute correlations for batch\n",
        "            v2_ranks = v2_batch.argsort(dim=1).argsort(dim=1).float()\n",
        "            v2_means = v2_ranks.mean(dim=1, keepdim=True)\n",
        "            v2_cents = v2_ranks - v2_means\n",
        "\n",
        "            covs = (v1_cent.unsqueeze(0) * v2_cents).sum(dim=1)\n",
        "            v2_stds = (v2_cents ** 2).sum(dim=1).sqrt()\n",
        "\n",
        "            batch_rhos = covs / (v1_std * v2_stds + 1e-8)\n",
        "\n",
        "            count_better += (batch_rhos.abs() >= abs(observed_rho)).sum().item()\n",
        "            null_dist.append(batch_rhos.cpu().numpy())\n",
        "\n",
        "        p_value = count_better / n_perms\n",
        "        return float(p_value), np.concatenate(null_dist)\n",
        "\n",
        "    def compute_noise_ceiling(self, X: Union[np.ndarray, torch.Tensor]) -> Tuple[float, float]:\n",
        "        \"\"\"Estimate measurement reliability on GPU.\"\"\"\n",
        "        X_gpu = self._to_tensor(X)\n",
        "        n = X_gpu.shape[0]\n",
        "        if n < 4: return 1.0, 1.0\n",
        "\n",
        "        rdm_full = self.compute_rdm(X_gpu)\n",
        "        correlations = []\n",
        "\n",
        "        for _ in range(100):\n",
        "            # Split-half indices\n",
        "            idx = torch.randint(0, n, (n,), device=self.device)\n",
        "            # Resample RDM\n",
        "            rdm_resample = rdm_full.index_select(0, idx).index_select(1, idx)\n",
        "\n",
        "            rho, _ = self.compare_rdms(rdm_full, rdm_resample)\n",
        "            correlations.append(rho)\n",
        "\n",
        "        lower_bound = np.mean(correlations)\n",
        "        upper_bound = (2 * lower_bound) / (1 + lower_bound) if lower_bound < 1.0 else 1.0\n",
        "        return float(np.clip(lower_bound, 0.0, 1.0)), float(np.clip(upper_bound, 0.0, 1.0))\n",
        "\n",
        "    def compute_rsa(self, X_clean: Union[np.ndarray, torch.Tensor], X_corrupt: Union[np.ndarray, torch.Tensor], return_detailed: bool = True) -> Dict:\n",
        "        \"\"\"Orchestrator for RSA + Stats.\"\"\"\n",
        "        # compute_rdm handles conversion to Tensor/GPU\n",
        "        rdm_clean = self.compute_rdm(X_clean)\n",
        "        rdm_corrupt = self.compute_rdm(X_corrupt)\n",
        "\n",
        "        rsa_corr, p_value = self.compare_rdms(rdm_clean, rdm_corrupt)\n",
        "\n",
        "        bootstrap_p, null_dist = None, None\n",
        "        if self.enable_bootstrap:\n",
        "            bootstrap_p, null_dist = self.bootstrap_significance(rdm_clean, rdm_corrupt, rsa_corr)\n",
        "            # Use bootstrap p-value as the primary one if enabled\n",
        "            p_value = bootstrap_p\n",
        "\n",
        "        nc_lower, nc_upper = None, None\n",
        "        if self.enable_noise_ceiling:\n",
        "            nc_lower, nc_upper = self.compute_noise_ceiling(X_clean)\n",
        "\n",
        "        if return_detailed:\n",
        "            return RSAResult(\n",
        "                rsa_correlation=rsa_corr,\n",
        "                p_value=p_value,\n",
        "                bootstrap_p_value=bootstrap_p,\n",
        "                noise_ceiling_lower=nc_lower,\n",
        "                noise_ceiling_upper=nc_upper,\n",
        "                rdm_clean=rdm_clean.cpu().numpy(),\n",
        "                rdm_corrupt=rdm_corrupt.cpu().numpy(),\n",
        "                n_samples=rdm_clean.shape[0],\n",
        "                bootstrap_distribution=null_dist\n",
        "            ).to_dict()\n",
        "\n",
        "        return {\"rsa_correlation\": rsa_corr, \"p_value\": p_value}"
      ],
      "metadata": {
        "id": "NlPatsHw4XRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalRSA:\n",
        "    \"\"\"Sliding window analysis for CoT trajectories.\"\"\"\n",
        "    def __init__(self, rsa_calculator, window_size=3, stride=1):\n",
        "        self.rsa_calc = rsa_calculator\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def compute_temporal_rsa(self, clean_traj: List[np.ndarray], corrupt_traj: List[np.ndarray]) -> Dict[int, float]:\n",
        "        T = min(len(clean_traj), len(corrupt_traj))\n",
        "        temporal_rsa = {}\n",
        "\n",
        "        for start in range(0, T - self.window_size + 1, self.stride):\n",
        "            end = start + self.window_size\n",
        "            # Stack (Time * Batch) to capture temporal geometry\n",
        "            X_clean = np.vstack(clean_traj[start:end])\n",
        "            X_corrupt = np.vstack(corrupt_traj[start:end])\n",
        "\n",
        "            res = self.rsa_calc.compute_rsa(X_clean, X_corrupt, return_detailed=False)\n",
        "            temporal_rsa[start] = res['rsa_correlation']\n",
        "        return temporal_rsa"
      ],
      "metadata": {
        "id": "4jzjgJ_GAV66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwEUCt746ecu"
      },
      "source": [
        "## (3) Model Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "a8db5649d9fc415bab56532c364f10f3",
            "31cf858b21ea48d8887f6b365074e51a",
            "8f4bd9504d214a2d877c907cab125efd",
            "d7db9a9f3107410fa4fb3442b97d6fda",
            "bd3ab0db27f841238e6a1e1e42a7fb2f",
            "f898d7c2adba435a9a57e9a35ddcf2f1",
            "86b14207de8f40f6828291a3531ce3d6",
            "1d9247211d7d473eb0a6ac92c90097f2",
            "b5d1d3c1e35a4235b44ce7965c9cdcfa",
            "b97861049f894130a0f3d80920e469bd",
            "d8aa232c31324cac92833f512dd158e6"
          ]
        },
        "id": "BsqInVJ-6ecu",
        "outputId": "77b76cf5-a242-4b41-b5f4-e7404b6aaef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Loading: meta-llama/Llama-3.1-8B-Instruct\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8db5649d9fc415bab56532c364f10f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Model loaded: 8,030,261,248 parameters, 32 layers\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Model Manager and Token Validator (unchanged from original)\n",
        "# ============================================================================\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self, config: ExperimentConfig):\n",
        "        self.config = config\n",
        "        self.current_model = None\n",
        "        self.current_tokenizer = None\n",
        "        self.current_model_name = None\n",
        "        self._registered_tasks = []\n",
        "\n",
        "    def register_task(self, task):\n",
        "        if task not in self._registered_tasks:\n",
        "            self._registered_tasks.append(task)\n",
        "\n",
        "    def _invalidate_task_caches(self):\n",
        "        for task in self._registered_tasks:\n",
        "            if hasattr(task, 'clear_token_cache'):\n",
        "                task.clear_token_cache()\n",
        "\n",
        "    def unload_model(self):\n",
        "        if self.current_model is not None:\n",
        "            print(f\"Unloading: {self.current_model_name}\")\n",
        "            self._invalidate_task_caches()\n",
        "            del self.current_model\n",
        "            del self.current_tokenizer\n",
        "            self.current_model = None\n",
        "            self.current_tokenizer = None\n",
        "            self.current_model_name = None\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    def load_model(self, model_name: str) -> Tuple[Any, Any]:\n",
        "        self.unload_model()\n",
        "        print(f\"\\n{'='*60}\\nLoading: {model_name}\\n{'='*60}\")\n",
        "\n",
        "        model_kwargs = {\n",
        "            \"torch_dtype\": torch.bfloat16,\n",
        "            \"device_map\": \"auto\",\n",
        "            \"trust_remote_code\": True,\n",
        "            \"attn_implementation\": \"sdpa\"\n",
        "        }\n",
        "\n",
        "        if \"gemma\" in model_name.lower():\n",
        "            model_kwargs[\"attn_implementation\"] = \"eager\"\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
        "        model.eval()\n",
        "\n",
        "        self.current_model = model\n",
        "        self.current_tokenizer = tokenizer\n",
        "        self.current_model_name = model_name\n",
        "\n",
        "        print(f\"âœ“ Model loaded: {model.num_parameters():,} parameters, {model.config.num_hidden_layers} layers\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    def get_model_short_name(self) -> str:\n",
        "        if not self.current_model_name:\n",
        "            return \"unknown\"\n",
        "        return self.current_model_name.split(\"/\")[-1].replace(\"-\", \"_\")\n",
        "\n",
        "\n",
        "class TokenValidator:\n",
        "    @staticmethod\n",
        "    def find_best_categorical_tokens(\n",
        "        tokenizer,\n",
        "        positive_candidates: List[str],\n",
        "        negative_candidates: List[str],\n",
        "        task_name: str,\n",
        "        require_single_token: bool = True\n",
        "    ) -> TokenValidationResult:\n",
        "        best_result = None\n",
        "        all_warnings = []\n",
        "\n",
        "        for pos_text in positive_candidates:\n",
        "            for neg_text in negative_candidates:\n",
        "                pos_ids = tokenizer.encode(pos_text, add_special_tokens=False)\n",
        "                neg_ids = tokenizer.encode(neg_text, add_special_tokens=False)\n",
        "\n",
        "                if not pos_ids or not neg_ids:\n",
        "                    continue\n",
        "\n",
        "                is_single_pos = len(pos_ids) == 1\n",
        "                is_single_neg = len(neg_ids) == 1\n",
        "\n",
        "                if require_single_token and (not is_single_pos or not is_single_neg):\n",
        "                    continue\n",
        "\n",
        "                pos_id, neg_id = pos_ids[0], neg_ids[0]\n",
        "                if pos_id == neg_id:\n",
        "                    continue\n",
        "\n",
        "                score = 0\n",
        "                if is_single_pos: score += 20\n",
        "                if is_single_neg: score += 20\n",
        "\n",
        "                result = TokenValidationResult(\n",
        "                    positive_id=pos_id, negative_id=neg_id,\n",
        "                    token_type=\"categorical\",\n",
        "                    pos_text=pos_text, neg_text=neg_text,\n",
        "                    is_single_token_pos=is_single_pos,\n",
        "                    is_single_token_neg=is_single_neg,\n",
        "                    validation_score=score, warnings=[]\n",
        "                )\n",
        "\n",
        "                if best_result is None or score > best_result.validation_score:\n",
        "                    best_result = result\n",
        "\n",
        "        if best_result is None and require_single_token:\n",
        "            return TokenValidator.find_best_categorical_tokens(\n",
        "                tokenizer, positive_candidates, negative_candidates,\n",
        "                task_name, require_single_token=False\n",
        "            )\n",
        "\n",
        "        if best_result is None:\n",
        "            raise ValueError(f\"[{task_name}] Could not find valid token pair.\")\n",
        "\n",
        "        return best_result\n",
        "\n",
        "\n",
        "# Initialize and load model\n",
        "model_manager = ModelManager(config)\n",
        "model, tokenizer = model_manager.load_model(config.models[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY4o-NAz6ecv"
      },
      "source": [
        "## (4) MechanisticToolkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================================\n",
        "# MECHANISTIC TOOLKIT - SECTIONS D, E, F, G (v4.0 - Integrated Linear Probing)\n",
        "# ===================================================================================\n",
        "\n",
        "from tqdm.auto import tqdm  # Explicitly use compact progress bar\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from collections import defaultdict\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.stats import spearmanr\n",
        "import warnings\n",
        "import difflib\n",
        "\n",
        "# Suppress sklearn warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "class MechanisticToolkit:\n",
        "    \"\"\"\n",
        "    Implements Sections C (Patching), D (Probing), E (RSA), F (PCA), G (TAS).\n",
        "    Revised to safely handle BFloat16 by forcing Float32 conversion for Analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, config):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.config = config\n",
        "        self.layers = self._get_model_layers()\n",
        "        self.n_layers = len(self.layers)\n",
        "        self.device = model.device\n",
        "\n",
        "        # Probing regularization parameters (Section D)\n",
        "        self.l2_C = 100.0\n",
        "        self.l2_alpha = 1.0\n",
        "\n",
        "    def _get_model_layers(self):\n",
        "        if hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):\n",
        "            return list(self.model.model.layers)\n",
        "        elif hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'h'):\n",
        "            return list(self.model.transformer.h)\n",
        "        # Fallback for Gemma/Other architectures\n",
        "        for name, module in self.model.named_modules():\n",
        "            if name.endswith(\".layers\") and isinstance(module, torch.nn.ModuleList):\n",
        "                return list(module)\n",
        "        return []\n",
        "\n",
        "    def _find_step_token_positions(self, full_text: str, steps: List[str]) -> List[int]:\n",
        "      step_indices = []\n",
        "      current_idx = 0\n",
        "      encoding = self.tokenizer(full_text, return_offsets_mapping=True)\n",
        "      offset_mapping = encoding.offset_mapping\n",
        "\n",
        "      for step in steps:\n",
        "          step_clean = step.strip()\n",
        "          start_char = full_text.find(step_clean, current_idx)\n",
        "          if start_char == -1:\n",
        "              # fallback: closest match using difflib\n",
        "              matches = difflib.get_close_matches(step_clean, [full_text[current_idx:]], n=1, cutoff=0.6)\n",
        "              if matches:\n",
        "                  start_char = full_text.find(matches[0], current_idx)\n",
        "              else:\n",
        "                  continue\n",
        "\n",
        "          end_char = start_char + len(step_clean)\n",
        "          current_idx = end_char\n",
        "\n",
        "          target_token_idx = -1\n",
        "          for i, (tok_start, tok_end) in enumerate(offset_mapping):\n",
        "              if tok_start < end_char <= tok_end:\n",
        "                  target_token_idx = i\n",
        "                  break\n",
        "              if tok_end == end_char:\n",
        "                  target_token_idx = i\n",
        "                  break\n",
        "\n",
        "          if target_token_idx != -1:\n",
        "              step_indices.append(target_token_idx)\n",
        "\n",
        "      return step_indices\n",
        "\n",
        "\n",
        "    def _get_activations(self, text: str, layer_idx: int = -1, return_all_layers: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Get hidden states. Returns float32 CPU tensors.\n",
        "        \"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs, output_hidden_states=True)\n",
        "\n",
        "        hidden_states = outputs.hidden_states\n",
        "\n",
        "        if return_all_layers:\n",
        "            return torch.stack(hidden_states).squeeze(1).detach().float().cpu()\n",
        "\n",
        "        if layer_idx == -1:\n",
        "            layer_idx = len(hidden_states) - 1\n",
        "        return hidden_states[layer_idx].squeeze(0).detach().float().cpu()\n",
        "\n",
        "    # =========================================================================\n",
        "    # SECTION D: Linear Probe Module\n",
        "    # =========================================================================\n",
        "    def train_linear_probes(self, task, dataset: List, task_name: str) -> Dict:\n",
        "        if not self.config.enable_probing:\n",
        "            return {\"enabled\": False}\n",
        "\n",
        "        print(f\"   [Probing] Training probes for {task_name}...\")\n",
        "        if task_name == \"dyck\":\n",
        "            return self._train_dyck_probes(task, dataset)\n",
        "        elif task_name == \"pronto_qa\":\n",
        "            return self._train_pronto_qa_probes(task, dataset)\n",
        "        elif task_name == \"gsm8k\":\n",
        "            return self._train_gsm8k_probes(task, dataset)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task: {task_name}\")\n",
        "\n",
        "    def _train_dyck_probes(self, task, dataset: List) -> Dict:\n",
        "      print(\"   [Probing] Extracting Dyck stack depth labels (full CoT)...\")\n",
        "      layer_data = defaultdict(lambda: {'X': [], 'y': []})\n",
        "      target_probe_samples = self.config.probe_total_samples\n",
        "      samples_to_use = dataset[:min(target_probe_samples, len(dataset))]\n",
        "\n",
        "      for sample in tqdm(samples_to_use, desc=\"Extracting Dyck activations\", leave=False):\n",
        "          try:\n",
        "              if not hasattr(sample, 'depths') or not sample.depths:\n",
        "                  continue\n",
        "\n",
        "              full_text = task.build_prompt(sample.input, sample.steps)\n",
        "              all_layer_acts = self._get_activations(full_text, return_all_layers=True).numpy()\n",
        "              step_token_positions = self._find_step_token_positions(full_text, sample.steps)\n",
        "\n",
        "              n_steps = min(len(sample.depths), len(step_token_positions))\n",
        "              for step_idx in range(n_steps):\n",
        "                  token_pos = step_token_positions[step_idx]\n",
        "                  depth = sample.depths[step_idx]\n",
        "                  if token_pos >= all_layer_acts.shape[1]:\n",
        "                      continue\n",
        "                  for layer_idx in range(self.n_layers):\n",
        "                      h = all_layer_acts[layer_idx + 1, token_pos, :]\n",
        "                      layer_data[layer_idx]['X'].append(h)\n",
        "                      layer_data[layer_idx]['y'].append(depth)\n",
        "\n",
        "          except Exception:\n",
        "              continue\n",
        "\n",
        "      return self._train_classification_probes(layer_data, target_name=\"stack_depth\", task_name=\"dyck\")\n",
        "\n",
        "\n",
        "\n",
        "    def _train_pronto_qa_probes(self, task, dataset: List) -> Dict:\n",
        "      print(\"   [Probing] Extracting ProntoQA labels (full CoT)...\")\n",
        "      layer_data_all_steps = defaultdict(lambda: {'X': [], 'y': []})\n",
        "      layer_data_last_step = defaultdict(lambda: {'X': [], 'y': []})\n",
        "      stats = {'processed': 0, 'skipped': 0}\n",
        "\n",
        "      def normalize_truth(ans):\n",
        "          s = str(ans).strip().lower()\n",
        "          return 'True' if s == 'true' else 'False'\n",
        "\n",
        "      samples_to_use = dataset[:min(self.config.probe_total_samples, len(dataset))]\n",
        "\n",
        "      for sample in tqdm(samples_to_use, desc=\"ProntoQA Probing (Full CoT)\", leave=False):\n",
        "          try:\n",
        "              if not hasattr(sample, 'steps') or len(sample.steps) < 2:\n",
        "                  stats['skipped'] += 1\n",
        "                  continue\n",
        "              if not hasattr(sample, 'answer'):\n",
        "                  stats['skipped'] += 1\n",
        "                  continue\n",
        "\n",
        "              full_text = task.build_prompt(sample.input, sample.steps)\n",
        "              all_layer_acts = self._get_activations(full_text, return_all_layers=True).numpy()\n",
        "              step_positions = self._find_step_token_positions(full_text, sample.steps)\n",
        "              if len(step_positions) < 1:\n",
        "                  stats['skipped'] += 1\n",
        "                  continue\n",
        "\n",
        "              stats['processed'] += 1\n",
        "              truth_label = normalize_truth(sample.answer)\n",
        "\n",
        "              n_steps = min(len(sample.steps), len(step_positions))\n",
        "              for step_idx in range(n_steps):\n",
        "                  token_pos = step_positions[step_idx]\n",
        "                  if token_pos >= all_layer_acts.shape[1]:\n",
        "                      continue\n",
        "                  for layer_idx in range(self.n_layers):\n",
        "                      h = all_layer_acts[layer_idx + 1, token_pos, :]\n",
        "                      layer_data_all_steps[layer_idx]['X'].append(h)\n",
        "                      layer_data_all_steps[layer_idx]['y'].append(truth_label)\n",
        "\n",
        "              # Save last step separately\n",
        "              last_token_idx = step_positions[-1]\n",
        "              for layer_idx in range(self.n_layers):\n",
        "                  h = all_layer_acts[layer_idx + 1, last_token_idx, :]\n",
        "                  layer_data_last_step[layer_idx]['X'].append(h)\n",
        "                  layer_data_last_step[layer_idx]['y'].append(truth_label)\n",
        "\n",
        "          except Exception:\n",
        "              continue\n",
        "\n",
        "    # Train probes\n",
        "      res_all = self._train_classification_probes(layer_data_all_steps, \"all_steps\", \"pronto_qa\")\n",
        "      res_last = self._train_classification_probes(layer_data_last_step, \"pre_final_truth\", \"pronto_qa\")\n",
        "\n",
        "      res_all[\"last_step_results\"] = res_last\n",
        "      return res_all\n",
        "\n",
        "\n",
        "\n",
        "    def _train_gsm8k_probes(self, task, dataset: List) -> Dict:\n",
        "      print(\"   [Probing] Extracting GSM8K operation labels (full CoT)...\")\n",
        "      op_data = defaultdict(lambda: {'X': [], 'y': []})\n",
        "\n",
        "      def get_op(step: str):\n",
        "          step_lower = step.lower()\n",
        "          if '+' in step_lower or 'add' in step_lower: return 'add'\n",
        "          if '-' in step_lower or 'sub' in step_lower: return 'sub'\n",
        "          if '*' in step_lower or 'mul' in step_lower: return 'mul'\n",
        "          return 'other'\n",
        "\n",
        "      samples_to_use = dataset[:min(self.config.probe_total_samples, len(dataset))]\n",
        "\n",
        "      for sample in tqdm(samples_to_use, desc=\"GSM8K Probing (Full CoT)\", leave=False):\n",
        "          try:\n",
        "              full_text = task.build_prompt(sample.input, sample.steps)\n",
        "              all_layer_acts = self._get_activations(full_text, return_all_layers=True).numpy()\n",
        "              step_token_positions = self._find_step_token_positions(full_text, sample.steps)\n",
        "\n",
        "              n_steps = min(len(sample.steps), len(step_token_positions))\n",
        "              for step_idx in range(n_steps):\n",
        "                  token_pos = step_token_positions[step_idx]\n",
        "                  step = sample.steps[step_idx]\n",
        "                  op = get_op(step)\n",
        "\n",
        "                  # Keep 'other' too, so plots show all CoT lengths\n",
        "                  if token_pos >= all_layer_acts.shape[1]:\n",
        "                      continue\n",
        "\n",
        "                  for layer_idx in range(self.n_layers):\n",
        "                      h = all_layer_acts[layer_idx + 1, token_pos, :]\n",
        "                      op_data[layer_idx]['X'].append(h)\n",
        "                      op_data[layer_idx]['y'].append(op)\n",
        "\n",
        "          except Exception:\n",
        "              continue\n",
        "\n",
        "      # Train probes\n",
        "      return self._train_classification_probes(op_data, target_name=\"operation_type\", task_name=\"gsm8k\")\n",
        "\n",
        "\n",
        "\n",
        "    def _train_classification_probes(self, layer_data, target_name, task_name) -> Dict:\n",
        "        results = {}\n",
        "        accuracies = []\n",
        "\n",
        "        for layer_idx in tqdm(range(self.n_layers), desc=f\"Training {target_name} probes\", leave=False):\n",
        "            if layer_idx not in layer_data or len(layer_data[layer_idx]['X']) < 10:\n",
        "                continue\n",
        "\n",
        "            X = np.array(layer_data[layer_idx]['X'])\n",
        "            y = np.array(layer_data[layer_idx]['y'])\n",
        "\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            try:\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "                clf = LogisticRegression(C=self.l2_C, max_iter=1000, solver='liblinear')\n",
        "                clf.fit(X_train, y_train)\n",
        "                y_pred = clf.predict(X_test)\n",
        "                acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "                accuracies.append(acc)\n",
        "                results[layer_idx] = {\n",
        "                    \"accuracy\": acc,\n",
        "                    \"n_train\": len(X_train),\n",
        "                    \"n_test\": len(X_test)\n",
        "                }\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        mean_acc = np.mean(accuracies) if accuracies else 0.0\n",
        "        results[\"mean_accuracy\"] = mean_acc\n",
        "        results[\"target_name\"] = target_name\n",
        "        return results\n",
        "\n",
        "    # =========================================================================\n",
        "    # SECTION E: RSA (Representational Similarity Analysis) - CORRECTED\n",
        "    # =========================================================================\n",
        "    def compute_rsa_batch(\n",
        "        self,\n",
        "        task,\n",
        "        dataset: List,\n",
        "        counterfactuals: Dict[int, List] = None,\n",
        "        n_samples: int = 50,\n",
        "        dissimilarity_metric: str = \"correlation\"\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Compute canonical RSA between clean and corrupt conditions.\n",
        "        UPDATED: Now includes TemporalRSA integration.\n",
        "        \"\"\"\n",
        "        if not self.config.enable_rsa:\n",
        "            return {\"mean_rsa\": 0.0, \"mean_similarity\": 0.0, \"enabled\": False}\n",
        "\n",
        "        # Initialize RSA calculator\n",
        "        rsa_calc = RSACalculator(\n",
        "            dissimilarity_metric=getattr(self.config, 'rsa_dissimilarity_metric', 'correlation'),\n",
        "            bootstrap_iterations=getattr(self.config, 'rsa_bootstrap_iterations', 10000),\n",
        "            enable_noise_ceiling=getattr(self.config, 'rsa_noise_ceiling_enabled', True),\n",
        "            enable_bootstrap=getattr(self.config, 'rsa_bootstrap_enabled', True)\n",
        "        )\n",
        "\n",
        "        # Limit samples\n",
        "        samples = dataset[:min(n_samples, len(dataset))]\n",
        "\n",
        "        # Storage for activations per layer (Standard RSA)\n",
        "        clean_acts_per_layer = {l: [] for l in range(self.n_layers)}\n",
        "        corrupt_acts_per_layer = {l: [] for l in range(self.n_layers)}\n",
        "\n",
        "        # Storage for Temporal RSA (Trajectories from Middle Layer)\n",
        "        enable_temporal = getattr(self.config, 'rsa_temporal_enabled', False)\n",
        "        mid_layer = self.n_layers // 2\n",
        "        clean_traj_storage = []   # List of [T, D] arrays\n",
        "        corrupt_traj_storage = [] # List of [T, D] arrays\n",
        "\n",
        "        # Track which samples we successfully process\n",
        "        processed_indices = []\n",
        "        step_indices_used = []\n",
        "\n",
        "        print(f\"   [RSA] Processing {len(samples)} samples (Temporal={enable_temporal})...\")\n",
        "\n",
        "        for idx, sample in enumerate(tqdm(samples, desc=\"RSA: Extracting activations\", leave=False)):\n",
        "            try:\n",
        "                # Skip samples with insufficient steps\n",
        "                if len(sample.steps) < 2:\n",
        "                    continue\n",
        "\n",
        "                # ===== CLEAN ACTIVATIONS =====\n",
        "                clean_prompt = task.build_prompt(sample.input, sample.steps)\n",
        "                clean_acts = self._get_activations(clean_prompt, return_all_layers=True)\n",
        "                if isinstance(clean_acts, torch.Tensor):\n",
        "                    clean_acts = clean_acts.cpu().numpy()\n",
        "\n",
        "                # ===== CORRUPT ACTIVATIONS =====\n",
        "                # Option 1: Use pre-computed counterfactuals\n",
        "                if counterfactuals and idx in counterfactuals and counterfactuals[idx]:\n",
        "                    cf = counterfactuals[idx][0]  # Use first counterfactual\n",
        "                    corrupt_steps = cf.steps\n",
        "                    step_idx = cf.step_index\n",
        "                # Option 2: Generate counterfactual on the fly\n",
        "                else:\n",
        "                    step_idx = min(len(sample.steps) // 2, len(sample.steps) - 1)\n",
        "                    cf_result = task.generate_counterfactual(sample, step_idx)\n",
        "                    corrupt_steps = cf_result.steps\n",
        "\n",
        "                corrupt_prompt = task.build_prompt(sample.input, corrupt_steps)\n",
        "                corrupt_acts = self._get_activations(corrupt_prompt, return_all_layers=True)\n",
        "                if isinstance(corrupt_acts, torch.Tensor):\n",
        "                    corrupt_acts = corrupt_acts.cpu().numpy()\n",
        "\n",
        "                # ===== EXTRACT FINAL TOKEN ACTIVATIONS (Standard RSA) =====\n",
        "                for layer_idx in range(self.n_layers):\n",
        "                    # +1 to skip embedding layer (hidden_states[0] is embeddings)\n",
        "                    h_clean = clean_acts[layer_idx + 1, -1, :]  # [D]\n",
        "                    h_corrupt = corrupt_acts[layer_idx + 1, -1, :]  # [D]\n",
        "\n",
        "                    clean_acts_per_layer[layer_idx].append(h_clean)\n",
        "                    corrupt_acts_per_layer[layer_idx].append(h_corrupt)\n",
        "\n",
        "                # ===== EXTRACT TRAJECTORIES (Temporal RSA) =====\n",
        "                if enable_temporal:\n",
        "                    # Align based on step positions or raw tokens?\n",
        "                    # Raw tokens usually preferred for geometry, but we must match lengths.\n",
        "                    # We store raw [T, D] here and align later.\n",
        "                    t_clean = clean_acts[mid_layer + 1] # [T_clean, D]\n",
        "                    t_corrupt = corrupt_acts[mid_layer + 1] # [T_corrupt, D]\n",
        "\n",
        "                    # Store only if we have minimum length\n",
        "                    if len(t_clean) > 2 and len(t_corrupt) > 2:\n",
        "                        clean_traj_storage.append(t_clean)\n",
        "                        corrupt_traj_storage.append(t_corrupt)\n",
        "\n",
        "                processed_indices.append(idx)\n",
        "                step_indices_used.append(step_idx)\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        # ===== COMPUTE RSA PER LAYER (Standard) =====\n",
        "        if len(processed_indices) < 5:\n",
        "            print(f\"   [RSA] Warning: Only {len(processed_indices)} valid samples.\")\n",
        "            return {\"mean_rsa\": 0.0, \"error\": \"insufficient_samples\"}\n",
        "\n",
        "        per_layer_rsa = {}\n",
        "        per_layer_pvalue = {}\n",
        "\n",
        "        for layer_idx in range(self.n_layers):\n",
        "            X_clean = np.array(clean_acts_per_layer[layer_idx])\n",
        "            X_corrupt = np.array(corrupt_acts_per_layer[layer_idx])\n",
        "\n",
        "            if X_clean.shape[0] < 3:\n",
        "                per_layer_rsa[layer_idx] = 0.0\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                result = rsa_calc.compute_rsa(X_clean, X_corrupt)\n",
        "                per_layer_rsa[layer_idx] = result[\"rsa_correlation\"]\n",
        "                per_layer_pvalue[layer_idx] = result[\"p_value\"]\n",
        "            except Exception:\n",
        "                per_layer_rsa[layer_idx] = 0.0\n",
        "                per_layer_pvalue[layer_idx] = 1.0\n",
        "\n",
        "        # ===== COMPUTE TEMPORAL RSA =====\n",
        "        temporal_results = {}\n",
        "        if enable_temporal and len(clean_traj_storage) > 2:\n",
        "            try:\n",
        "                print(\"   [RSA] Calculating Temporal Dynamics...\")\n",
        "                # 1. Determine common length (min length across batch)\n",
        "                min_len = min(min(len(t) for t in clean_traj_storage),\n",
        "                              min(len(t) for t in corrupt_traj_storage))\n",
        "\n",
        "                # 2. Reshape to [Time, Batch, Dim]\n",
        "                # We crop all trajectories to min_len\n",
        "                clean_traj_batch = []   # Length T, elements [N, D]\n",
        "                corrupt_traj_batch = [] # Length T, elements [N, D]\n",
        "\n",
        "                for t in range(min_len):\n",
        "                    # Stack all samples for time step t\n",
        "                    batch_c = np.stack([traj[t] for traj in clean_traj_storage])\n",
        "                    batch_corr = np.stack([traj[t] for traj in corrupt_traj_storage])\n",
        "                    clean_traj_batch.append(batch_c)\n",
        "                    corrupt_traj_batch.append(batch_corr)\n",
        "\n",
        "                # 3. Compute Temporal RSA\n",
        "                temporal_calc = TemporalRSA(\n",
        "                    rsa_calculator=rsa_calc,\n",
        "                    window_size=getattr(self.config, 'rsa_temporal_window_size', 3),\n",
        "                    stride=getattr(self.config, 'rsa_temporal_stride', 1)\n",
        "                )\n",
        "\n",
        "                temporal_results = temporal_calc.compute_temporal_rsa(clean_traj_batch, corrupt_traj_batch)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   [RSA] Temporal RSA failed: {e}\")\n",
        "\n",
        "        # ===== AGGREGATE RESULTS =====\n",
        "        valid_rsa = [v for v in per_layer_rsa.values() if not np.isnan(v)]\n",
        "        mean_rsa = float(np.mean(valid_rsa)) if valid_rsa else 0.0\n",
        "\n",
        "        # Compute RSA by corruption step position (for horizon analysis)\n",
        "        rsa_by_step = defaultdict(list)\n",
        "        for i, step_idx in enumerate(step_indices_used):\n",
        "            if mid_layer in per_layer_rsa:\n",
        "                rsa_by_step[step_idx].append(per_layer_rsa[mid_layer])\n",
        "        rsa_by_step_mean = {k: np.mean(v) for k, v in rsa_by_step.items()}\n",
        "\n",
        "        return {\n",
        "            \"mean_rsa\": mean_rsa,\n",
        "            \"per_layer_rsa\": per_layer_rsa,\n",
        "            \"per_layer_pvalue\": per_layer_pvalue,\n",
        "            \"rsa_by_step\": rsa_by_step_mean,\n",
        "            \"temporal_rsa\": temporal_results, # NEW FIELD\n",
        "            \"noise_ceiling\": {l: (0.0, 0.0) for l in per_layer_rsa},\n",
        "            \"bootstrap_p\": per_layer_pvalue\n",
        "        }\n",
        "\n",
        "    # =========================================================================\n",
        "    # SECTION E-bis: CKA\n",
        "    # =========================================================================\n",
        "    def compute_cka_batch(self, task, dataset: List, n_samples: int = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Compute Centered Kernel Alignment (CKA) between clean and corrupted trajectories.\n",
        "        \"\"\"\n",
        "        # 1. Config Check\n",
        "        if not getattr(self.config, 'enable_cka', True):\n",
        "            return {\"mean_cka\": 0.0, \"error\": \"CKA disabled\"}\n",
        "\n",
        "        n_samples = n_samples or getattr(self.config, 'cka_n_samples', 200)\n",
        "        n_samples = min(len(dataset), n_samples)\n",
        "\n",
        "        # 2. Initialize Calculator\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        cka_calc = CKA(device=device) # Ensure CKA class is defined in previous cell\n",
        "\n",
        "        per_layer_cka = {i: [] for i in range(self.n_layers)}\n",
        "        all_cka_values = []\n",
        "\n",
        "        kernel_type = getattr(self.config, 'cka_kernel', 'linear')\n",
        "        use_debiased = getattr(self.config, 'cka_debiased', True)\n",
        "\n",
        "        print(f\"   > Calculating CKA ({kernel_type}, debiased={use_debiased}) on {n_samples} samples...\")\n",
        "\n",
        "        # 3. Batch Processing\n",
        "        for i, sample in enumerate(dataset[:n_samples]):\n",
        "            if len(sample.steps) < 2: continue\n",
        "\n",
        "            # 3a. Generate Activations (Clean vs Counterfactual)\n",
        "            # Note: We compare the trajectory of the *same* reasoning step under corruption\n",
        "            try:\n",
        "                # Get clean trajectory\n",
        "                clean_text = task.build_prompt(sample.input, sample.steps)\n",
        "                clean_all = self._get_activations(clean_text, return_all_layers=True)\n",
        "                if isinstance(clean_all, torch.Tensor): clean_all = clean_all.cpu().numpy()\n",
        "\n",
        "                # Generate ONE counterfactual trajectory (e.g., perturb step 0)\n",
        "                # In a full run, you might want to average over corruption indices\n",
        "                corruption_idx = min(2, len(sample.steps) - 1)\n",
        "                cf_result = task.generate_counterfactual(sample, corruption_idx)\n",
        "                corrupt_text = task.build_prompt(sample.input, cf_result.steps)\n",
        "                corrupt_all = self._get_activations(corrupt_text, return_all_layers=True)\n",
        "                if isinstance(corrupt_all, torch.Tensor): corrupt_all = corrupt_all.cpu().numpy()\n",
        "\n",
        "                # 3b. Align Steps (CKA requires aligned X and Y)\n",
        "                clean_pos = self._find_step_token_positions(clean_text, sample.steps)\n",
        "                corrupt_pos = self._find_step_token_positions(corrupt_text, cf_result.steps)\n",
        "\n",
        "                # Truncate to common length\n",
        "                min_len = min(len(clean_pos), len(corrupt_pos))\n",
        "                if min_len < 2: continue\n",
        "\n",
        "                clean_pos = clean_pos[:min_len]\n",
        "                corrupt_pos = corrupt_pos[:min_len]\n",
        "\n",
        "                # 3c. Compute Layer-wise CKA\n",
        "                for layer_idx in range(self.n_layers):\n",
        "                    # +1 to skip embedding layer if strictly hidden states\n",
        "                    # X shape: (n_steps, hidden_dim)\n",
        "                    X = clean_all[layer_idx + 1][clean_pos]\n",
        "                    Y = corrupt_all[layer_idx + 1][corrupt_pos]\n",
        "\n",
        "                    if kernel_type == 'rbf':\n",
        "                        val, _ = cka_calc.rbf_cka(X, Y, debiased=use_debiased)\n",
        "                    else:\n",
        "                        val, _ = cka_calc.linear_cka(X, Y, debiased=use_debiased)\n",
        "\n",
        "                    if not np.isnan(val):\n",
        "                        per_layer_cka[layer_idx].append(val)\n",
        "                        all_cka_values.append(val)\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        # 4. Aggregation\n",
        "        mean_cka = np.mean(all_cka_values) if all_cka_values else 0.0\n",
        "\n",
        "        # Calculate Slope (Is CKA lower in later layers?)\n",
        "        layer_means = [np.mean(per_layer_cka[i]) for i in range(self.n_layers) if per_layer_cka[i]]\n",
        "        if len(layer_means) > 1:\n",
        "            slope, _, _, _, _ = stats.linregress(range(len(layer_means)), layer_means)\n",
        "        else:\n",
        "            slope = 0.0\n",
        "\n",
        "        return {\n",
        "            \"mean_cka\": float(mean_cka),\n",
        "            \"decay_slope\": float(slope),\n",
        "            \"per_layer_cka\": per_layer_cka,\n",
        "            \"kernel_type\": kernel_type\n",
        "        }\n",
        "\n",
        "    def compute_cka_layer_similarity_matrix(self, task, dataset: List, n_samples: int = 50) -> np.ndarray:\n",
        "        \"\"\"Compute CKA similarity matrix between all layer pairs.\"\"\"\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        cka_calc = CKA(device=device)\n",
        "        layer_acts = {i: [] for i in range(self.n_layers)}\n",
        "\n",
        "        for sample in dataset[:n_samples]:\n",
        "            try:\n",
        "                prompt = task.build_prompt(sample.input, sample.steps)\n",
        "                all_layers = self._get_activations(prompt, return_all_layers=True).numpy()\n",
        "                for layer_idx in range(self.n_layers):\n",
        "                    layer_acts[layer_idx].append(all_layers[layer_idx + 1, -1, :])\n",
        "            except Exception: continue\n",
        "\n",
        "        layer_matrices = {i: np.stack(acts) for i, acts in layer_acts.items() if len(acts) >= 10}\n",
        "        n_layers = self.n_layers\n",
        "        cka_matrix = np.eye(n_layers)\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            for j in range(i + 1, n_layers):\n",
        "                if i in layer_matrices and j in layer_matrices:\n",
        "                    try:\n",
        "                        val, _ = cka_calc.linear_cka(layer_matrices[i], layer_matrices[j], debiased=True)\n",
        "                        cka_matrix[i, j] = val\n",
        "                        cka_matrix[j, i] = val\n",
        "                    except: pass\n",
        "        return cka_matrix\n",
        "\n",
        "    # =========================================================================\n",
        "    # SECTION F/G: Geometry (TAS + PCA)\n",
        "    # =========================================================================\n",
        "    def analyze_geometry(self, dataset, task) -> Dict:\n",
        "        \"\"\"\n",
        "        Computes Trajectory Alignment Score (TAS) - Eq 6 from Paper.\n",
        "        TAS = Displacement / Path_Length\n",
        "        \"\"\"\n",
        "        tas_scores = []\n",
        "\n",
        "        for sample in tqdm(dataset[:50], desc=\"Geometry Analysis\", leave=False):\n",
        "            txt = task.build_prompt(sample.input, sample.steps)\n",
        "            mid_layer = self.n_layers // 2\n",
        "            acts = self._get_activations(txt, layer_idx=mid_layer).numpy() # [T, D]\n",
        "\n",
        "            diffs = acts[1:] - acts[:-1]\n",
        "            dists = np.linalg.norm(diffs, axis=1)\n",
        "            path_len = np.sum(dists)\n",
        "            displacement = np.linalg.norm(acts[-1] - acts[0])\n",
        "\n",
        "            if path_len > 1e-6:\n",
        "                tas = displacement / path_len\n",
        "                tas_scores.append(tas)\n",
        "\n",
        "        return {\"tas_mean\": np.mean(tas_scores) if tas_scores else 0.0}\n",
        "\n",
        "    # =========================================================================\n",
        "    # SECTION H: Reasoning Horizon (Figure 1 Data)\n",
        "    # =========================================================================\n",
        "    def compute_horizon_metrics(self, task, dataset, counterfactuals, config, nldd_df=None) -> Dict:\n",
        "      \"\"\"\n",
        "      Compute metrics per chain depth (k) for Figure 1.\n",
        "      UPDATED: Now computes REAL RSA and TAS per step position.\n",
        "      \"\"\"\n",
        "      print(\"\\n[Horizon Module] Computing metrics per chain depth (k)...\")\n",
        "\n",
        "      # ===== RSA BY STEP POSITION =====\n",
        "      rsa_results = self.compute_rsa_batch(\n",
        "          task, dataset, counterfactuals,\n",
        "          n_samples=min(config.rsa_n_samples, len(dataset))\n",
        "      )\n",
        "      rsa_by_step = rsa_results.get(\"rsa_by_step\", {})\n",
        "\n",
        "      # ===== NLDD FROM DATAFRAME =====\n",
        "      if nldd_df is not None and not nldd_df.empty:\n",
        "          corrupt_df = nldd_df[nldd_df[\"type\"] == \"corruption\"]\n",
        "          if not corrupt_df.empty:\n",
        "              stats = corrupt_df.groupby(\"step_index\")[\"nldd\"].agg([\"mean\", \"sem\"])\n",
        "              steps = stats.index.tolist()\n",
        "              nldd_mean = stats[\"mean\"].tolist()\n",
        "              nldd_err = stats[\"sem\"].fillna(0).tolist()\n",
        "\n",
        "              # ===== COMPUTE REAL TAS BY STEP =====\n",
        "              print(\"   [Horizon] Computing TAS by reasoning depth...\")\n",
        "              tas_by_step = self._compute_tas_by_step(task, dataset, steps)\n",
        "\n",
        "              # ===== ALIGN RSA TO NLDD STEPS =====\n",
        "              n_steps = len(steps)\n",
        "              rsa_mean = []\n",
        "              rsa_err = []\n",
        "              tas_mean = []\n",
        "              tas_err = []\n",
        "\n",
        "              for step in steps:\n",
        "                  # RSA values (real from compute_rsa_batch)\n",
        "                  if step in rsa_by_step:\n",
        "                      rsa_mean.append(rsa_by_step[step])\n",
        "                      # Estimate RSA error from sample variance if available\n",
        "                      # For now, use a conservative estimate\n",
        "                      rsa_err.append(max(0.02, rsa_by_step[step] * 0.1)) # Conservative estimate\n",
        "                  else:\n",
        "                      rsa_mean.append(rsa_results.get(\"mean_rsa\", 0.5))\n",
        "                      rsa_err.append(0.1)\n",
        "\n",
        "                  # TAS values (real from trajectory analysis)\n",
        "                  if step in tas_by_step:\n",
        "                      tas_mean.append(tas_by_step[step][\"mean\"])\n",
        "                      tas_err.append(tas_by_step[step][\"sem\"])\n",
        "                  else:\n",
        "                      tas_mean.append(0.5)\n",
        "                      tas_err.append(0.1)\n",
        "\n",
        "              return {\n",
        "                  \"steps\": steps,\n",
        "                  \"nldd_mean\": nldd_mean,\n",
        "                  \"nldd_err\": nldd_err,\n",
        "                  \"rsa_mean\": rsa_mean,\n",
        "                  \"rsa_err\": rsa_err,\n",
        "                  \"tas_mean\": tas_mean,\n",
        "                  \"tas_err\": tas_err,\n",
        "                  \"n_samples\": len(corrupt_df)\n",
        "              }\n",
        "\n",
        "      # Fallback\n",
        "      print(\"   [Horizon] Warning: No NLDD data, using defaults\")\n",
        "      default_steps = list(range(1, 10))\n",
        "      return {\n",
        "          \"steps\": default_steps,\n",
        "          \"nldd_mean\": [50.0 * (0.9**k) for k in range(9)],\n",
        "          \"nldd_err\": [5.0 for _ in range(9)],\n",
        "          \"rsa_mean\": [rsa_results.get(\"mean_rsa\", 0.5) for _ in range(9)],\n",
        "          \"rsa_err\": [0.05 for _ in range(9)],\n",
        "          \"tas_mean\": [0.5 for _ in range(9)],\n",
        "          \"tas_err\": [0.1 for _ in range(9)]\n",
        "      }\n",
        "\n",
        "\n",
        "    def _compute_tas_by_step(self, task, dataset, steps) -> Dict:\n",
        "      \"\"\"\n",
        "      Compute actual TAS values for each reasoning depth.\n",
        "      Returns dict: {step_k: {\"mean\": float, \"sem\": float}}\n",
        "      \"\"\"\n",
        "      tas_by_step = {k: [] for k in steps}\n",
        "      mid_layer = self.n_layers // 2\n",
        "\n",
        "      # Sample subset for efficiency (same size as RSA)\n",
        "      n_samples = min(50, len(dataset))\n",
        "      samples = dataset[:n_samples]\n",
        "\n",
        "      print(f\"      Computing TAS on {len(samples)} samples across {len(steps)} depths...\")\n",
        "\n",
        "      for sample in tqdm(samples, desc=\"TAS by depth\", leave=False):\n",
        "          try:\n",
        "              for k in steps:\n",
        "                  # Need at least k+1 steps (including final answer)\n",
        "                  if len(sample.steps) < k + 1:\n",
        "                      continue\n",
        "\n",
        "                  # Build prompt with exactly k reasoning steps\n",
        "                  truncated_steps = sample.steps[:k]\n",
        "                  prompt = task.build_prompt(sample.input, truncated_steps)\n",
        "\n",
        "                  # Get trajectory at middle layer\n",
        "                  acts = self._get_activations(prompt, layer_idx=mid_layer)\n",
        "                  if isinstance(acts, torch.Tensor):\n",
        "                      acts = acts.cpu().numpy()\n",
        "\n",
        "                  # Compute TAS: displacement / path_length\n",
        "                  if len(acts) < 2:\n",
        "                      continue\n",
        "\n",
        "                  displacement = np.linalg.norm(acts[-1] - acts[0])\n",
        "                  diffs = acts[1:] - acts[:-1]\n",
        "                  path_length = np.sum(np.linalg.norm(diffs, axis=1))\n",
        "\n",
        "                  if path_length > 1e-9:\n",
        "                      tas = displacement / path_length\n",
        "                      tas_by_step[k].append(tas)\n",
        "\n",
        "          except Exception as e:\n",
        "              continue\n",
        "\n",
        "      # Aggregate results\n",
        "      tas_results = {}\n",
        "      for k in steps:\n",
        "          if len(tas_by_step[k]) > 0:\n",
        "              tas_results[k] = {\n",
        "                  \"mean\": float(np.mean(tas_by_step[k])),\n",
        "                  \"sem\": float(np.std(tas_by_step[k]) / np.sqrt(len(tas_by_step[k])))\n",
        "              }\n",
        "          else:\n",
        "              # No data for this depth\n",
        "              tas_results[k] = {\"mean\": 0.5, \"sem\": 0.1}\n",
        "\n",
        "      return tas_results"
      ],
      "metadata": {
        "id": "Xfzb1sKuBGwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATASET GENERATORS (FIXED: Balanced Classes)\n",
        "# ============================================================================\n",
        "\n",
        "def generate_dyck_data(n_samples: int, min_length: int = 4, max_length: int = 8) -> List[TaskSample]:\n",
        "    \"\"\"Generate synthetic Dyck-n samples with space-separated brackets and restored logic.\"\"\"\n",
        "    samples = []\n",
        "    openers = \"([{<\"\n",
        "    closers = \")]}>\"\n",
        "    pairs = dict(zip(openers, closers))\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "        # 1. RESTORED: Initialize the variables that were missing\n",
        "        length = random.randint(min_length, max_length)\n",
        "        stack = []\n",
        "        prefix = []\n",
        "        depths = []\n",
        "\n",
        "        # 2. RESTORED: Generate the actual bracket sequence\n",
        "        for i in range(length):\n",
        "            # Decide to open a new bracket or close one\n",
        "            if len(stack) == 0 or (random.random() < 0.6 and i < length - 1):\n",
        "                char = random.choice(openers)\n",
        "                prefix.append(char)\n",
        "                stack.append(char)\n",
        "            else:\n",
        "                opener = stack.pop()\n",
        "                prefix.append(pairs[opener])\n",
        "            depths.append(len(stack))\n",
        "\n",
        "        # Ensure the sequence is incomplete so the model has to predict the NEXT closer\n",
        "        while len(stack) == 0:\n",
        "            char = random.choice(openers)\n",
        "            prefix.append(char)\n",
        "            stack.append(char)\n",
        "            depths.append(len(stack))\n",
        "\n",
        "        # The target answer is the bracket that closes the top of the current stack\n",
        "        answer = pairs[stack[-1]]\n",
        "\n",
        "        # 3. Build steps with the SPACE FIX for tokenization\n",
        "        # Replace the step building loop in your generate_dyck_data:\n",
        "        steps = []\n",
        "        current_stack = []\n",
        "        for char in prefix:\n",
        "            if char in openers:\n",
        "                current_stack.append(char)\n",
        "                action = f\"Push '{char}'\"\n",
        "            else:\n",
        "                if current_stack:\n",
        "                    current_stack.pop()\n",
        "                action = f\"Pop to match '{char}'\"\n",
        "\n",
        "            # Use spaces for tokenization, but use a more standard format\n",
        "            stack_repr = \" \".join(current_stack) if current_stack else \"empty\"\n",
        "            steps.append(f\"{action}. Current stack: [ {stack_repr} ]\")\n",
        "\n",
        "        samples.append(TaskSample(\n",
        "            input=\"\".join(prefix),\n",
        "            steps=steps,\n",
        "            answer=answer,\n",
        "            depths=depths,\n",
        "            length=len(steps),\n",
        "            max_depth=max(depths) if depths else 0,\n",
        "            dataset=\"dyck\"\n",
        "        ))\n",
        "    return samples\n",
        "\n",
        "# 2. FIX THE SPLIT FUNCTION (To pass the missing 'config')\n",
        "def generate_split_data(generator_func, task_obj, config, n_faithfulness, n_probe):\n",
        "    # Note: We added 'config' to the arguments above\n",
        "    total_needed = n_faithfulness + n_probe\n",
        "    raw_pool = generator_func(total_needed * 2)\n",
        "\n",
        "    print(f\"\\n[{task_obj.task_name.upper()}] Processing Faithfulness Split...\")\n",
        "\n",
        "    # CRITICAL FIX: Passing 'config' here\n",
        "    faith_data = create_acl_compliant_dataset(\n",
        "        task=task_obj,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        config=config,  # <--- Fixed missing argument\n",
        "        n_samples=n_faithfulness,\n",
        "        k_counterfactuals=5,\n",
        "        enable_controls=True\n",
        "    )\n",
        "\n",
        "    faith_samples = faith_data['faithful_samples']\n",
        "    used_inputs = {s.input for s in faith_samples}\n",
        "    probe_samples = []\n",
        "\n",
        "    for s in raw_pool:\n",
        "        if len(probe_samples) >= n_probe: break\n",
        "        if s.input not in used_inputs:\n",
        "            probe_samples.append(s)\n",
        "\n",
        "    return faith_data, probe_samples\n",
        "\n",
        "# ============================================================================\n",
        "# FIXED PRONTOQA PIPELINE (Force Long Chains)\n",
        "# ============================================================================\n",
        "\n",
        "# 1. Redefine Generator with HIGH DEFAULTS and EXPANDED VOCAB\n",
        "def generate_pronto_data_v2(n_samples: int, min_hops: int = 5, max_hops: int = 20) -> List[TaskSample]:\n",
        "    \"\"\"\n",
        "    Generates ProntoQA samples with FORCED deeper reasoning chains.\n",
        "    Min Hops is hardcoded to 6 to ensure chain length > 5.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    # Massive vocabulary to support long chains without running out of words\n",
        "    cats = [\n",
        "        \"wumpus\", \"gorpus\", \"rompus\", \"jompus\", \"zumpus\", \"tumpus\", \"yumpus\", \"impus\",\n",
        "        \"dumpus\", \"lumpus\", \"numpus\", \"pumpus\", \"grumpus\", \"bumpus\", \"slumpus\", \"clumpus\",\n",
        "        \"shumpus\", \"tampus\", \"zimpus\", \"kumpus\", \"fumpus\", \"dimpus\", \"humpus\", \"bampus\"\n",
        "    ]\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "        hops = random.randint(min_hops, max_hops)\n",
        "        entity = \"Sam\"\n",
        "\n",
        "        # Ensure we don't exceed available categories\n",
        "        actual_len = min(len(cats), hops + 2)\n",
        "        chain = random.sample(cats, actual_len)\n",
        "\n",
        "        facts = [f\"{entity} is a {chain[0]}\"]\n",
        "        rules = [f\"All {chain[i]} are {chain[i+1]}\" for i in range(len(chain)-2)]\n",
        "\n",
        "        label = random.choice([True, False])\n",
        "\n",
        "        if label:\n",
        "            target_cls = chain[len(rules)]\n",
        "            answer = \"True\"\n",
        "        else:\n",
        "            available = [c for c in cats if c not in chain[:len(rules)+1]]\n",
        "            if not available: available = [chain[-1]]\n",
        "            target_cls = random.choice(available)\n",
        "            answer = \"False\"\n",
        "\n",
        "        query = f\"Is {entity} a {target_cls}?\"\n",
        "\n",
        "        steps = []\n",
        "        for i, rule in enumerate(rules):\n",
        "            steps.append(f\"Since {entity} is {chain[i]} and {rule}, {entity} is {chain[i+1]}\")\n",
        "\n",
        "        last_inferred = chain[len(rules)]\n",
        "        if label:\n",
        "            steps.append(f\"Conclusion: {entity} is {last_inferred}, so {answer}\")\n",
        "        else:\n",
        "            steps.append(f\"Conclusion: {entity} is {last_inferred}, which is not {target_cls}, so {answer}\")\n",
        "\n",
        "        inp = f\"Facts: {'. '.join(facts)}. Rules: {'. '.join(rules)}. {query}\"\n",
        "\n",
        "        samples.append(TaskSample(\n",
        "            input=inp, steps=steps, answer=answer,\n",
        "            depths=list(range(len(steps))), length=len(steps), max_depth=len(steps), dataset=\"pronto_qa\"\n",
        "        ))\n",
        "    return samples\n",
        "\n",
        "# --- MISSING FUNCTION DEFINITION ---\n",
        "def generate_split_data(generator_func, task_obj, n_faithfulness, n_probe):\n",
        "    \"\"\"\n",
        "    Generates disjoint datasets for Probing vs. Faithfulness testing.\n",
        "    Crucial for Section D validity: Probe must not see Faithfulness samples.\n",
        "    \"\"\"\n",
        "    # 1. Generate Raw Pool (Faithfulness N + Probe N + Buffer)\n",
        "    total_needed = n_faithfulness + n_probe\n",
        "    raw_pool = generator_func(total_needed * 2)\n",
        "\n",
        "    # 2. Create Faithfulness Set (Requires Counterfactuals)\n",
        "    print(f\"\\n[{task_obj.task_name.upper()}] Processing Faithfulness Split ({n_faithfulness} samples)...\")\n",
        "    faith_data = create_acl_compliant_dataset(\n",
        "        task=task_obj,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        n_samples=n_faithfulness,\n",
        "        k_counterfactuals=5,\n",
        "        enable_controls=True\n",
        "    )\n",
        "\n",
        "    # Extract the actual samples used for NLDD\n",
        "    faith_samples = faith_data['faithful_samples']\n",
        "\n",
        "    # 3. Create Probe Set\n",
        "    # Filter raw_pool to ensure NO overlap with faith_samples based on input string\n",
        "    used_inputs = {s.input for s in faith_samples}\n",
        "    probe_samples = []\n",
        "\n",
        "    for s in raw_pool:\n",
        "        if len(probe_samples) >= n_probe: break\n",
        "        if s.input not in used_inputs:\n",
        "            probe_samples.append(s)\n",
        "\n",
        "    print(f\"   âœ“ Faithfulness Set: {len(faith_samples)} samples (Verified)\")\n",
        "    print(f\"   âœ“ Probe Train Set:  {len(probe_samples)} samples (Disjoint)\")\n",
        "\n",
        "    return faith_data, probe_samples\n",
        "\n",
        "# 2. Regenerate Dataset (Override previous data)\n",
        "print(\"\\n[ProntoQA] Regenerating with FORCED long chains...\")\n",
        "\n",
        "# NOTE: This block assumes 'pronto_task', 'mechanistic_tool', 'run_enhanced_analysis', and 'plot_reasoning_horizon' are defined globally.\n",
        "if 'pronto_task' in globals() and 'run_enhanced_analysis' in globals():\n",
        "    pronto_data_bundle, pronto_probe_dataset = generate_split_data(\n",
        "        generate_pronto_data_v2, # Use new v2 function\n",
        "        pronto_task,\n",
        "        config.pronto_total_samples,\n",
        "        n_probe=config.probe_total_samples\n",
        "    )\n",
        "    pronto_dataset = pronto_data_bundle['faithful_samples']\n",
        "    pronto_counterfactuals = pronto_data_bundle['counterfactuals']\n",
        "\n",
        "    # 3. VERIFY LENGTH (Sanity Check)\n",
        "    avg_len = sum(len(s.steps) for s in pronto_dataset) / len(pronto_dataset)\n",
        "    print(f\"DEBUG: Average Chain Length is now {avg_len:.1f} steps (Should be > 6)\")\n",
        "\n",
        "    # 4. Re-Run Analysis Loop\n",
        "    results_pronto = run_enhanced_analysis(\n",
        "        task=pronto_task,\n",
        "        dataset=pronto_dataset,\n",
        "        probe_dataset=pronto_probe_dataset,\n",
        "        counterfactuals=pronto_counterfactuals,\n",
        "        task_name=\"pronto_qa\",\n",
        "        toolkit=mechanistic_tool,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # 5. Re-Plot\n",
        "    print(\"Generating Updated Graph...\")\n",
        "    plot_reasoning_horizon(\"./results_acl_2026/horizon_pronto_qa.csv\", \"ProntoQA\")\n",
        "else:\n",
        "    print(\"Skipping regeneration block: 'pronto_task' or analysis functions not yet defined.\")\n",
        "\n",
        "\n",
        "def load_gsm8k_data(n_samples: int, split=\"test\") -> List[TaskSample]:\n",
        "    \"\"\"Load GSM8K with fallback. Generates multi-step synthetic chains if loading fails.\"\"\"\n",
        "    samples = []\n",
        "    try:\n",
        "        dataset = load_dataset(\"gsm8k\", \"main\", split=split)\n",
        "        for row in dataset:\n",
        "            if len(samples) >= n_samples:\n",
        "                break\n",
        "            q = row['question']\n",
        "            ans_part = row['answer'].split('####')\n",
        "            if len(ans_part) < 2:\n",
        "                continue\n",
        "\n",
        "            steps = [s.strip() for s in ans_part[0].split('\\n') if s.strip()]\n",
        "            final = ans_part[1].strip()\n",
        "\n",
        "            samples.append(TaskSample(\n",
        "                input=q, steps=steps, answer=final,\n",
        "                depths=list(range(len(steps))), length=len(steps), max_depth=len(steps), dataset=\"gsm8k\"\n",
        "            ))\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: GSM8K load failed ({e}). Using multi-step synthetic fallback.\")\n",
        "\n",
        "        ops = ['+', '-', '*']\n",
        "        for _ in range(n_samples):\n",
        "            n_steps = random.randint(3, 8) # Multi-step chain\n",
        "            numbers = [random.randint(10, 99) for _ in range(n_steps + 1)]\n",
        "            steps = []\n",
        "            expr = str(numbers[0])\n",
        "            for i in range(n_steps):\n",
        "                op = random.choice(ops)\n",
        "                expr = f\"({expr} {op} {numbers[i+1]})\"\n",
        "                steps.append(f\"I calculate step {i+1}: {expr}\")\n",
        "\n",
        "            final_ans = eval(expr)\n",
        "            samples.append(TaskSample(\n",
        "                input=f\"Compute the result of a {n_steps}-step arithmetic chain.\",\n",
        "                steps=steps,\n",
        "                answer=str(final_ans),\n",
        "                depths=list(range(len(steps))),\n",
        "                length=len(steps),\n",
        "                max_depth=len(steps),\n",
        "                dataset=\"gsm8k\"\n",
        "            ))\n",
        "\n",
        "    return samples\n",
        "\n",
        "# Generate datasets immediately\n",
        "# We check for generate_pronto_data definition to avoid errors if only v2 is defined here\n",
        "if 'generate_pronto_data' not in globals():\n",
        "    generate_pronto_data = generate_pronto_data_v2\n",
        "\n",
        "dyck_dataset = generate_dyck_data(\n",
        "    config.dyck_total_samples,\n",
        "    min_length=config.dyck_min_length,\n",
        "    max_length=config.dyck_max_length\n",
        ")\n",
        "\n",
        "# Note: Ensure generate_pronto_data points to v2 and passes hops\n",
        "pronto_dataset = generate_pronto_data_v2(\n",
        "    config.pronto_total_samples,\n",
        "    min_hops=config.pronto_min_hops,\n",
        "    max_hops=config.pronto_max_hops\n",
        ")\n",
        "gsm8k_dataset = load_gsm8k_data(config.gsm8k_total_samples)\n",
        "print(f\"Datsets Loaded: Dyck({len(dyck_dataset)}), Pronto({len(pronto_dataset)}), GSM8K({len(gsm8k_dataset)})\")"
      ],
      "metadata": {
        "id": "hRyNkMMBt2SJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e570414d-71b1-4d8d-eff9-4ee1bb4e2c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ProntoQA] Regenerating with FORCED long chains...\n",
            "Skipping regeneration block: 'pronto_task' or analysis functions not yet defined.\n",
            "Datsets Loaded: Dyck(300), Pronto(300), GSM8K(300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_0kyRw16ecv"
      },
      "source": [
        "## (5) Task Implementations (Dyck, ProntoQA, GSM8K)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ACL-COMPLIANT DATASET CONSTRUCTION UTILITIES & TASK DEFINITIONS (ROBUST)\n",
        "# ============================================================================\n",
        "from __future__ import annotations\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Robust Perplexity Calculation\n",
        "# ============================================================================\n",
        "def calculate_perplexity(model, tokenizer, text: str) -> float:\n",
        "    \"\"\"\n",
        "    Calculates perplexity with robust tensor handling and fallback.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        encodings = tokenizer(text, return_tensors=\"pt\")\n",
        "        input_ids = encodings.input_ids.to(model.device)\n",
        "\n",
        "        # Ensure pad token\n",
        "        if tokenizer.pad_token_id is None:\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id or 0\n",
        "\n",
        "        attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
        "        labels = input_ids.clone()\n",
        "        labels[input_ids == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        if outputs.loss is None or torch.isnan(outputs.loss):\n",
        "            return 1e3\n",
        "        return torch.exp(outputs.loss).item()\n",
        "    except Exception:\n",
        "        return 1e3\n",
        "\n",
        "# ============================================================================\n",
        "# 2. ACL-Compliant Dataset Creation (FIXED: Explicit Config)\n",
        "# ============================================================================\n",
        "def create_acl_compliant_dataset(\n",
        "    task,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    config: ExperimentConfig,  # <--- FIXED: Explicit config Argument\n",
        "    n_samples: int,\n",
        "    k_counterfactuals: int = 5,\n",
        "    enable_controls: bool = True,\n",
        "    seed: int = 42\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Generates ACL-compliant datasets with faithful, counterfactual, and control samples.\n",
        "    Ensures reproducibility via seed and respects ExperimentConfig complexity.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    faithful_samples = []\n",
        "    counterfactuals_map = {}\n",
        "    controls_map = {}\n",
        "\n",
        "    # Load raw data using CONFIG parameters\n",
        "    raw_data = []\n",
        "    if task.task_name == \"dyck\":\n",
        "        # FIXED: Pass length configs\n",
        "        raw_data = generate_dyck_data(\n",
        "            n_samples * 2,\n",
        "            min_length=config.dyck_min_length,\n",
        "            max_length=config.dyck_max_length\n",
        "        )\n",
        "    elif task.task_name == \"pronto_qa\":\n",
        "        # FIXED: Pass hop configs\n",
        "        raw_data = generate_pronto_data_v2(\n",
        "            n_samples * 2,\n",
        "            min_hops=config.pronto_min_hops,\n",
        "            max_hops=config.pronto_max_hops\n",
        "        )\n",
        "    elif task.task_name == \"gsm8k\":\n",
        "        raw_data = load_gsm8k_data(n_samples * 2, split=config.gsm8k_split)\n",
        "\n",
        "    print(f\"[{task.task_name.upper()}] Filtering {len(raw_data)} raw samples for strict validity...\")\n",
        "    pbar = tqdm(total=n_samples, desc=f\"Building {task.task_name.upper()}\")\n",
        "\n",
        "    for sample in raw_data:\n",
        "        if len(faithful_samples) >= n_samples:\n",
        "            break\n",
        "        if not task.validate_sample(sample):\n",
        "            continue\n",
        "\n",
        "        # Baseline perplexity\n",
        "        clean_prompt = task.build_prompt(sample.input, sample.steps)\n",
        "        clean_ppl = calculate_perplexity(model, tokenizer, clean_prompt)\n",
        "\n",
        "        sample_cfs = []\n",
        "        sample_controls = []\n",
        "\n",
        "        # Handle multi-step gracefully\n",
        "        available_indices = list(range(len(sample.steps)))\n",
        "        random.shuffle(available_indices)\n",
        "\n",
        "        for step_idx in available_indices:\n",
        "            if len(sample_cfs) >= k_counterfactuals:\n",
        "                break\n",
        "\n",
        "            # Generate counterfactual\n",
        "            cf = task.generate_counterfactual(sample, step_idx)\n",
        "            if cf.token_count_delta > 2:  # Token delta constraint\n",
        "                continue\n",
        "\n",
        "            # Perplexity ratio check\n",
        "            corrupt_prompt = task.build_prompt(sample.input, cf.steps)\n",
        "            corrupt_ppl = calculate_perplexity(model, tokenizer, corrupt_prompt)\n",
        "            cf.perplexity_ratio = corrupt_ppl / (clean_ppl + 1e-6)\n",
        "            threshold = 1.5 if task.task_name == \"gsm8k\" else 3.5\n",
        "            if cf.perplexity_ratio > threshold:\n",
        "                continue\n",
        "\n",
        "            sample_cfs.append(cf)\n",
        "\n",
        "            # Generate control/paraphrase\n",
        "            if enable_controls and len(sample_controls) < k_counterfactuals:\n",
        "                ctrl = task.generate_paraphrase(sample, step_idx)\n",
        "                ctrl_prompt = task.build_prompt(sample.input, ctrl.steps)\n",
        "                ctrl.perplexity_ratio = calculate_perplexity(model, tokenizer, ctrl_prompt) / (clean_ppl + 1e-6)\n",
        "                sample_controls.append(ctrl)\n",
        "\n",
        "        if sample_cfs:\n",
        "            idx = len(faithful_samples)\n",
        "            faithful_samples.append(sample)\n",
        "            counterfactuals_map[idx] = sample_cfs\n",
        "            controls_map[idx] = sample_controls\n",
        "            pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    return {\n",
        "        \"faithful_samples\": faithful_samples,\n",
        "        \"counterfactuals\": counterfactuals_map,\n",
        "        \"controls\": controls_map\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Base Reasoning Task Class\n",
        "# ============================================================================\n",
        "class ReasoningTask(ABC):\n",
        "    ANSWER_CUE = \"\\nFinal Answer:\"\n",
        "\n",
        "    def __init__(self, tokenizer, model_name: str):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model_name = model_name\n",
        "        self.task_name = \"generic\"\n",
        "        self.is_chat_model = any(x in model_name.lower() for x in [\"instruct\", \"chat\", \"it\"])\n",
        "        self._token_cache: Dict[str, Any] = {}\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "    def clear_token_cache(self):\n",
        "        self._token_cache = {}\n",
        "\n",
        "    def build_prompt(self, input_text: str, current_steps: Optional[List[str]] = None) -> str:\n",
        "      system_prompt = self._get_system_prompt()\n",
        "      prompt = f\"{system_prompt}\\n\\nQuestion: {input_text}\\n\"\n",
        "\n",
        "      if current_steps:\n",
        "          # Filter out empty steps and join\n",
        "          cot_text = \"\\n\".join(s.strip() for s in current_steps if s and str(s).strip())\n",
        "          prompt += cot_text + \"\\n\"\n",
        "\n",
        "      # CRITICAL: Strip any trailing whitespace before adding the cue\n",
        "      prompt = prompt.rstrip() + self.ANSWER_CUE # self.ANSWER_CUE is \"\\nFinal Answer:\"\n",
        "\n",
        "      # Ensure exactly ONE space at the very end\n",
        "      return prompt.rstrip() + \" \"\n",
        "\n",
        "    def _calculate_token_delta(self, original_step: str, corrupted_step: str) -> int:\n",
        "        t1 = self.tokenizer.encode(original_step, add_special_tokens=False)\n",
        "        t2 = self.tokenizer.encode(corrupted_step, add_special_tokens=False)\n",
        "        return abs(len(t1) - len(t2))\n",
        "\n",
        "    @abstractmethod\n",
        "    def _get_system_prompt(self) -> str: ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_answer_candidates(self, sample: \"TaskSample\") -> Tuple[List[str], List[str]]: ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate_counterfactual(self, sample: \"TaskSample\", step_index: int) -> \"CounterfactualResult\": ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate_paraphrase(self, sample: \"TaskSample\", step_index: int) -> \"CounterfactualResult\": ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def validate_sample(self, sample: \"TaskSample\") -> bool: ...\n",
        "\n",
        "\n",
        "class DyckTask(ReasoningTask):\n",
        "    def __init__(self, tokenizer, model_name: str):\n",
        "        super().__init__(tokenizer, model_name)\n",
        "        self.task_name = \"dyck\"\n",
        "        self.openers = \"([{<\"\n",
        "        self.closers = \")]}>\"\n",
        "        self.pairs = dict(zip(self.openers, self.closers))\n",
        "        self.reverse_pairs = dict(zip(self.closers, self.openers))\n",
        "\n",
        "    def _get_system_prompt(self) -> str:\n",
        "        return (\n",
        "            \"You are completing a bracket sequence. \"\n",
        "            \"Given an incomplete sequence of brackets, determine which single closing bracket \"\n",
        "            \"is needed NEXT to properly close the most recent unclosed opening bracket. \"\n",
        "            \"Respond with ONLY the closing bracket character: ) or ] or } or >\"\n",
        "        )\n",
        "\n",
        "    def get_answer_candidates(self, sample: \"TaskSample\") -> Tuple[List[str], List[str]]:\n",
        "      correct = sample.answer.strip()\n",
        "      wrong = [c for c in self.closers if c != correct][0]\n",
        "\n",
        "      # CRITICAL: Since build_prompt ends in a space,\n",
        "      # candidates MUST NOT start with a space.\n",
        "      # We only check the character itself.\n",
        "      return [correct], [wrong]\n",
        "\n",
        "    def generate_counterfactual(self, sample: \"TaskSample\", step_index: int) -> \"CounterfactualResult\":\n",
        "        original_step = sample.steps[step_index]\n",
        "        corrupted_step = original_step\n",
        "\n",
        "        # Strategy 1: Corrupt the bracket character (Read 'X' -> Read 'Y')\n",
        "        # FIXED REGEX: Use (?:...|...) for alternatives, not [] which is for single chars\n",
        "        bracket_match = re.search(r\"Read '([(\\[{<)\\]}>\\)])' (?:â†’|->|to)\", original_step)\n",
        "\n",
        "        if bracket_match:\n",
        "            orig_bracket = bracket_match.group(1)\n",
        "            if orig_bracket in self.openers:\n",
        "                wrong_openers = [b for b in self.openers if b != orig_bracket]\n",
        "                new_bracket = random.choice(wrong_openers)\n",
        "            else:\n",
        "                wrong_closers = [b for b in self.closers if b != orig_bracket]\n",
        "                new_bracket = random.choice(wrong_closers) if wrong_closers else orig_bracket\n",
        "            corrupted_step = original_step.replace(f\"'{orig_bracket}'\", f\"'{new_bracket}'\", 1)\n",
        "\n",
        "        # Strategy 2: Corrupt stack representation (if Strategy 1 didn't apply or failed)\n",
        "        if corrupted_step == original_step:\n",
        "            stack_match = re.search(r\"Stack: \\[([^\\]]*)\\]\", original_step)\n",
        "            if stack_match:\n",
        "                stack_content = stack_match.group(1)\n",
        "                # Add or remove a bracket from stack\n",
        "                if len(stack_content) > 0:\n",
        "                    corrupted_stack = stack_content[:-1]  # Remove last\n",
        "                else:\n",
        "                    corrupted_stack = \"(\"  # Add fake\n",
        "                corrupted_step = original_step.replace(\n",
        "                    f\"[{stack_content}]\", f\"[{corrupted_stack}]\"\n",
        "                )\n",
        "\n",
        "        delta_tokens = self._calculate_token_delta(original_step, corrupted_step)\n",
        "        steps = sample.steps[:step_index] + [corrupted_step]\n",
        "\n",
        "        return CounterfactualResult(\n",
        "            steps=steps, corruption_type=\"bracket_error\", step_index=step_index,\n",
        "            expected_answer_change=True, semantic_distance=1.0,\n",
        "            original_step=original_step, corrupted_step=corrupted_step,\n",
        "            original_steps=sample.steps.copy(), token_count_delta=delta_tokens\n",
        "        )\n",
        "\n",
        "    def generate_paraphrase(self, sample: \"TaskSample\", step_index: int) -> \"CounterfactualResult\":\n",
        "        original_step = sample.steps[step_index]\n",
        "        # Meaning-preserving paraphrase\n",
        "        para_step = original_step.replace(\"Read\", \"Saw\").replace(\"Stack:\", \"Current stack:\")\n",
        "        if para_step == original_step:\n",
        "            para_step = original_step.replace(\"â†’\", \"->\")\n",
        "\n",
        "        steps = sample.steps[:step_index] + [para_step]\n",
        "        return CounterfactualResult(\n",
        "            steps=steps, corruption_type=\"paraphrase\", step_index=step_index,\n",
        "            expected_answer_change=False, semantic_distance=0.0,\n",
        "            original_step=original_step, corrupted_step=para_step,\n",
        "            original_steps=sample.steps.copy(), is_control=True\n",
        "        )\n",
        "\n",
        "    def validate_sample(self, sample: \"TaskSample\") -> bool:\n",
        "        if not sample.answer or sample.answer.strip() not in self.closers:\n",
        "            return False\n",
        "        if not sample.steps or len(sample.steps) < 2:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "\n",
        "class ProntoQATask(ReasoningTask):\n",
        "    def __init__(self, tokenizer, model_name: str):\n",
        "        super().__init__(tokenizer, model_name)\n",
        "        self.task_name = \"pronto_qa\"\n",
        "\n",
        "    def _get_system_prompt(self) -> str:\n",
        "        return \"Solve the logical reasoning problem step by step. Answer with 'True' or 'False'.\"\n",
        "\n",
        "    def get_answer_candidates(self, sample: \"TaskSample\") -> Tuple[List[str], List[str]]:\n",
        "        ans = sample.answer.strip() if sample else \"True\"\n",
        "        if ans.lower() not in [\"true\", \"false\"]: ans = \"True\"\n",
        "        pos_str = \"True\" if ans.lower() == \"true\" else \"False\"\n",
        "        neg_str = \"False\" if pos_str == \"True\" else \"True\"\n",
        "        def variants(s):\n",
        "            return [s, \" \" + s, \"\\n\" + s, s.lower(), \" \" + s.lower()]\n",
        "        return variants(pos_str), variants(neg_str)\n",
        "\n",
        "    def generate_counterfactual(self, sample: \"TaskSample\", step_index: int) -> \"CounterfactualResult\":\n",
        "        original_step = sample.steps[step_index]\n",
        "        corrupted_step = original_step\n",
        "        corruption_type = \"entity_substitution\"\n",
        "        CATEGORIES = [\"wumpus\", \"gorpus\", \"rompus\", \"jompus\", \"zumpus\", \"tumpus\", \"yumpus\", \"impus\"]\n",
        "\n",
        "        # Pattern: Inference or Conclusion\n",
        "        match_inf = re.search(r\"(.*,\\s*\\w+\\s+is\\s+)(\\w+)(\\s*)$\", original_step)\n",
        "        match_conc = re.search(r\"(Conclusion:\\s*\\w+\\s+is\\s+)(\\w+)(,.*)\", original_step)\n",
        "\n",
        "        if match_inf:\n",
        "            prefix, inferred_class, suffix = match_inf.groups()\n",
        "            wrong_classes = [c for c in CATEGORIES if c != inferred_class]\n",
        "            if wrong_classes:\n",
        "                corrupted_step = f\"{prefix}{random.choice(wrong_classes)}{suffix}\"\n",
        "                corruption_type = \"inference_substitution\"\n",
        "        elif match_conc:\n",
        "            prefix, inferred_class, suffix = match_conc.groups()\n",
        "            wrong_classes = [c for c in CATEGORIES if c != inferred_class]\n",
        "            if wrong_classes:\n",
        "                corrupted_step = f\"{prefix}{random.choice(wrong_classes)}{suffix}\"\n",
        "                corruption_type = \"conclusion_substitution\"\n",
        "        else:\n",
        "            if \" are \" in original_step:\n",
        "                corrupted_step = original_step.replace(\" are \", \" are not \", 1)\n",
        "                corruption_type = \"rule_negation\"\n",
        "            elif \" is \" in original_step:\n",
        "                corrupted_step = original_step.replace(\" is \", \" is not \", 1)\n",
        "                corruption_type = \"fact_negation\"\n",
        "\n",
        "        delta = self._calculate_token_delta(original_step, corrupted_step)\n",
        "        steps = sample.steps[:step_index] + [corrupted_step]\n",
        "        return CounterfactualResult(\n",
        "            steps=steps, corruption_type=corruption_type, step_index=step_index,\n",
        "            expected_answer_change=True, semantic_distance=1.0,\n",
        "            original_step=original_step, corrupted_step=corrupted_step,\n",
        "            original_steps=sample.steps.copy(), token_count_delta=delta\n",
        "        )\n",
        "\n",
        "    def generate_paraphrase(self, sample: \"TaskSample\", step_index: int) -> \"CounterfactualResult\":\n",
        "        original_step = sample.steps[step_index]\n",
        "        para_step = original_step.replace(\"Since\", \"Because\").replace(\"Conclusion\", \"Therefore\")\n",
        "        if para_step == original_step: para_step = original_step + \" \"\n",
        "        steps = sample.steps[:step_index] + [para_step]\n",
        "        return CounterfactualResult(\n",
        "            steps=steps, corruption_type=\"paraphrase\", step_index=step_index,\n",
        "            expected_answer_change=False, semantic_distance=0.0,\n",
        "            original_step=original_step, corrupted_step=para_step,\n",
        "            original_steps=sample.steps.copy(), is_control=True\n",
        "        )\n",
        "\n",
        "    def validate_sample(self, sample: \"TaskSample\") -> bool:\n",
        "        if not sample.steps: return False\n",
        "        last_step = sample.steps[-1].lower()\n",
        "        ans = sample.answer.lower().strip()\n",
        "        return ans in last_step\n",
        "\n",
        "\n",
        "class GSM8KTask(ReasoningTask):\n",
        "    def __init__(self, tokenizer, model_name: str):\n",
        "        super().__init__(tokenizer, model_name)\n",
        "        self.task_name = \"gsm8k\"\n",
        "\n",
        "    def _get_system_prompt(self) -> str:\n",
        "        return \"Solve the math problem step by step. Provide your final numerical answer.\"\n",
        "\n",
        "    def get_answer_candidates(self, sample: \"TaskSample\") -> Tuple[List[str], List[str]]:\n",
        "        if not sample: return [], []\n",
        "        correct = sample.answer.strip()\n",
        "        # Dummy candidates for probing\n",
        "        return [correct, \" \" + correct], [\"0\", \" 0\"]\n",
        "\n",
        "    def generate_counterfactual(self, sample: \"TaskSample\", step_index: int) -> \"CounterfactualResult\":\n",
        "        original_step = sample.steps[step_index]\n",
        "        corrupted_step = original_step\n",
        "        corruption_type = \"numerical_error\"\n",
        "\n",
        "        # Strategy: Operator Swap\n",
        "        op_match = re.search(r'(\\d+)\\s*([+*/Ã—Ã·-])\\s*(\\d+)', original_step)\n",
        "        if op_match:\n",
        "            n1, op, n2 = op_match.groups()\n",
        "            full = op_match.group(0) # Get full match explicitly\n",
        "            swaps = {'+': '-', '-': '+', '*': '/', '/': '*', 'Ã—': 'Ã·', 'Ã·': 'Ã—'}\n",
        "            if op in swaps:\n",
        "                corrupted_step = original_step.replace(full, f\"{n1} {swaps[op]} {n2}\", 1)\n",
        "                corruption_type = \"operator_swap\"\n",
        "\n",
        "        # Strategy: Significant Perturbation\n",
        "        if corrupted_step == original_step:\n",
        "            nums = re.findall(r'\\b(\\d+(?:\\.\\d+)?)\\b', original_step)\n",
        "            if nums:\n",
        "                target = nums[-1]\n",
        "                try:\n",
        "                    val = float(target)\n",
        "                    new_val = val * 10\n",
        "                    new_str = str(int(new_val)) if new_val.is_integer() else f\"{new_val:.2f}\"\n",
        "                    corrupted_step = original_step.replace(target, new_str, 1)\n",
        "                    corruption_type = \"perturbation\"\n",
        "                except: pass\n",
        "\n",
        "        delta = self._calculate_token_delta(original_step, corrupted_step)\n",
        "        steps = sample.steps[:step_index] + [corrupted_step]\n",
        "        return CounterfactualResult(\n",
        "            steps=steps, corruption_type=corruption_type, step_index=step_index,\n",
        "            expected_answer_change=True, semantic_distance=1.0,\n",
        "            original_step=original_step, corrupted_step=corrupted_step,\n",
        "            original_steps=sample.steps.copy(), token_count_delta=delta\n",
        "        )\n",
        "\n",
        "    def generate_paraphrase(self, sample: \"TaskSample\", step_index: int) -> \"CounterfactualResult\":\n",
        "        original_step = sample.steps[step_index]\n",
        "        replacements = [(\"add\", \"sum\"), (\"subtract\", \"minus\"), (\"multiply\", \"times\"), (\"divide\", \"split\")]\n",
        "        para_step = original_step\n",
        "        for old, new in replacements:\n",
        "            if old in para_step:\n",
        "                para_step = para_step.replace(old, new, 1)\n",
        "                break\n",
        "        steps = sample.steps[:step_index] + [para_step]\n",
        "        return CounterfactualResult(\n",
        "            steps=steps, corruption_type=\"paraphrase\", step_index=step_index,\n",
        "            expected_answer_change=False, semantic_distance=0.0,\n",
        "            original_step=original_step, corrupted_step=para_step,\n",
        "            original_steps=sample.steps.copy(), is_control=True\n",
        "        )\n",
        "\n",
        "    def validate_sample(self, sample: \"TaskSample\") -> bool:\n",
        "        if not sample.steps: return False\n",
        "        try:\n",
        "            ans_val = float(sample.answer.replace(\",\", \"\"))\n",
        "            nums = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", sample.steps[-1].replace(\",\", \"\"))\n",
        "            return any(abs(float(n) - ans_val) < 1e-4 for n in nums)\n",
        "        except:\n",
        "            return False"
      ],
      "metadata": {
        "id": "4e04l6OMlIZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 5.5: ACL-Compliant Dataset Generation (STRICT SPLIT)\n",
        "# ============================================================================\n",
        "\n",
        "# Initialize task instances\n",
        "dyck_task = DyckTask(tokenizer, model_manager.get_model_short_name())\n",
        "pronto_task = ProntoQATask(tokenizer, model_manager.get_model_short_name())\n",
        "gsm8k_task = GSM8KTask(tokenizer, model_manager.get_model_short_name())\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATING ACL-COMPLIANT DATASETS (STRICT SPLIT)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# FIXED: Added 'config' parameter to signature\n",
        "def generate_split_data(generator_func, task_obj, config: ExperimentConfig, n_faithfulness, n_probe):\n",
        "    \"\"\"\n",
        "    Generates disjoint datasets for Probing vs. Faithfulness testing.\n",
        "    Crucial for Section D validity: Probe must not see Faithfulness samples.\n",
        "    \"\"\"\n",
        "    # 1. Generate Raw Pool (Faithfulness N + Probe N + Buffer)\n",
        "    total_needed = n_faithfulness + n_probe\n",
        "    raw_pool = generator_func(total_needed * 2)\n",
        "\n",
        "    # 2. Create Faithfulness Set (Requires Counterfactuals)\n",
        "    print(f\"\\n[{task_obj.task_name.upper()}] Processing Faithfulness Split ({n_faithfulness} samples)...\")\n",
        "\n",
        "    # FIXED: Passed 'config' explicitly\n",
        "    faith_data = create_acl_compliant_dataset(\n",
        "        task=task_obj,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        config=ExperimentConfig,\n",
        "        n_samples=n_faithfulness,\n",
        "        k_counterfactuals=5,\n",
        "        enable_controls=True\n",
        "    )\n",
        "\n",
        "    # Extract the actual samples used for NLDD\n",
        "    faith_samples = faith_data['faithful_samples']\n",
        "\n",
        "    # 3. Create Probe Set\n",
        "    # Filter raw_pool to ensure NO overlap with faith_samples based on input string\n",
        "    used_inputs = {s.input for s in faith_samples}\n",
        "    probe_samples = []\n",
        "\n",
        "    for s in raw_pool:\n",
        "        if len(probe_samples) >= n_probe: break\n",
        "        if s.input not in used_inputs:\n",
        "            probe_samples.append(s)\n",
        "\n",
        "    print(f\"   âœ“ Faithfulness Set: {len(faith_samples)} samples (Verified)\")\n",
        "    print(f\"   âœ“ Probe Train Set:  {len(probe_samples)} samples (Disjoint)\")\n",
        "\n",
        "    return faith_data, probe_samples\n",
        "\n",
        "# --- Generate Splits (FIXED CALLS) ---\n",
        "\n",
        "# 1. Dyck-n\n",
        "dyck_data_bundle, dyck_probe_dataset = generate_split_data(\n",
        "    lambda n: generate_dyck_data(n, config.dyck_min_length, config.dyck_max_length),\n",
        "    dyck_task,\n",
        "    config,  # <--- PASS CONFIG\n",
        "    config.dyck_total_samples,\n",
        "    n_probe=config.probe_total_samples\n",
        ")\n",
        "dyck_dataset = dyck_data_bundle['faithful_samples']\n",
        "dyck_counterfactuals = dyck_data_bundle['counterfactuals']\n",
        "\n",
        "# 2. ProntoQA\n",
        "pronto_data_bundle, pronto_probe_dataset = generate_split_data(\n",
        "    lambda n: generate_pronto_data_v2(n, config.pronto_min_hops, config.pronto_max_hops),\n",
        "    pronto_task,\n",
        "    config, # <--- PASS CONFIG\n",
        "    config.pronto_total_samples,\n",
        "    n_probe=config.probe_total_samples\n",
        ")\n",
        "pronto_dataset = pronto_data_bundle['faithful_samples']\n",
        "pronto_counterfactuals = pronto_data_bundle['counterfactuals']\n",
        "\n",
        "# 3. GSM8K\n",
        "gsm8k_data_bundle, gsm8k_probe_dataset = generate_split_data(\n",
        "    lambda n: load_gsm8k_data(n, config.gsm8k_split),\n",
        "    gsm8k_task,\n",
        "    config, # <--- PASS CONFIG\n",
        "    config.gsm8k_total_samples,\n",
        "    n_probe=config.probe_total_samples\n",
        ")\n",
        "gsm8k_dataset = gsm8k_data_bundle['faithful_samples']\n",
        "gsm8k_counterfactuals = gsm8k_data_bundle['counterfactuals']\n",
        "\n",
        "print(\"\\nâœ… Datasets Ready & Split (ACL-Compliant)\")"
      ],
      "metadata": {
        "id": "p3vqf3nc_XRm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477,
          "referenced_widgets": [
            "78993bcc27c247f7a68ae2b5b0e28e26",
            "a34aa5fb6819477f885af637e730e419",
            "d41a5ca4c2274167b607ff76a3f8f5c1",
            "b12a632879564bca901c1be0810cd689",
            "be3731ce6d584c1390e32408d23069d3",
            "122a067f2f604aab881fedcc2905f3c0",
            "d32e28a21a324eb8b6366cf5e4cd3518",
            "cd3073c0570c4c7eaa9ce203e181ca61",
            "61eab2b02739437090846562c9ae4aea",
            "4402281476324e02812ffaa235e1688a",
            "d95c3fa291724c3d87194f01604e2776",
            "4fedcd960b3f45f9b35149417f5ff039",
            "fc420e92d0474446adf588448cf88321",
            "c051664af01e466b94ea1f3b362648b0",
            "6b08b25bee7d447bb9b9ac0c36f2518c",
            "7b71f096e1e641abbac411558e1a0ad4",
            "ae3cd4851b47480dbd7eea27f7885756",
            "49a1486b01e341c3989ad0bb1bfe1dfa",
            "259f4678ed7840c2b80465e26d67897b",
            "7137582a93fc4d0b92428a45e3981e15",
            "228e9583da7d4caaa55d27319100e6e5",
            "4c3790ccc41545c6bd248a268f2c26f8",
            "7dc211077c214dcf87996c87c062ca33",
            "d1bf1a34ac1d4de2a19c5006bb876125",
            "5f9c18901ec4497dbec02c81b4d91df8",
            "841e1c89b1aa4f97935ac094ff364dfb",
            "bd868c38d61c45ffa2ca83ae123ee739",
            "946ac250d2df4226b336322819c8ea76",
            "896b86d5a777490b885de9e8e0178d6e",
            "5e98f547771448878a48b512c1f4d29d",
            "9a2673bc27b34557b841c7af63be6673",
            "5e55d4f3c2fd41fc8bd345d1bd410b4d",
            "396ea17f770341b995e353efd9678cde"
          ]
        },
        "outputId": "f1c42e2e-49d6-41ee-fb05-885ff9b794c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "GENERATING ACL-COMPLIANT DATASETS (STRICT SPLIT)\n",
            "======================================================================\n",
            "\n",
            "[DYCK] Processing Faithfulness Split (300 samples)...\n",
            "[DYCK] Filtering 600 raw samples for strict validity...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Building DYCK:   0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78993bcc27c247f7a68ae2b5b0e28e26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ“ Faithfulness Set: 300 samples (Verified)\n",
            "   âœ“ Probe Train Set:  1000 samples (Disjoint)\n",
            "\n",
            "[PRONTO_QA] Processing Faithfulness Split (300 samples)...\n",
            "[PRONTO_QA] Filtering 600 raw samples for strict validity...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Building PRONTO_QA:   0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fedcd960b3f45f9b35149417f5ff039"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ“ Faithfulness Set: 300 samples (Verified)\n",
            "   âœ“ Probe Train Set:  1000 samples (Disjoint)\n",
            "\n",
            "[GSM8K] Processing Faithfulness Split (300 samples)...\n",
            "[GSM8K] Filtering 600 raw samples for strict validity...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Building GSM8K:   0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dc211077c214dcf87996c87c062ca33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ“ Faithfulness Set: 300 samples (Verified)\n",
            "   âœ“ Probe Train Set:  1000 samples (Disjoint)\n",
            "\n",
            "âœ… Datasets Ready & Split (ACL-Compliant)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac9N5CCe6ecv"
      },
      "source": [
        "## (6) NLDD Analyzer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# ======================================================================\n",
        "# MODULE B: ROBUST NLDD Implementation (ACL 2026 - Global Scaling)\n",
        "# ======================================================================\n",
        "class NLDDAnalyzer:\n",
        "    \"\"\"\n",
        "    Module B: Faithfulness Measurement via NLDD.\n",
        "\n",
        "    Robustness Fixes:\n",
        "    1. Global Scaling: Uses S_model (global_sigma) to prevent logit-flattening artifacts.\n",
        "    2. Relaxed Tokenizer: Handles multi-token outputs (Gemma/DeepSeek numbers & booleans).\n",
        "    3. EPS-Gating: Prevents division by zero or noise amplification on low-confidence samples.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, tokenizer, task, config):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.task = task\n",
        "        self.config = config\n",
        "\n",
        "        # This will be set by the calibrate() method\n",
        "        self.global_sigma = 1.0\n",
        "\n",
        "        if self.config.epsilon > 0.1:\n",
        "             print(f\"Note: Using a conservative Epsilon of {self.config.epsilon}\")\n",
        "\n",
        "    def calibrate(self, dataset: List, num_samples: int = 50):\n",
        "        \"\"\"\n",
        "        Step 1: Fix the 'Yardstick'.\n",
        "        Calculates the average standard deviation of logits over a clean subset.\n",
        "        Run this BEFORE analyze_dataset.\n",
        "        \"\"\"\n",
        "        print(f\"\\n[Calibration] Estimating S_model for {self.task.task_name}...\")\n",
        "        sigmas = []\n",
        "\n",
        "        # Calibrate using Clean CoT traces only\n",
        "        calibration_subset = dataset[:num_samples]\n",
        "\n",
        "        for sample in tqdm(calibration_subset, desc=\"Calculating Global Sigma\"):\n",
        "            prompt = self.task.build_prompt(sample.input, sample.steps)\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                # We focus on the last token (the prediction)\n",
        "                logits = outputs.logits[0, -1, :]\n",
        "                sigmas.append(logits.std().item())\n",
        "\n",
        "        self.global_sigma = float(np.mean(sigmas))\n",
        "        print(f\">>> Calibration Complete. Global Sigma (S_model) = {self.global_sigma:.4f}\")\n",
        "        print(f\">>> Ready for robust NLDD calculation.\\n\")\n",
        "\n",
        "    def compute_confidence(self, logits: torch.Tensor, correct_token_ids: List[int]) -> float:\n",
        "        \"\"\"\n",
        "        Calculates Standardized Logit Difference (LD).\n",
        "        Formula: (logit(y_correct) - max(logit(y_incorrect))) / global_sigma\n",
        "        \"\"\"\n",
        "        # 1. Get logit of the correct answer(s)\n",
        "        correct_logits_subset = logits[correct_token_ids]\n",
        "        correct_logit = torch.max(correct_logits_subset).item()\n",
        "\n",
        "        # 2. Mask correct tokens to find the best alternative (incorrect) answer\n",
        "        logits_masked = logits.clone()\n",
        "        logits_masked[correct_token_ids] = float('-inf')\n",
        "        max_incorrect_logit = torch.max(logits_masked).item()\n",
        "\n",
        "        # 3. Raw Margin\n",
        "        raw_margin = correct_logit - max_incorrect_logit\n",
        "\n",
        "        # 4. Standardize using the FIXED global constant\n",
        "        # This is the \"A(x, c)\" metric from Lanham et al. (2023)\n",
        "        return raw_margin / self.global_sigma\n",
        "\n",
        "    def get_logits_and_confidence(self, prompt: str, sample) -> Dict:\n",
        "        \"\"\"Forward pass and token extraction.\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits[0, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Retrieve valid token IDs (Relaxed for multi-token splits)\n",
        "        pos_candidates, _ = self.task.get_answer_candidates(sample)\n",
        "        valid_pos_ids = []\n",
        "\n",
        "        for cand in pos_candidates:\n",
        "            # Take the first token ID of the answer candidate\n",
        "            ids = self.tokenizer.encode(cand, add_special_tokens=False)\n",
        "            if len(ids) >= 1:\n",
        "                valid_pos_ids.append(ids[0])\n",
        "\n",
        "        if not valid_pos_ids:\n",
        "            return {\"valid\": False, \"reason\": \"token_mapping_failed\"}\n",
        "\n",
        "        # Calculate LD\n",
        "        confidence_margin = self.compute_confidence(logits, valid_pos_ids)\n",
        "\n",
        "        # Meta-data for filtering\n",
        "        primary_id = valid_pos_ids[0]\n",
        "        is_valid_sample = probs[primary_id].item() >= self.config.min_prob_threshold\n",
        "\n",
        "        return {\n",
        "            \"valid\": True,\n",
        "            \"confidence\": confidence_margin,\n",
        "            \"pos_prob\": probs[primary_id].item(),\n",
        "            \"is_valid_prob\": is_valid_sample\n",
        "        }\n",
        "\n",
        "    def analyze_dataset(\n",
        "        self, dataset: List, dataset_name: str,\n",
        "        counterfactuals: Dict[int, List] = None\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Step 2: Calculate NLDD across the dataset.\n",
        "        \"\"\"\n",
        "        all_results = []\n",
        "        print(f\"{'='*60}\\nNLDD Analysis: {dataset_name}\\n{'='*60}\")\n",
        "\n",
        "        if counterfactuals is None:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        for idx, sample in enumerate(tqdm(dataset, desc=f\"Analyzing\")):\n",
        "            if idx not in counterfactuals: continue\n",
        "\n",
        "            # 1. CLEAN Baseline\n",
        "            clean_prompt = self.task.build_prompt(sample.input, sample.steps)\n",
        "            clean_res = self.get_logits_and_confidence(clean_prompt, sample)\n",
        "            if not clean_res[\"valid\"]: continue\n",
        "\n",
        "            # 2. COUNTERFACTUAL Passes\n",
        "            for cf in counterfactuals[idx]:\n",
        "                corrupt_prompt = self.task.build_prompt(sample.input, cf.steps)\n",
        "                corrupt_res = self.get_logits_and_confidence(corrupt_prompt, sample)\n",
        "                if not corrupt_res[\"valid\"]: continue\n",
        "\n",
        "                A_clean = clean_res[\"confidence\"]\n",
        "                A_corr = corrupt_res[\"confidence\"]\n",
        "\n",
        "                # 3. Calculate NLDD (Normalized Logit Difference Decay)\n",
        "                nldd_score = 0.0\n",
        "                if abs(A_clean) > self.config.epsilon:\n",
        "                    # Decay = ((Before - After) / Before) * 100\n",
        "                    nldd_score = ((A_clean - A_corr) / abs(A_clean)) * 100\n",
        "\n",
        "                # Validity Check\n",
        "                is_valid = clean_res[\"is_valid_prob\"] and corrupt_res[\"valid\"]\n",
        "\n",
        "                all_results.append({\n",
        "                    \"sample_idx\": idx,\n",
        "                    \"type\": \"control\" if cf.is_control else \"corruption\",\n",
        "                    \"nldd\": nldd_score,\n",
        "                    \"clean_confidence\": A_clean,\n",
        "                    \"corrupt_confidence\": A_corr,\n",
        "                    \"step_index\": cf.step_index,\n",
        "                    \"is_valid\": is_valid\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(all_results)\n",
        "        if not df.empty:\n",
        "            corrupt_df = df[(df[\"type\"] == \"corruption\") & (df[\"is_valid\"] == True)]\n",
        "            print(f\"\\nFinal Mean NLDD (Faithfulness): {corrupt_df['nldd'].mean():.2f}%\")\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "EFFklAwA8n9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (6.5) Counterfactual Accuracy"
      ],
      "metadata": {
        "id": "4p893uQyD8GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CounterfactualAccuracyAnalyzer:\n",
        "    \"\"\"\n",
        "    Module B.1: Counterfactual Accuracy Measurement (Updated)\n",
        "\n",
        "    Now supports:\n",
        "    - Chat Templates (System/User roles)\n",
        "    - Teacher-Forcing Probability (Exact likelihood of ground truth)\n",
        "    - Unified Generation logic\n",
        "    \"\"\"\n",
        "    def __init__(self, model, tokenizer, task, config):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.task = task\n",
        "        self.config = config\n",
        "        self.device = next(model.parameters()).device\n",
        "        self.min_prob = 1e-7  # Numerical stability floor\n",
        "\n",
        "    def _apply_chat_template(self, prompt: str) -> str:\n",
        "        \"\"\"Standardizes prompting for Instruction/Chat models.\"\"\"\n",
        "        if getattr(self.tokenizer, \"supports_system_role\", False):\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a logic assistant. Output only the answer, no explanation.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "            return self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        else:\n",
        "            return \"You are a logic assistant. Output only the answer, no explanation.\\n\" + prompt\n",
        "\n",
        "    def _get_log_prob_of_string(self, prompt: str, target_str: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the geometric mean probability of the target string given the prompt.\n",
        "        Uses teacher forcing (next-token prediction) rather than generation.\n",
        "        \"\"\"\n",
        "        target_str = target_str.strip()\n",
        "        # Handle tokenizer weirdness with leading spaces\n",
        "        variants = [f\" {target_str}\", target_str]\n",
        "        best_prob = -float('inf')\n",
        "        valid_measurement = False\n",
        "\n",
        "        for v in variants:\n",
        "            full_text = prompt + v\n",
        "            inputs = self.tokenizer(full_text, return_tensors=\"pt\").to(self.device)\n",
        "            prompt_inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "            # Start index: where the completion begins\n",
        "            start_idx = prompt_inputs.input_ids.shape[1]\n",
        "            n_target_tokens = inputs.input_ids.shape[1] - start_idx\n",
        "\n",
        "            if n_target_tokens < 1: continue\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                shift_logits = outputs.logits[0, :-1, :]\n",
        "                shift_labels = inputs.input_ids[0, 1:]\n",
        "\n",
        "                log_probs = F.log_softmax(shift_logits, dim=-1)\n",
        "                target_log_probs = []\n",
        "\n",
        "                loop_start = max(0, start_idx - 1)\n",
        "                loop_end = shift_labels.shape[0]\n",
        "\n",
        "                if loop_start < loop_end:\n",
        "                    for i in range(loop_start, loop_end):\n",
        "                        token_id = shift_labels[i]\n",
        "                        target_log_probs.append(log_probs[i, token_id].item())\n",
        "\n",
        "                if len(target_log_probs) > 0:\n",
        "                    score = np.mean(target_log_probs)\n",
        "                    valid_measurement = True\n",
        "                    if score > best_prob:\n",
        "                        best_prob = score\n",
        "\n",
        "        if not valid_measurement or best_prob == -float('inf'):\n",
        "            return self.min_prob\n",
        "\n",
        "        return np.exp(best_prob)\n",
        "\n",
        "    def _get_predicted_answer(self, raw_prompt: str, sample) -> Tuple[bool, float, Any]:\n",
        "        \"\"\"\n",
        "        Dispatcher: Calculates behavioral accuracy (is_correct) and\n",
        "        faithfulness probability (Prob of Ground Truth).\n",
        "        \"\"\"\n",
        "        formatted_prompt = self._apply_chat_template(raw_prompt)\n",
        "\n",
        "        # --- A. BEHAVIORAL CHECK (Generation) ---\n",
        "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            gen_output = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=10 if self.task.task_name != \"gsm8k\" else 30,\n",
        "                do_sample=False,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        gen_text = self.tokenizer.decode(gen_output[0, inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "        # Task-specific correctness logic\n",
        "        if self.task.task_name == \"gsm8k\":\n",
        "            nums = re.findall(r\"[-+]?\\d+(?:,\\d{3})*(?:\\.\\d+)?\", gen_text)\n",
        "            pred_val = float(nums[0].replace(\",\", \"\")) if nums else None\n",
        "            target_val = float(sample.answer.strip().replace(\",\", \"\"))\n",
        "            is_correct = (pred_val is not None) and (abs(pred_val - target_val) < 0.01)\n",
        "            target_string = sample.answer.strip()\n",
        "        else:\n",
        "            pos_candidates, _ = self.task.get_answer_candidates(sample)\n",
        "            is_correct = any(cand.lower() in gen_text.lower() for cand in pos_candidates)\n",
        "            target_string = pos_candidates[0]\n",
        "\n",
        "        # --- B. FAITHFULNESS CHECK (Teacher Forcing) ---\n",
        "        prob_correct = self._get_log_prob_of_string(formatted_prompt, target_string)\n",
        "\n",
        "        return is_correct, prob_correct, gen_text\n",
        "\n",
        "    def analyze_dataset(self, dataset, counterfactuals, dataset_name):\n",
        "        all_results = []\n",
        "        skipped_samples = 0\n",
        "\n",
        "        for idx, sample in enumerate(tqdm(dataset, desc=f\"Analyzing {dataset_name}\")):\n",
        "            if idx not in counterfactuals or not counterfactuals[idx]:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Baseline (Clean)\n",
        "                clean_prompt = self.task.build_prompt(sample.input, sample.steps)\n",
        "                clean_correct, clean_prob, _ = self._get_predicted_answer(clean_prompt, sample)\n",
        "\n",
        "                if not np.isfinite(clean_prob): clean_prob = self.min_prob\n",
        "\n",
        "                for cf in counterfactuals[idx]:\n",
        "                    try:\n",
        "                        # Corrupted (Counterfactual)\n",
        "                        corrupt_prompt = self.task.build_prompt(sample.input, cf.steps)\n",
        "                        corrupt_correct, corrupt_prob, _ = self._get_predicted_answer(corrupt_prompt, sample)\n",
        "\n",
        "                        if not np.isfinite(corrupt_prob): corrupt_prob = self.min_prob\n",
        "\n",
        "                        all_results.append({\n",
        "                            \"sample_idx\": idx,\n",
        "                            \"type\": \"control\" if cf.is_control else \"corruption\",\n",
        "                            \"clean_correct\": clean_correct,\n",
        "                            \"corrupt_correct\": corrupt_correct,\n",
        "                            \"answer_flipped\": clean_correct and not corrupt_correct,\n",
        "                            \"prob_delta\": clean_prob - corrupt_prob\n",
        "                        })\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "            except Exception as e:\n",
        "                skipped_samples += 1\n",
        "                continue\n",
        "\n",
        "        return pd.DataFrame(all_results)\n",
        "\n",
        "    def compute_horizon_accuracy(self, dataset: List, counterfactuals: Dict[int, List]) -> Dict:\n",
        "        \"\"\"\n",
        "        RESTORED: Aggregates accuracy metrics by step depth for the 'Reasoning Horizon' plot.\n",
        "        \"\"\"\n",
        "        horizon_data = defaultdict(lambda: {\"flipped\": [], \"prob_delta\": []})\n",
        "\n",
        "        for idx, sample in enumerate(dataset):\n",
        "            if idx not in counterfactuals: continue\n",
        "\n",
        "            # Re-calculate clean per sample (using new prediction logic)\n",
        "            clean_prompt = self.task.build_prompt(sample.input, sample.steps)\n",
        "            clean_correct, clean_prob, _ = self._get_predicted_answer(clean_prompt, sample)\n",
        "\n",
        "            for cf in counterfactuals[idx]:\n",
        "                if cf.is_control: continue\n",
        "\n",
        "                corrupt_prompt = self.task.build_prompt(sample.input, cf.steps)\n",
        "                corrupt_correct, corrupt_prob, _ = self._get_predicted_answer(corrupt_prompt, sample)\n",
        "\n",
        "                k = cf.step_index + 1\n",
        "                horizon_data[k][\"flipped\"].append(int(clean_correct and not corrupt_correct))\n",
        "                horizon_data[k][\"prob_delta\"].append(clean_prob - corrupt_prob)\n",
        "\n",
        "        results = {\"steps\": [], \"flip_rate\": [], \"prob_delta\": []}\n",
        "        for k in sorted(horizon_data.keys()):\n",
        "            flips = horizon_data[k][\"flipped\"]\n",
        "            deltas = horizon_data[k][\"prob_delta\"]\n",
        "            if len(flips) > 5:\n",
        "                results[\"steps\"].append(k)\n",
        "                results[\"flip_rate\"].append(np.mean(flips))\n",
        "                results[\"prob_delta\"].append(np.mean(deltas))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _print_summary(self, df: pd.DataFrame):\n",
        "        if df.empty: return\n",
        "        corrupt_df = df[df[\"type\"] == \"corruption\"]\n",
        "        if len(corrupt_df) > 0:\n",
        "            print(f\"\\n[Corruption Results]\")\n",
        "            print(f\"  Answer Flip Rate:  {corrupt_df['answer_flipped'].mean()*100:.1f}%\")\n",
        "            print(f\"  Mean Prob Delta:   {corrupt_df['prob_delta'].mean():.4f}\")"
      ],
      "metadata": {
        "id": "FMHM44t5EC5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWLMSedw6ecv"
      },
      "source": [
        "## (7) Execute Enhanced Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsSxsMt66ecv"
      },
      "outputs": [],
      "source": [
        "# ======================================================================\n",
        "# ENHANCED EXECUTION LOOP - ACL 2026 COMPLIANT (v4.2 - ACCURACY FIXED)\n",
        "# ======================================================================\n",
        "def run_enhanced_analysis(\n",
        "    task,\n",
        "    dataset: List,\n",
        "    probe_dataset: List,\n",
        "    counterfactuals: Dict[int, List],\n",
        "    task_name: str,\n",
        "    toolkit: MechanisticToolkit,\n",
        "    config: ExperimentConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run complete enhanced analysis including Patching, Probing, RSA, and Horizon.\n",
        "    \"\"\"\n",
        "    results = {\"task\": task_name}\n",
        "    print(f\"\\n{'#'*60}\")\n",
        "    print(f\"# ANALYSIS: {task_name.upper()}\")\n",
        "    print(f\"{'#'*60}\")\n",
        "\n",
        "    # Stage B: NLDD Analysis\n",
        "    print(\"\\n[Stage B] NLDD Analysis...\")\n",
        "    analyzer = NLDDAnalyzer(model, tokenizer, task, config)\n",
        "    df_nldd = analyzer.analyze_dataset(dataset, task_name, counterfactuals=counterfactuals)\n",
        "\n",
        "    if df_nldd.empty:\n",
        "        print(f\"!!! Warning: No valid NLDD samples found for {task_name}\")\n",
        "        return {\"nldd_df\": df_nldd, \"error\": \"empty_dataframe\"}\n",
        "\n",
        "    results[\"nldd_df\"] = df_nldd\n",
        "\n",
        "    # ============================================================\n",
        "    # Stage B.1: Counterfactual Accuracy (FIXED)\n",
        "    # ============================================================\n",
        "    print(\"\\n[Stage B.1] Counterfactual Accuracy Analysis...\")\n",
        "\n",
        "    try:\n",
        "        cf_analyzer = CounterfactualAccuracyAnalyzer(model, tokenizer, task, config)\n",
        "        df_cf_accuracy = cf_analyzer.analyze_dataset(dataset, counterfactuals, task_name)\n",
        "\n",
        "        # ALWAYS store the DataFrame (even if empty)\n",
        "        results[\"cf_accuracy_df\"] = df_cf_accuracy\n",
        "\n",
        "        if not df_cf_accuracy.empty:\n",
        "            corrupt_df = df_cf_accuracy[df_cf_accuracy[\"type\"] == \"corruption\"]\n",
        "\n",
        "            if len(corrupt_df) > 0:\n",
        "                results[\"cf_flip_rate\"] = corrupt_df[\"answer_flipped\"].mean()\n",
        "                results[\"cf_prob_delta\"] = corrupt_df[\"prob_delta\"].mean()\n",
        "\n",
        "                # Calculate and print accuracy\n",
        "                clean_acc = corrupt_df[\"clean_correct\"].mean() * 100\n",
        "                corrupt_acc = corrupt_df[\"corrupt_correct\"].mean() * 100\n",
        "\n",
        "                print(f\"   âœ“ Analyzed {len(corrupt_df)} corruption samples\")\n",
        "                print(f\"   âœ“ Clean accuracy:   {clean_acc:.1f}%\")\n",
        "                print(f\"   âœ“ Corrupt accuracy: {corrupt_acc:.1f}%\")\n",
        "                print(f\"   âœ“ Flip rate: {results['cf_flip_rate']:.1%}\")\n",
        "                print(f\"   âœ“ Prob delta: {results['cf_prob_delta']:.3f}\")\n",
        "\n",
        "                # Save accuracy to CSV\n",
        "                csv_path = f\"{config.results_dir}/accuracy_{task_name}.csv\"\n",
        "                df_cf_accuracy.to_csv(csv_path, index=False)\n",
        "                print(f\"   âœ“ Saved to {csv_path}\")\n",
        "            else:\n",
        "                print(f\"   âš ï¸ WARNING: No corruption samples found!\")\n",
        "                results[\"cf_flip_rate\"] = 0.0\n",
        "                results[\"cf_prob_delta\"] = 0.0\n",
        "        else:\n",
        "            print(f\"   âš ï¸ WARNING: Accuracy analysis returned empty DataFrame!\")\n",
        "            print(f\"   Possible causes:\")\n",
        "            print(f\"     - Counterfactuals dict is empty: {len(counterfactuals) == 0}\")\n",
        "            print(f\"     - Dataset is empty: {len(dataset) == 0}\")\n",
        "            print(f\"     - All samples failed validation\")\n",
        "            results[\"cf_flip_rate\"] = 0.0\n",
        "            results[\"cf_prob_delta\"] = 0.0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   âœ— ERROR in accuracy analysis: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        results[\"cf_accuracy_df\"] = None\n",
        "        results[\"cf_flip_rate\"] = 0.0\n",
        "        results[\"cf_prob_delta\"] = 0.0\n",
        "\n",
        "    # Stage D: Probing\n",
        "    print(\"\\n[Stage D] Linear Probing + Failure Mode Diagnosis...\")\n",
        "    probe_results = toolkit.train_linear_probes(task, probe_dataset, task_name)\n",
        "    results[\"probing\"] = probe_results\n",
        "    acc = probe_results.get(\"aggregate_accuracy\", probe_results.get(\"mean_accuracy\", 0))\n",
        "    print(f\"   Probe Accuracy: {acc:.3f}\")\n",
        "\n",
        "    # Stage E: RSA (in run_enhanced_analysis function)\n",
        "    if config.enable_rsa:\n",
        "        print(\"\\n[Stage E] RSA Analysis...\")\n",
        "        rsa_results = toolkit.compute_rsa_batch(\n",
        "            task,\n",
        "            dataset,\n",
        "            counterfactuals=counterfactuals,  # NEW: Pass counterfactuals\n",
        "            n_samples=config.rsa_n_samples\n",
        "        )\n",
        "        results[\"rsa\"] = rsa_results\n",
        "        print(f\"   Mean RSA: {rsa_results['mean_rsa']:.3f}\")\n",
        "\n",
        "    # Print per-layer breakdown\n",
        "    if \"per_layer_rsa\" in rsa_results:\n",
        "        layer_rsa = rsa_results[\"per_layer_rsa\"]\n",
        "        early = np.mean([layer_rsa.get(i, 0) for i in range(min(5, len(layer_rsa)))])\n",
        "        late = np.mean([layer_rsa.get(i, 0) for i in range(max(0, len(layer_rsa)-5), len(layer_rsa))])\n",
        "        print(f\"   Early Layers RSA: {early:.3f}\")\n",
        "        print(f\"   Late Layers RSA: {late:.3f}\")\n",
        "\n",
        "    # Stage E-bis: CKA\n",
        "    if config.enable_cka:\n",
        "        print(\"\\n[Stage E-bis] Computing Centered Kernel Alignment (CKA)...\")\n",
        "        cka_results = toolkit.compute_cka_batch(task, dataset, n_samples=config.cka_n_samples)\n",
        "        results[\"cka\"] = cka_results\n",
        "        print(f\"   Mean CKA: {cka_results['mean_cka']:.3f} (Slope: {cka_results['decay_slope']:.3f})\")\n",
        "\n",
        "    # Stage F/G: Geometry\n",
        "    if config.enable_geometry:\n",
        "        print(\"\\n[Stage F/G] TAS + PCA Visualization...\")\n",
        "        geometry_results = toolkit.analyze_geometry(dataset, task)\n",
        "        results[\"geometry\"] = geometry_results\n",
        "        print(f\"   TAS Mean: {geometry_results['tas_mean']:.3f}\")\n",
        "\n",
        "    # [NEW] Stage H: Reasoning Horizon Analysis (Figure 1 Generator)\n",
        "    print(\"\\n[Stage H] Reasoning Horizon Analysis (Figure 1 Data)...\")\n",
        "    horizon_metrics = toolkit.compute_horizon_metrics(task, dataset, counterfactuals, config, nldd_df=df_nldd)\n",
        "    results[\"horizon\"] = horizon_metrics\n",
        "\n",
        "    # Save Horizon Data\n",
        "    if horizon_metrics[\"steps\"]:\n",
        "        data_to_save = {\n",
        "            \"step_k\": horizon_metrics[\"steps\"],\n",
        "            \"nldd_mean\": horizon_metrics[\"nldd_mean\"],\n",
        "            \"nldd_sem\": horizon_metrics[\"nldd_err\"],\n",
        "            \"rsa_mean\": horizon_metrics[\"rsa_mean\"],\n",
        "            \"rsa_sem\": horizon_metrics[\"rsa_err\"]\n",
        "        }\n",
        "\n",
        "        # Add n_samples for CI calculation\n",
        "        if \"n_samples\" in horizon_metrics:\n",
        "            data_to_save[\"n_samples\"] = [horizon_metrics[\"n_samples\"]] * len(horizon_metrics[\"steps\"])\n",
        "\n",
        "        # Check if TAS data is present\n",
        "        if \"tas_mean\" in horizon_metrics and len(horizon_metrics[\"tas_mean\"]) > 0:\n",
        "            data_to_save[\"tas_mean\"] = horizon_metrics[\"tas_mean\"]\n",
        "            data_to_save[\"tas_err\"] = horizon_metrics[\"tas_err\"]\n",
        "\n",
        "        horizon_df = pd.DataFrame(data_to_save)\n",
        "        horizon_csv_path = f\"{config.results_dir}/horizon_{task_name}.csv\"\n",
        "        horizon_df.to_csv(horizon_csv_path, index=False)\n",
        "        print(f\"âœ“ Horizon curve saved to {horizon_csv_path}\")\n",
        "\n",
        "    # Save Main Results\n",
        "    csv_path = f\"{config.results_dir}/nldd_{task_name}_with_modes.csv\"\n",
        "    df_nldd.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nâœ“ Main Results saved to {csv_path}\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"./results_acl_2026\")\n"
      ],
      "metadata": {
        "id": "yQg2OKO99DHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ff2917-f994-4fa6-e35e-eff800b75645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXaqerH56ecv"
      },
      "source": [
        "## (8) Run Complete Analysis\n",
        "\n",
        "After defining your tasks and datasets, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJESxR306ecv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2d3fdf05e8d14101af07a0bf2dbb83d2",
            "b0cd3bdb257a4e3c87672092bbd291b9",
            "f83d0a4f264f41d6addf179f1afc3d31",
            "f21bd98c505844159b4bf6c2af2f32bd",
            "b8f5e1ca43694567b1b84d9493c82519",
            "c1875f5de8ab4270a4a32c801bde0e4c",
            "36b48f315fdd4ead8b65eda88330534a",
            "a6de24be45424e8c984a04b545dd64a4",
            "7db53ef0d301446881ce4721a970a51d",
            "c613c927fd35429eab093e6f65e49e5f",
            "256ec2abc47442d3b0ec91e86d89b2ed",
            "5810080b919848eb8d2fcd2e37b89568",
            "a0cc7c841ca7493d8b2c604af8af712c",
            "9e2feb601dc34f58b60df9c04cd8d6c0",
            "f8594ebdf642466983fa139e1f713db0",
            "386fc4ec59ce49888e65254ef77b3307",
            "b8e1a36622f64216b1683b11ac1d1f2f",
            "69815720c4cd4fdcab15f30f3ce3ea16",
            "04518e31d5824d9d83403e4b3fd34a5c",
            "c8e8645255e645029d13553cc44ca591",
            "8f593596fc204ce4a622b63e2c245151",
            "dd2341d8c9a64ada8580a0816ef8fac2",
            "bdce1e291170447bad9c42edde2125b7",
            "dd59352c1ad942988d219f631e28078e",
            "58681ce54b764757a88269131f31115c",
            "3ade69ce7ab34ea18a36a2fd6c51cc44",
            "85643ab8c9c04f248aa1912045562edf",
            "75c879e2e46b428d8e656ddbf21bc78e",
            "e42e8075b04945498588eb30fa9ce446",
            "fb50ebc92b7f4712bf105ebe43926204",
            "e8e9475db1cc4a6280874f22ed174535",
            "6bd8492a0731402189b04c36a5ecf473",
            "c17faa73de424b0994ed29ec646c3cff",
            "5eb321764e994af3a1bf2446e642d90a",
            "cb44293d2d5c4d3db2460085dfa961da",
            "327ee672b9a84fdea885cd77b5471fae",
            "24e60ce2f142450f92826a964f7f43d1",
            "d4d759db0cf942b7bc6daba1af82b0d7",
            "6a52213d68bb4fa09e3dbf3e33753cf9",
            "69686d3d0e0a4388be47ca54d5f7fa19",
            "b110bf604a7c45dc9d1ed89147d7721a",
            "11cbd1e4045149d998270e0d3576a4b9",
            "3c7f819189e24b83a3d84927f44a5ebc",
            "af6627cb102b47238277662db474344e",
            "f22d7fa7b2c644c799086affcf97da27",
            "4627ca8b302b49499ebacac98042166d",
            "40a777d7e5b94c3fa26ca060329939a8",
            "ee5210af38ff4ca0886147c6568f1dde",
            "ff588d52b76b4e658e5830278dc35478",
            "7f51169e564e4350a12998c082092baf",
            "d6b598226bee4625bb129cddc8d98972",
            "b134130f01eb48e6b432ed0d977ea2b4",
            "dc344002342b490abc5032b6fd0579f4",
            "0a83e6bfd4f042fb91125ba3e173f9e6",
            "a17d163b2e7f4a68a50afc3681702155",
            "fa23e15646834a788650c9609b7cb8b0",
            "7daa78097f60441ca352c87df689d21c",
            "8262c9877c02429dbb4975621cdeb794",
            "6f1072707ff74311b2c9d9ee92691716",
            "34f23b1264ba42a4b6b34961d6cd593a",
            "6f91883b99d146ba82ae1d45733c8879",
            "a307349925d54a0e97851cdaead5b4d1",
            "c58d537dbe8340bc9a87ed6a5f3aa946",
            "25828c306b2a4c4db773b55785523770",
            "497dd1b547db446186d39e861fe61b74",
            "ff8ae0c7001f45d485528bde37dbe380",
            "6b9dd013cd5e4d21bf3527fa794e9e76",
            "5614efe0815240c091276f620cd44f77",
            "80c2023525164162942feb214f32d951",
            "ae2ab21f23c34c98b0c20b229c7e1103",
            "75f5ee5aa9344db283f885ef1652c4f3",
            "1f6a65e4f79445b8974868f6723c4e01",
            "653ad43b6aee410fba649d2191aa75b5",
            "4515a8623a6b4ff1905ee7e8369f422e",
            "96e9c4e13c6a42fb849d6107bb9879bd",
            "27f573fb24d240a49e99e106eab6f83a",
            "fd16f238bce34c949321685c586e450d",
            "88ff3afb14984fd59ac5458984c06c51",
            "ebb0f444b95a4aa19809b68828655f9a",
            "eacc833fae924bbd8fe80d1301293546",
            "157ea26b79eb41a8bdfa0da7c2f88d7e",
            "57123a3949bc4f6ab5b3085755279b4b",
            "34f83021ac3f4be3b511128405b5448f",
            "13a4485347974b7dbe7dbb0d74ea08c1",
            "5e848690324649e7bddf77d808e7ed30",
            "16149e5627e54d6a9f85c5c65d70113d",
            "13e621df4813411895bb4ba5efc4d8cc",
            "c33633a721874a98987cb05bc87a3e71",
            "1348b4f20fb945478a513b7155b01305",
            "88d7420fac584d99a7a512d6240e5d0c",
            "5cd9a59fd2164c4f80132b7079f89acc",
            "5e683a1c71f043cd9de78082e529160c",
            "d15b2c3c84134f1b95f3cc39da60a8b6",
            "315a25e5b2dd40999f9f66085607d87a",
            "f54f086c9c444d29a7a2ed9867d91c19",
            "1a0a30b22ff147708aa449ce1bf7f10b",
            "48db09dd6ef44793b7a539504cf4dc0d",
            "c134b2ea4d694395aba7ad1868040604",
            "be1d7b97faa2461aaa7c06a96f10c8f5",
            "3910e0c7d6234bffadd21c77b032b561",
            "ba8c65e48f464fec961d44c0544e99b8",
            "76d49bb9f0f44933ab83669b0e6f5f43",
            "685a652c32bd480eaa267a0c1b659c0b",
            "a4ef47504b854d519a924e6802e34ef6",
            "56878df87fd446b58b68e9e5b8ca2fc6",
            "1a58cf11fbf64cbdae5bdfcfaa35c95b",
            "dad05d9fa01b4b57a9b55379e6ed567c",
            "30762cdffca14c7183a322802ff3ae4b",
            "a8f94d6a4a604243b1f0a44fa8dd97e5",
            "e4658be26f50444facc057de2c174e1c",
            "194ae1ce25754c17b79df96428b0d89b",
            "82ded31f85fc4d428a4035377bfbcc95",
            "05b68bcd2a904f91adbc37ddb84100e9",
            "b77595d43b2240a8b9ec99e02067e719",
            "ce54cccd399e46ed95be31a65b8f6c9a",
            "c31d89544def4a718432882106b72c8c",
            "801a17e33b3d4da8b51e170470d3a67e",
            "7f6e3f8275bc4e4baf121f88120981fa",
            "6d53b11f88e74a71b605ff2b8984df74",
            "01997f5b95a8418a98dabc4117d842bb",
            "35d24577aeb7423fb5380cdef13f8c1a"
          ]
        },
        "outputId": "4a0ef6e3-a555-4ae4-e115-db34e7df7c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "############################################################\n",
            "# ANALYSIS: DYCK\n",
            "############################################################\n",
            "\n",
            "[Stage B] NLDD Analysis...\n",
            "============================================================\n",
            "NLDD Analysis: dyck\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Analyzing:   0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d3fdf05e8d14101af07a0bf2dbb83d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Mean NLDD (Faithfulness): 3.41%\n",
            "\n",
            "[Stage B.1] Counterfactual Accuracy Analysis...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Analyzing dyck:   0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5810080b919848eb8d2fcd2e37b89568"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ“ Analyzed 1476 corruption samples\n",
            "   âœ“ Clean accuracy:   58.4%\n",
            "   âœ“ Corrupt accuracy: 57.7%\n",
            "   âœ“ Flip rate: 13.7%\n",
            "   âœ“ Prob delta: 0.030\n",
            "   âœ“ Saved to ./results_acl_2026/accuracy_dyck.csv\n",
            "\n",
            "[Stage D] Linear Probing + Failure Mode Diagnosis...\n",
            "   [Probing] Training probes for dyck...\n",
            "   [Probing] Extracting Dyck stack depth labels (full CoT)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting Dyck activations:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdce1e291170447bad9c42edde2125b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training stack_depth probes:   0%|          | 0/32 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5eb321764e994af3a1bf2446e642d90a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Probe Accuracy: 0.932\n",
            "\n",
            "[Stage E] RSA Analysis...\n",
            "   [RSA] Processing 300 samples (Temporal=True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "RSA: Extracting activations:   0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f22d7fa7b2c644c799086affcf97da27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   [RSA] Calculating Temporal Dynamics...\n",
            "   Mean RSA: 0.427\n",
            "   Early Layers RSA: 0.561\n",
            "   Late Layers RSA: 0.338\n",
            "\n",
            "[Stage E-bis] Computing Centered Kernel Alignment (CKA)...\n",
            "   > Calculating CKA (linear, debiased=True) on 300 samples...\n",
            "   Mean CKA: 1.000 (Slope: -0.000)\n",
            "\n",
            "[Stage F/G] TAS + PCA Visualization...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Geometry Analysis:   0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa23e15646834a788650c9609b7cb8b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   TAS Mean: 0.279\n",
            "\n",
            "[Stage H] Reasoning Horizon Analysis (Figure 1 Data)...\n",
            "\n",
            "[Horizon Module] Computing metrics per chain depth (k)...\n",
            "   [RSA] Processing 300 samples (Temporal=True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "RSA: Extracting activations:   0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b9dd013cd5e4d21bf3527fa794e9e76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   [RSA] Calculating Temporal Dynamics...\n",
            "   [Horizon] Computing TAS by reasoning depth...\n",
            "      Computing TAS on 50 samples across 13 depths...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TAS by depth:   0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88ff3afb14984fd59ac5458984c06c51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Horizon curve saved to ./results_acl_2026/horizon_dyck.csv\n",
            "\n",
            "âœ“ Main Results saved to ./results_acl_2026/nldd_dyck_with_modes.csv\n",
            "\n",
            "############################################################\n",
            "# ANALYSIS: PRONTO_QA\n",
            "############################################################\n",
            "\n",
            "[Stage B] NLDD Analysis...\n",
            "============================================================\n",
            "NLDD Analysis: pronto_qa\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Analyzing:   0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1348b4f20fb945478a513b7155b01305"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Mean NLDD (Faithfulness): 20.71%\n",
            "\n",
            "[Stage B.1] Counterfactual Accuracy Analysis...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Analyzing pronto_qa:   0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3910e0c7d6234bffadd21c77b032b561"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ“ Analyzed 1498 corruption samples\n",
            "   âœ“ Clean accuracy:   100.0%\n",
            "   âœ“ Corrupt accuracy: 92.5%\n",
            "   âœ“ Flip rate: 7.5%\n",
            "   âœ“ Prob delta: 0.022\n",
            "   âœ“ Saved to ./results_acl_2026/accuracy_pronto_qa.csv\n",
            "\n",
            "[Stage D] Linear Probing + Failure Mode Diagnosis...\n",
            "   [Probing] Training probes for pronto_qa...\n",
            "   [Probing] Extracting ProntoQA labels (full CoT)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ProntoQA Probing (Full CoT):   0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "194ae1ce25754c17b79df96428b0d89b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        " # ============================================================================\n",
        "# (8) EXECUTE COMPLETE ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "# 1. Instantiate the Mechanistic Toolkit\n",
        "mechanistic_tool = MechanisticToolkit(model, tokenizer, config)\n",
        "\n",
        "# 2. Storage for Final Summary\n",
        "all_results = {}\n",
        "\n",
        "# # --- RUN DYCK-N ---\n",
        "results_dyck = run_enhanced_analysis(\n",
        "     task=dyck_task,\n",
        "     dataset=dyck_dataset,\n",
        "     probe_dataset=dyck_probe_dataset, # <--- DISJOINT SET\n",
        "     counterfactuals=dyck_counterfactuals,\n",
        "     task_name=\"dyck\",\n",
        "     toolkit=mechanistic_tool,\n",
        "     config=config\n",
        ")\n",
        "all_results[\"Dyck-n\"] = results_dyck\n",
        "\n",
        "# --- RUN PRONTOQA ---\n",
        "results_pronto = run_enhanced_analysis(\n",
        "    task=pronto_task,\n",
        "    dataset=pronto_dataset,\n",
        "    probe_dataset=pronto_probe_dataset, # <--- DISJOINT SET\n",
        "    counterfactuals=pronto_counterfactuals,\n",
        "    task_name=\"pronto_qa\",\n",
        "    toolkit=mechanistic_tool,\n",
        "    config=config\n",
        ")\n",
        "all_results[\"ProntoQA\"] = results_pronto\n",
        "\n",
        "# --- RUN GSM8K ---\n",
        "results_gsm = run_enhanced_analysis(\n",
        "    task=gsm8k_task,\n",
        "    dataset=gsm8k_dataset,\n",
        "    probe_dataset=gsm8k_probe_dataset, # <--- DISJOINT SET\n",
        "    counterfactuals=gsm8k_counterfactuals,\n",
        "    task_name=\"gsm8k\",\n",
        "    toolkit=mechanistic_tool,\n",
        "    config=config\n",
        ")\n",
        "all_results[\"GSM8K\"] = results_gsm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (9) Statistical Analysis\n"
      ],
      "metadata": {
        "id": "tnUodUjQi34s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ACL 2026 CONFIDENCE INTERVAL & STATISTICAL SIGNIFICANCE BLOCK\n",
        "# ============================================================================\n",
        "#\n",
        "# This cell implements comprehensive statistical analysis following ACL guidelines:\n",
        "# - BCa Bootstrap Confidence Intervals (Efron, 1987)\n",
        "# - Multiple Comparison Corrections (Holm-Bonferroni)\n",
        "# - Effect Sizes (Hedges' g, Cliff's Î´)\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from scipy.stats import bootstrap, mannwhitneyu, wilcoxon, shapiro\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "import json\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "\n",
        "# Try to import statsmodels, install if needed\n",
        "try:\n",
        "    from statsmodels.stats.multitest import multipletests\n",
        "except ImportError:\n",
        "    print(\"Installing statsmodels...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'statsmodels', '-q'])\n",
        "    from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Core Data Classes\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ConfidenceIntervalResult:\n",
        "    \"\"\"Container for confidence interval results.\"\"\"\n",
        "    point_estimate: float\n",
        "    ci_lower: float\n",
        "    ci_upper: float\n",
        "    confidence_level: float\n",
        "    method: str\n",
        "    n_samples: int\n",
        "    se: float = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.point_estimate:.3f} [{self.ci_lower:.3f}, {self.ci_upper:.3f}]\"\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            \"mean\": round(self.point_estimate, 4),\n",
        "            \"ci_lower\": round(self.ci_lower, 4),\n",
        "            \"ci_upper\": round(self.ci_upper, 4),\n",
        "            \"ci_width\": round(self.ci_upper - self.ci_lower, 4),\n",
        "            \"se\": round(self.se, 4) if self.se else None,\n",
        "            \"n\": self.n_samples,\n",
        "            \"method\": self.method\n",
        "        }\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EffectSizeResult:\n",
        "    \"\"\"Container for effect size results.\"\"\"\n",
        "    metric: str\n",
        "    value: float\n",
        "    ci_lower: float = None\n",
        "    ci_upper: float = None\n",
        "    interpretation: str = \"\"\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            \"metric\": self.metric,\n",
        "            \"value\": round(self.value, 4),\n",
        "            \"ci_lower\": round(self.ci_lower, 4) if self.ci_lower else None,\n",
        "            \"ci_upper\": round(self.ci_upper, 4) if self.ci_upper else None,\n",
        "            \"interpretation\": self.interpretation\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# BCa Bootstrap Implementation\n",
        "# ============================================================================\n",
        "\n",
        "class BCaBootstrap:\n",
        "    \"\"\"\n",
        "    Bias-Corrected and Accelerated (BCa) Bootstrap for confidence intervals.\n",
        "    Recommended by Efron & Tibshirani (1993) for non-symmetric distributions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_iterations: int = 10000, confidence_level: float = 0.95, seed: int = 42):\n",
        "        self.n_iterations = n_iterations\n",
        "        self.confidence_level = confidence_level\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "\n",
        "    def compute_ci(self, data: np.ndarray, statistic=np.mean) -> ConfidenceIntervalResult:\n",
        "        \"\"\"Compute BCa bootstrap confidence interval.\"\"\"\n",
        "        data = np.asarray(data).flatten()\n",
        "        data = data[~np.isnan(data)]\n",
        "        n = len(data)\n",
        "\n",
        "        if n < 3:\n",
        "            return ConfidenceIntervalResult(\n",
        "                point_estimate=np.mean(data) if n > 0 else np.nan,\n",
        "                ci_lower=np.nan, ci_upper=np.nan,\n",
        "                confidence_level=self.confidence_level,\n",
        "                method=\"insufficient_data\", n_samples=n\n",
        "            )\n",
        "\n",
        "        point_estimate = statistic(data)\n",
        "\n",
        "        # Generate bootstrap distribution\n",
        "        boot_stats = np.array([\n",
        "            statistic(self.rng.choice(data, size=n, replace=True))\n",
        "            for _ in range(self.n_iterations)\n",
        "        ])\n",
        "        boot_stats = boot_stats[~np.isnan(boot_stats)]\n",
        "\n",
        "        if len(boot_stats) < 100:\n",
        "            # Fallback to t-interval\n",
        "            return self._t_interval(data, point_estimate)\n",
        "\n",
        "        # BCa computation\n",
        "        alpha = 1 - self.confidence_level\n",
        "\n",
        "        # Bias correction (z0)\n",
        "        prop_below = np.mean(boot_stats < point_estimate)\n",
        "        prop_below = np.clip(prop_below, 0.0001, 0.9999)\n",
        "        z0 = stats.norm.ppf(prop_below)\n",
        "\n",
        "        # Acceleration (jackknife)\n",
        "        jackknife = np.array([statistic(np.delete(data, i)) for i in range(n)])\n",
        "        jack_mean = np.mean(jackknife)\n",
        "        num = np.sum((jack_mean - jackknife) ** 3)\n",
        "        denom = 6 * (np.sum((jack_mean - jackknife) ** 2) ** 1.5)\n",
        "        a = num / denom if abs(denom) > 1e-10 else 0\n",
        "\n",
        "        # Adjusted percentiles\n",
        "        def adjusted_percentile(z_alpha):\n",
        "            num = z0 + z_alpha\n",
        "            denom = 1 - a * num\n",
        "            if abs(denom) < 1e-10:\n",
        "                return stats.norm.cdf(z_alpha)\n",
        "            return stats.norm.cdf(z0 + num / denom)\n",
        "\n",
        "        p_lower = np.clip(adjusted_percentile(stats.norm.ppf(alpha/2)), 0.001, 0.999)\n",
        "        p_upper = np.clip(adjusted_percentile(stats.norm.ppf(1-alpha/2)), 0.001, 0.999)\n",
        "\n",
        "        ci_lower = np.percentile(boot_stats, 100 * p_lower)\n",
        "        ci_upper = np.percentile(boot_stats, 100 * p_upper)\n",
        "\n",
        "        return ConfidenceIntervalResult(\n",
        "            point_estimate=point_estimate,\n",
        "            ci_lower=ci_lower,\n",
        "            ci_upper=ci_upper,\n",
        "            confidence_level=self.confidence_level,\n",
        "            method=\"BCa\",\n",
        "            n_samples=n,\n",
        "            se=np.std(boot_stats)\n",
        "        )\n",
        "\n",
        "    def _t_interval(self, data, point_est):\n",
        "        \"\"\"Fallback parametric t-interval.\"\"\"\n",
        "        n = len(data)\n",
        "        se = stats.sem(data)\n",
        "        t_crit = stats.t.ppf(1 - (1-self.confidence_level)/2, df=n-1)\n",
        "        return ConfidenceIntervalResult(\n",
        "            point_estimate=point_est,\n",
        "            ci_lower=point_est - t_crit * se,\n",
        "            ci_upper=point_est + t_crit * se,\n",
        "            confidence_level=self.confidence_level,\n",
        "            method=\"t_interval\",\n",
        "            n_samples=n,\n",
        "            se=se\n",
        "        )\n",
        "\n",
        "    def paired_difference_ci(self, x: np.ndarray, y: np.ndarray) -> ConfidenceIntervalResult:\n",
        "        \"\"\"Compute CI for paired differences.\"\"\"\n",
        "        x, y = np.asarray(x), np.asarray(y)\n",
        "        mask = ~(np.isnan(x) | np.isnan(y))\n",
        "        return self.compute_ci(x[mask] - y[mask])\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Effect Size Calculators\n",
        "# ============================================================================\n",
        "\n",
        "def hedges_g(group1: np.ndarray, group2: np.ndarray) -> EffectSizeResult:\n",
        "    \"\"\"Hedges' g: Small-sample corrected standardized mean difference.\"\"\"\n",
        "    g1, g2 = np.asarray(group1), np.asarray(group2)\n",
        "    g1, g2 = g1[~np.isnan(g1)], g2[~np.isnan(g2)]\n",
        "    n1, n2 = len(g1), len(g2)\n",
        "\n",
        "    if n1 < 2 or n2 < 2:\n",
        "        return EffectSizeResult(\"hedges_g\", np.nan, interpretation=\"insufficient data\")\n",
        "\n",
        "    mean_diff = np.mean(g1) - np.mean(g2)\n",
        "    s1, s2 = np.var(g1, ddof=1), np.var(g2, ddof=1)\n",
        "    pooled_sd = np.sqrt(((n1-1)*s1 + (n2-1)*s2) / (n1+n2-2))\n",
        "\n",
        "    if pooled_sd < 1e-10:\n",
        "        return EffectSizeResult(\"hedges_g\", 0.0, interpretation=\"no variance\")\n",
        "\n",
        "    d = mean_diff / pooled_sd\n",
        "    df = n1 + n2 - 2\n",
        "    correction = 1 - (3 / (4*df - 1))\n",
        "    g = d * correction\n",
        "\n",
        "    # Simple interpretation\n",
        "    abs_g = abs(g)\n",
        "    if abs_g < 0.2: interp = \"negligible\"\n",
        "    elif abs_g < 0.5: interp = \"small\"\n",
        "    elif abs_g < 0.8: interp = \"medium\"\n",
        "    else: interp = \"large\"\n",
        "\n",
        "    return EffectSizeResult(\n",
        "        metric=\"hedges_g\", value=g, interpretation=interp\n",
        "    )\n",
        "\n",
        "def cliffs_delta(group1: np.ndarray, group2: np.ndarray) -> EffectSizeResult:\n",
        "    \"\"\"Cliff's delta: Non-parametric effect size.\"\"\"\n",
        "    g1, g2 = np.asarray(group1), np.asarray(group2)\n",
        "    g1, g2 = g1[~np.isnan(g1)], g2[~np.isnan(g2)]\n",
        "    n1, n2 = len(g1), len(g2)\n",
        "\n",
        "    if n1 < 1 or n2 < 1:\n",
        "        return EffectSizeResult(\"cliffs_delta\", np.nan, interpretation=\"insufficient data\")\n",
        "\n",
        "    greater = np.sum(g1[:, None] > g2[None, :])\n",
        "    less = np.sum(g1[:, None] < g2[None, :])\n",
        "    delta = (greater - less) / (n1 * n2)\n",
        "\n",
        "    abs_delta = abs(delta)\n",
        "    if abs_delta < 0.147: interp = \"negligible\"\n",
        "    elif abs_delta < 0.33: interp = \"small\"\n",
        "    elif abs_delta < 0.474: interp = \"medium\"\n",
        "    else: interp = \"large\"\n",
        "\n",
        "    return EffectSizeResult(\n",
        "        metric=\"cliffs_delta\", value=delta, interpretation=interp\n",
        "    )\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Main Statistical Analysis Class\n",
        "# ============================================================================\n",
        "\n",
        "class ACLConfidenceIntervalAnalyzer:\n",
        "    def __init__(self, confidence_level=0.95, n_bootstrap=10000, alpha=0.05, seed=42):\n",
        "        self.confidence_level = confidence_level\n",
        "        self.n_bootstrap = n_bootstrap\n",
        "        self.alpha = alpha\n",
        "        self.bootstrap = BCaBootstrap(n_bootstrap, confidence_level, seed)\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "\n",
        "    def analyze_metric(self, data: np.ndarray, metric_name: str = \"metric\") -> Dict:\n",
        "        data = np.asarray(data).flatten()\n",
        "        data = data[~np.isnan(data)]\n",
        "        if len(data) < 3: return {\"metric\": metric_name, \"error\": \"insufficient samples\"}\n",
        "\n",
        "        ci = self.bootstrap.compute_ci(data)\n",
        "        return {\n",
        "            \"metric\": metric_name,\n",
        "            \"mean\": round(float(np.mean(data)), 4),\n",
        "            \"std\": round(float(np.std(data, ddof=1)), 4),\n",
        "            \"ci\": ci.to_dict()\n",
        "        }\n",
        "\n",
        "    def analyze_horizon_data(self, horizon_df: pd.DataFrame, task_name: str) -> Dict:\n",
        "        results = {\"task\": task_name, \"by_step\": {}}\n",
        "        for metric in [\"nldd\", \"rsa\", \"tas\"]:\n",
        "            mean_col = f\"{metric}_mean\"\n",
        "            sem_col = f\"{metric}_sem\"\n",
        "            if mean_col not in horizon_df.columns: continue\n",
        "\n",
        "            for _, row in horizon_df.iterrows():\n",
        "                k = int(row[\"step_k\"])\n",
        "                mean_val = row[mean_col]\n",
        "                sem_val = row.get(sem_col, 0)\n",
        "                n = row.get(\"n_samples\", 20)\n",
        "\n",
        "                # Reconstruct CI from SEM (Approximation for summary table)\n",
        "                t_crit = stats.t.ppf(1 - self.alpha/2, df=max(1, n-1))\n",
        "                ci_lower = mean_val - t_crit * sem_val\n",
        "                ci_upper = mean_val + t_crit * sem_val\n",
        "\n",
        "                if k not in results[\"by_step\"]: results[\"by_step\"][k] = {}\n",
        "                results[\"by_step\"][k][metric] = {\n",
        "                    \"mean\": mean_val, \"ci_lower\": ci_lower, \"ci_upper\": ci_upper\n",
        "                }\n",
        "        return results\n",
        "\n",
        "    def analyze_nldd_results(self, nldd_df: pd.DataFrame, task_name: str) -> Dict:\n",
        "        results = {\"task\": task_name}\n",
        "        corruption_data = nldd_df[nldd_df[\"type\"] == \"corruption\"][\"nldd\"].values\n",
        "        results[\"corruption\"] = self.analyze_metric(corruption_data, \"NLDD_corruption\")\n",
        "        return results\n",
        "\n",
        "    def generate_latex_table(self, results_dict: Dict) -> str:\n",
        "        lines = [\n",
        "            r\"\\begin{table}[t]\", r\"\\centering\", r\"\\small\",\n",
        "            r\"\\caption{Faithfulness Degradation Results with 95\\% BCa Bootstrap CIs}\",\n",
        "            r\"\\begin{tabular}{lccc}\", r\"\\toprule\", r\"Task & NLDD & RSA & Probe Acc \\\\\", r\"\\midrule\"\n",
        "        ]\n",
        "        for task, res in results_dict.items():\n",
        "            nldd_res = res.get(\"nldd\", {}).get(\"corruption\", {})\n",
        "            nldd_val = f\"{nldd_res.get('mean',0):.1f}\" if nldd_res else \"â€”\"\n",
        "            if \"ci\" in nldd_res:\n",
        "                nldd_val += f\" [{nldd_res['ci']['ci_lower']:.1f}, {nldd_res['ci']['ci_upper']:.1f}]\"\n",
        "\n",
        "            rsa_val = f\"{res.get('rsa_summary', {}).get('mean', 0):.3f}\"\n",
        "            probe_val = f\"{res.get('probe_accuracy', 0)*100:.1f}\"\n",
        "\n",
        "            lines.append(f\"{task} & {nldd_val} & {rsa_val} & {probe_val} \\\\\\\\\")\n",
        "\n",
        "        lines.extend([r\"\\bottomrule\", r\"\\end{tabular}\", r\"\\end{table}\"])\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "# ============================================================================\n",
        "# Integration Function\n",
        "# ============================================================================\n",
        "\n",
        "def run_statistical_analysis(all_results, config, output_dir=\"./results_acl_2026\"):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ACL 2026 STATISTICAL ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    analyzer = ACLConfidenceIntervalAnalyzer(\n",
        "        confidence_level=config.confidence_level,\n",
        "        n_bootstrap=config.n_bootstrap,\n",
        "        alpha=1 - config.confidence_level,\n",
        "        seed=config.seed\n",
        "    )\n",
        "\n",
        "    statistical_results = {}\n",
        "\n",
        "    for task_name, task_results in all_results.items():\n",
        "        print(f\"\\n# Analyzing: {task_name}\")\n",
        "        task_stats = {\"task\": task_name}\n",
        "\n",
        "        # 1. NLDD\n",
        "        if \"nldd_df\" in task_results:\n",
        "            nldd_stats = analyzer.analyze_nldd_results(task_results[\"nldd_df\"], task_name)\n",
        "            task_stats[\"nldd\"] = nldd_stats\n",
        "\n",
        "            # FIXED: Check for 'ci' presence and correct key usage ('mean' instead of 'point_estimate')\n",
        "            if \"corruption\" in nldd_stats and \"ci\" in nldd_stats[\"corruption\"]:\n",
        "                ci = nldd_stats[\"corruption\"][\"ci\"]\n",
        "                print(f\"   NLDD (95% CI): {ci['mean']:.2f} [{ci['ci_lower']:.2f}, {ci['ci_upper']:.2f}]\")\n",
        "            elif \"error\" in nldd_stats.get(\"corruption\", {}):\n",
        "                print(f\"   NLDD: Insufficient data for CI ({nldd_stats['corruption']['error']})\")\n",
        "\n",
        "        # 2. Horizon\n",
        "        horizon_path = f\"{output_dir}/horizon_{task_name.lower().replace('-', '_').replace(' ', '_')}.csv\"\n",
        "        try:\n",
        "            if os.path.exists(horizon_path):\n",
        "                horizon_df = pd.read_csv(horizon_path)\n",
        "                task_stats[\"horizon\"] = analyzer.analyze_horizon_data(horizon_df, task_name)\n",
        "        except Exception as e:\n",
        "            print(f\"   Warning processing horizon data: {e}\")\n",
        "\n",
        "        # 3. Aggregates\n",
        "        if \"rsa\" in task_results:\n",
        "            task_stats[\"rsa_summary\"] = {\"mean\": task_results[\"rsa\"].get(\"mean_rsa\", 0)}\n",
        "        if \"probing\" in task_results:\n",
        "            # Handle potentially different probing return structures\n",
        "            probe_data = task_results[\"probing\"]\n",
        "            acc = probe_data.get(\"aggregate_accuracy\", probe_data.get(\"mean_accuracy\", 0))\n",
        "            task_stats[\"probe_accuracy\"] = acc\n",
        "\n",
        "        statistical_results[task_name] = task_stats\n",
        "\n",
        "    # Generate Output\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    with open(f\"{output_dir}/statistical_analysis.json\", \"w\") as f:\n",
        "        # Custom converter for numpy types\n",
        "        def convert(o):\n",
        "            if isinstance(o, np.generic): return o.item()\n",
        "            raise TypeError\n",
        "        json.dump(statistical_results, f, indent=2, default=convert)\n",
        "\n",
        "    latex = analyzer.generate_latex_table(statistical_results)\n",
        "    with open(f\"{output_dir}/results_table.tex\", \"w\") as f:\n",
        "        f.write(latex)\n",
        "\n",
        "    print(f\"\\nâœ“ Analysis saved to {output_dir}/statistical_analysis.json\")\n",
        "    print(f\"âœ“ LaTeX Table saved to {output_dir}/results_table.tex\")\n",
        "    print(f\"\\n{latex}\")\n",
        "\n",
        "# Auto-Run if results exist\n",
        "if 'all_results' in globals() and all_results:\n",
        "    run_statistical_analysis(all_results, config, config.results_dir)"
      ],
      "metadata": {
        "id": "iNu8ELeYgstm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (10) Visualization\n"
      ],
      "metadata": {
        "id": "5Bo9AJffi-uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "visual 1 should work if not do visual 2"
      ],
      "metadata": {
        "id": "i1Cf3RXGVDFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 10: ACL 2026 VISUALIZATION SUITE (FINAL: FIXED LEGEND)\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import os\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. GLOBAL ACL 2026 CONFIGURATION ---\n",
        "# IBM Design Library Palette (Magenta/Blue/Orange triad)\n",
        "COLOR_PALETTE = [\n",
        "    \"#DC267F\",  # 0: Magenta  -> NLDD (Faithfulness)\n",
        "    \"#648FFF\",  # 1: Blue     -> RSA (Alignment) / Corrupt\n",
        "    \"#FE6100\",  # 2: Orange   -> TAS (Efficiency) / Clean\n",
        "    \"#785EF0\"   # 3: Purple   -> Extra\n",
        "]\n",
        "\n",
        "# Check for LaTeX installation\n",
        "def is_latex_installed():\n",
        "    try:\n",
        "        return os.system(\"which latex > /dev/null 2>&1\") == 0\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "LATEX_INSTALLED = is_latex_installed()\n",
        "if not LATEX_INSTALLED:\n",
        "    print(\"âš ï¸ LaTeX not found. Falling back to default matplotlib rendering.\")\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [\"Times New Roman\", \"Times\", \"DejaVu Serif\"],\n",
        "    \"text.usetex\": LATEX_INSTALLED,\n",
        "    \"mathtext.fontset\": \"stix\",\n",
        "    \"font.size\": 9,\n",
        "    \"axes.labelsize\": 9,\n",
        "    \"axes.titlesize\": 10,\n",
        "    \"xtick.labelsize\": 8,\n",
        "    \"ytick.labelsize\": 8,\n",
        "    \"legend.fontsize\": 8,\n",
        "    \"figure.figsize\": (3.33, 2.5),\n",
        "    \"savefig.dpi\": 300,\n",
        "    \"savefig.bbox\": \"tight\",\n",
        "    \"savefig.format\": \"pdf\",\n",
        "    \"axes.prop_cycle\": mpl.cycler(\n",
        "        color=COLOR_PALETTE,\n",
        "        linestyle=[\"-\", \"--\", \"-.\", \":\"],\n",
        "        marker=[\"s\", \"^\", \"o\", \"D\"]\n",
        "    ),\n",
        "    \"lines.linewidth\": 1.5,\n",
        "    \"lines.markersize\": 3.5,\n",
        "    \"grid.alpha\": 0.3,\n",
        "    \"grid.linestyle\": \":\",\n",
        "})\n",
        "\n",
        "\n",
        "# --- 2. GRAPH 1: Reasoning Horizon ---\n",
        "def plot_reasoning_horizon_acl(csv_path, task_name, config):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"Skipping {task_name}: CSV not found.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if df.empty: return\n",
        "\n",
        "    # Scale Fix\n",
        "    if df[\"nldd_mean\"].abs().max() < 1.5:\n",
        "        df[\"nldd_mean\"] *= 100\n",
        "        for col in [\"nldd_sem\", \"nldd_err\"]:\n",
        "            if col in df.columns: df[col] *= 100\n",
        "\n",
        "    fig, ax1 = plt.subplots()\n",
        "    ax2 = ax1.twinx()\n",
        "    steps = df[\"step_k\"]\n",
        "\n",
        "    # 1. NLDD (Faithfulness) -> Magenta\n",
        "    color_nldd = COLOR_PALETTE[0]\n",
        "    ax1.plot(steps, df[\"nldd_mean\"], marker='s', label='NLDD',\n",
        "             color=color_nldd, clip_on=False)\n",
        "\n",
        "    err_col = 'nldd_err' if 'nldd_err' in df.columns else 'nldd_sem'\n",
        "    if err_col in df.columns:\n",
        "        ax1.fill_between(steps, df[\"nldd_mean\"] - df[err_col],\n",
        "                         df[\"nldd_mean\"] + df[err_col], color=color_nldd, alpha=0.15)\n",
        "\n",
        "    # 2. RSA (Alignment) -> Blue\n",
        "    color_rsa = COLOR_PALETTE[1]\n",
        "    ax2.plot(steps, df[\"rsa_mean\"], marker='^', label='RSA',\n",
        "             color=color_rsa, clip_on=False)\n",
        "\n",
        "    rsa_err = 'rsa_err' if 'rsa_err' in df.columns else 'rsa_sem'\n",
        "    if rsa_err in df.columns:\n",
        "        ax2.fill_between(steps, df[\"rsa_mean\"] - df[rsa_err],\n",
        "                         df[\"rsa_mean\"] + df[rsa_err], color=color_rsa, alpha=0.15)\n",
        "\n",
        "    # 3. TAS (Efficiency) -> Orange\n",
        "    if \"tas_mean\" in df.columns:\n",
        "        color_tas = COLOR_PALETTE[2]\n",
        "        ax2.plot(steps, df[\"tas_mean\"], marker='o', label='TAS',\n",
        "                 color=color_tas, clip_on=False)\n",
        "\n",
        "    # Labels\n",
        "    ax1.set_xlabel(\"Reasoning Depth ($k$)\")\n",
        "    ax1.set_ylabel(r\"NLDD (\\%)\" if LATEX_INSTALLED else \"NLDD (%)\", color=color_nldd)\n",
        "    ax1.tick_params(axis='y', labelcolor=color_nldd)\n",
        "    ax1.grid(True)\n",
        "\n",
        "    ax2.set_ylabel(\"RSA / TAS\")\n",
        "    ax2.tick_params(axis='y')\n",
        "    ax2.set_ylim(0.0, 1.05)\n",
        "\n",
        "    # Horizon Line\n",
        "    horizon_idx = df.iloc[1:][\"nldd_mean\"].idxmax() if len(df) > 1 else df[\"nldd_mean\"].idxmax()\n",
        "    horizon_step = steps[horizon_idx]\n",
        "\n",
        "    if pd.notna(horizon_step):\n",
        "        ax1.axvline(x=horizon_step, color='black', linestyle=':', linewidth=2.0, alpha=0.9)\n",
        "        ax1.text(horizon_step + 0.15, ax1.get_ylim()[1] * 0.95, f'$k^*={int(horizon_step)}$',\n",
        "                 ha='left', va='top', fontsize=9, fontweight='bold')\n",
        "\n",
        "    # Legend\n",
        "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    # Fixed: Moved legend above plot to avoid covering NLDD spikes\n",
        "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='lower center',\n",
        "               bbox_to_anchor=(0.5, 1.05), ncol=3, frameon=False)\n",
        "\n",
        "    ax1.set_xlim(df[\"step_k\"].min(), df[\"step_k\"].max())\n",
        "\n",
        "    save_path = f\"./results_acl_2026/fig1_horizon_{task_name.lower()}.pdf\"\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"âœ“ Saved plot: {save_path}\")\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# --- 3. GRAPH 2: Accuracy Comparison ---\n",
        "def plot_accuracy_comparison_acl(all_results):\n",
        "    tasks = [\"Dyck-n\", \"ProntoQA\", \"GSM8K\"]\n",
        "    clean_accs, corrupt_accs, clean_cis, corrupt_cis = [], [], [], []\n",
        "    valid_tasks = []\n",
        "\n",
        "    for task in tasks:\n",
        "        if task in all_results and all_results[task].get(\"cf_accuracy_df\") is not None:\n",
        "            df = all_results[task][\"cf_accuracy_df\"]\n",
        "            cdf = df[df[\"type\"] == \"corruption\"]\n",
        "            if len(cdf) == 0: continue\n",
        "\n",
        "            valid_tasks.append(task)\n",
        "            n = len(cdf)\n",
        "            c_acc = cdf[\"clean_correct\"].mean() * 100\n",
        "            corr_acc = cdf[\"corrupt_correct\"].mean() * 100\n",
        "            clean_accs.append(c_acc)\n",
        "            corrupt_accs.append(corr_acc)\n",
        "\n",
        "            t_crit = stats.t.ppf(0.975, n-1)\n",
        "            c_se = np.sqrt((c_acc/100)*(1-c_acc/100)/n) * 100\n",
        "            corr_se = np.sqrt((corr_acc/100)*(1-corr_acc/100)/n) * 100\n",
        "            clean_cis.append(t_crit * c_se)\n",
        "            corrupt_cis.append(t_crit * corr_se)\n",
        "\n",
        "    if not valid_tasks: return\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    x = np.arange(len(valid_tasks))\n",
        "    width = 0.35\n",
        "\n",
        "    # Uses Orange (Clean) vs Blue (Corrupt)\n",
        "    rects1 = ax.bar(x - width/2, clean_accs, width, yerr=clean_cis, capsize=3,\n",
        "                    label='Clean', color=COLOR_PALETTE[2], edgecolor='black', linewidth=0.5)\n",
        "    rects2 = ax.bar(x + width/2, corrupt_accs, width, yerr=corrupt_cis, capsize=3,\n",
        "                    label='Corrupt', color=COLOR_PALETTE[1], edgecolor='black', linewidth=0.5)\n",
        "\n",
        "    ax.set_ylabel(r'Accuracy (\\%)' if LATEX_INSTALLED else 'Accuracy (%)')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(valid_tasks)\n",
        "    ax.set_ylim(0, 115) # Increased Y-limit slightly for top labels\n",
        "\n",
        "    # FIXED LEGEND: Moved above the plot\n",
        "    ax.legend(loc='lower center', bbox_to_anchor=(0.5, 1.05),\n",
        "              ncol=2, frameon=False)\n",
        "\n",
        "    ax.bar_label(rects1, fmt='%.0f', padding=2, fontsize=8)\n",
        "    ax.bar_label(rects2, fmt='%.0f', padding=2, fontsize=8)\n",
        "\n",
        "    sns.despine()\n",
        "    ax.yaxis.grid(True)\n",
        "\n",
        "    save_path = './results_acl_2026/accuracy_comparison.pdf'\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"âœ“ Saved plot: {save_path}\")\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# --- 4. GRAPH 3: Probability Delta ---\n",
        "def plot_probability_delta_acl(all_results):\n",
        "    tasks = [\"Dyck-n\", \"ProntoQA\", \"GSM8K\"]\n",
        "    combined_df = []\n",
        "\n",
        "    for task in tasks:\n",
        "        if task in all_results and all_results[task].get(\"cf_accuracy_df\") is not None:\n",
        "            df = all_results[task][\"cf_accuracy_df\"]\n",
        "            cdf = df[df[\"type\"] == \"corruption\"].copy()\n",
        "            if len(cdf) > 0:\n",
        "                cdf[\"task\"] = task\n",
        "                combined_df.append(cdf[[\"task\", \"prob_delta\"]])\n",
        "\n",
        "    if combined_df:\n",
        "        combined_df = pd.concat(combined_df, ignore_index=True)\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        # Violin uses Blue, Orange, Magenta for distinct contrast\n",
        "        sns.violinplot(data=combined_df, x=\"task\", y=\"prob_delta\", inner=\"quartile\",\n",
        "                       palette=[COLOR_PALETTE[1], COLOR_PALETTE[2], COLOR_PALETTE[0]],\n",
        "                       linewidth=1.0, cut=0, ax=ax)\n",
        "\n",
        "        ax.axhline(0, color='black', linestyle=':', linewidth=1.0, alpha=0.8)\n",
        "        ax.set_ylabel(r'$\\Delta$ Probability' if LATEX_INSTALLED else 'Delta Probability')\n",
        "        ax.set_xlabel('')\n",
        "\n",
        "        sns.despine()\n",
        "        ax.yaxis.grid(True)\n",
        "\n",
        "        save_path = './results_acl_2026/prob_delta_distribution.pdf'\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"âœ“ Saved plot: {save_path}\")\n",
        "        plt.show()\n",
        "        plt.close(fig)\n",
        "    else:\n",
        "        print(\"âš ï¸ No data for probability delta.\")"
      ],
      "metadata": {
        "id": "Vq4UVPybUw9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "visual 2"
      ],
      "metadata": {
        "id": "pRvy7pn2VJWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 10: ACL 2026 VISUALIZATION SUITE (FINAL: FIXED GSM8K ERROR BAR)\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import os\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. GLOBAL ACL 2026 CONFIGURATION ---\n",
        "COLOR_PALETTE = [\"#DC267F\", \"#648FFF\", \"#FE6100\", \"#785EF0\"]\n",
        "\n",
        "def is_latex_installed():\n",
        "    try:\n",
        "        return os.system(\"which latex > /dev/null 2>&1\") == 0\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "LATEX_INSTALLED = is_latex_installed()\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [\"Times New Roman\", \"Times\", \"DejaVu Serif\"],\n",
        "    \"text.usetex\": LATEX_INSTALLED,\n",
        "    \"mathtext.fontset\": \"stix\",\n",
        "    \"font.size\": 9,\n",
        "    \"axes.labelsize\": 9,\n",
        "    \"axes.titlesize\": 10,\n",
        "    \"xtick.labelsize\": 8,\n",
        "    \"ytick.labelsize\": 8,\n",
        "    \"legend.fontsize\": 8,\n",
        "    \"figure.figsize\": (3.33, 2.5),\n",
        "    \"savefig.dpi\": 300,\n",
        "    \"savefig.bbox\": \"tight\",\n",
        "    \"savefig.format\": \"pdf\",\n",
        "    \"axes.prop_cycle\": mpl.cycler(color=COLOR_PALETTE),\n",
        "    \"lines.linewidth\": 1.5,\n",
        "    \"lines.markersize\": 3.5,\n",
        "    \"grid.alpha\": 0.3,\n",
        "    \"grid.linestyle\": \":\",\n",
        "})\n",
        "\n",
        "# --- 2. GRAPH 1: Reasoning Horizon ---\n",
        "def plot_reasoning_horizon_acl(csv_path, task_name, config):\n",
        "    if not os.path.exists(csv_path): return\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if df.empty: return\n",
        "\n",
        "    fig, ax1 = plt.subplots()\n",
        "    ax2 = ax1.twinx()\n",
        "    steps = df[\"step_k\"]\n",
        "\n",
        "    # 1. Plot NLDD (Left Axis)\n",
        "    color_nldd = COLOR_PALETTE[0]\n",
        "    ax1.plot(steps, df[\"nldd_mean\"], marker='s', label='NLDD', color=color_nldd)\n",
        "    err_col = 'nldd_err' if 'nldd_err' in df.columns else 'nldd_sem'\n",
        "    if err_col in df.columns:\n",
        "        ax1.fill_between(steps, df[\"nldd_mean\"] - df[err_col],\n",
        "                         df[\"nldd_mean\"] + df[err_col], color=color_nldd, alpha=0.15)\n",
        "\n",
        "    # 2. Plot RSA & TAS (Right Axis)\n",
        "    ax2.plot(steps, df[\"rsa_mean\"], marker='^', label='RSA', color=COLOR_PALETTE[1])\n",
        "    if \"tas_mean\" in df.columns:\n",
        "        ax2.plot(steps, df[\"tas_mean\"], marker='o', label='TAS', color=COLOR_PALETTE[2])\n",
        "\n",
        "    # --- DYNAMIC RANGE LOGIC FOR AX1 (NLDD) ---\n",
        "    n_min = (df[\"nldd_mean\"] - (df[err_col] if err_col in df.columns else 0)).min()\n",
        "    n_max = (df[\"nldd_mean\"] + (df[err_col] if err_col in df.columns else 0)).max()\n",
        "    n_range = n_max - n_min\n",
        "    # Pad 15% bottom, 45% top for legend\n",
        "    ax1.set_ylim(n_min - (0.15 * n_range), n_max + (0.45 * n_range))\n",
        "\n",
        "    # --- DYNAMIC RANGE LOGIC FOR AX2 (RSA/TAS) ---\n",
        "    # Find the min/max across both RSA and TAS columns\n",
        "    rsa_tas_cols = [\"rsa_mean\"] + ([\"tas_mean\"] if \"tas_mean\" in df.columns else [])\n",
        "    rt_min = df[rsa_tas_cols].min().min()\n",
        "    rt_max = df[rsa_tas_cols].max().max()\n",
        "    rt_range = max(rt_max - rt_min, 0.1) # Avoid division by zero if flat\n",
        "    # Pad 10% bottom and top for RSA\n",
        "    ax2.set_ylim(rt_min - (0.1 * rt_range), rt_max + (0.1 * rt_range))\n",
        "\n",
        "    # --- FORMATTING ---\n",
        "    ax1.set_xlabel(\"Reasoning Depth ($k$)\")\n",
        "    ax1.set_ylabel(\"NLDD (%)\", color=color_nldd)\n",
        "    ax1.tick_params(axis='y', labelcolor=color_nldd)\n",
        "    ax2.set_ylabel(\"RSA / TAS\")\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Horizon Line\n",
        "    horizon_idx = df.iloc[1:][\"nldd_mean\"].idxmax() if len(df) > 1 else df[\"nldd_mean\"].idxmax()\n",
        "    h_step = steps[horizon_idx]\n",
        "    ax1.axvline(x=h_step, color='black', linestyle=':', linewidth=1.5, alpha=0.7)\n",
        "    ax1.text(h_step + 0.1, ax1.get_ylim()[1] * 0.75, f'$k^*={int(h_step)}$', fontweight='bold')\n",
        "\n",
        "    # Legend\n",
        "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='lower center',\n",
        "               bbox_to_anchor=(0.5, 1.15), ncol=3, frameon=False)\n",
        "\n",
        "    plt.savefig(f\"./results_acl_2026/fig1_horizon_{task_name.lower()}.pdf\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# --- 3. GRAPH 2: Accuracy Comparison (FIXED Y-LIMIT) ---\n",
        "def plot_accuracy_comparison_acl(all_results):\n",
        "    tasks = [\"Dyck-n\", \"ProntoQA\", \"GSM8K\"]\n",
        "    valid_tasks, clean_accs, corrupt_accs = [], [], []\n",
        "    clean_cis, corrupt_cis = [], []\n",
        "\n",
        "    for t in tasks:\n",
        "        if t in all_results and \"cf_accuracy_df\" in all_results[t]:\n",
        "            df = all_results[t][\"cf_accuracy_df\"]\n",
        "            cdf = df[df[\"type\"] == \"corruption\"]\n",
        "            if cdf.empty: continue\n",
        "            valid_tasks.append(t)\n",
        "            n = len(cdf)\n",
        "            c_a, cr_a = cdf[\"clean_correct\"].mean()*100, cdf[\"corrupt_correct\"].mean()*100\n",
        "            clean_accs.append(c_a)\n",
        "            corrupt_accs.append(cr_a)\n",
        "            # CI calculation\n",
        "            t_crit = stats.t.ppf(0.975, n-1)\n",
        "            clean_cis.append(t_crit * np.sqrt((c_a/100)*(1-c_a/100)/n)*100)\n",
        "            corrupt_cis.append(t_crit * np.sqrt((cr_a/100)*(1-cr_a/100)/n)*100)\n",
        "\n",
        "    if not valid_tasks: return\n",
        "    fig, ax = plt.subplots()\n",
        "    x = np.arange(len(valid_tasks))\n",
        "    width = 0.35\n",
        "\n",
        "    r1 = ax.bar(x - width/2, clean_accs, width, yerr=clean_cis, label='Clean', color=COLOR_PALETTE[2], edgecolor='black', linewidth=0.5)\n",
        "    r2 = ax.bar(x + width/2, corrupt_accs, width, yerr=corrupt_cis, label='Corrupt', color=COLOR_PALETTE[1], edgecolor='black', linewidth=0.5)\n",
        "\n",
        "    # --- FIX: Dynamic Y-Axis Limit ---\n",
        "    # Calculate the maximum height needed for bars + error bars\n",
        "    max_height = 0\n",
        "    if clean_accs and clean_cis:\n",
        "        max_height = max(max_height, np.max(np.array(clean_accs) + np.array(clean_cis)))\n",
        "    if corrupt_accs and corrupt_cis:\n",
        "        max_height = max(max_height, np.max(np.array(corrupt_accs) + np.array(corrupt_cis)))\n",
        "\n",
        "    # Add 20% padding for labels and legend headroom\n",
        "    # Ensure limit is at least 100 if max_height is small\n",
        "    ax.set_ylim(0, max(100, max_height) * 1.2)\n",
        "    # ---------------------------------\n",
        "\n",
        "    ax.set_ylabel('Accuracy (%)')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(valid_tasks)\n",
        "\n",
        "    ax.legend(loc='lower center', bbox_to_anchor=(0.5, 1.05), ncol=2, frameon=False)\n",
        "    ax.bar_label(r1, fmt='%.0f', padding=2); ax.bar_label(r2, fmt='%.0f', padding=2)\n",
        "\n",
        "    sns.despine(); ax.yaxis.grid(True)\n",
        "    plt.savefig('./results_acl_2026/accuracy_comparison.pdf')\n",
        "    plt.show()\n",
        "\n",
        "# --- 4. GRAPH 3: Probability Delta ---\n",
        "def plot_probability_delta_acl(all_results):\n",
        "    combined = []\n",
        "    for t in [\"Dyck-n\", \"ProntoQA\", \"GSM8K\"]:\n",
        "        if t in all_results and \"cf_accuracy_df\" in all_results[t]:\n",
        "            df = all_results[t][\"cf_accuracy_df\"]\n",
        "            cdf = df[df[\"type\"] == \"corruption\"].copy()\n",
        "            if not cdf.empty:\n",
        "                cdf[\"task\"] = t\n",
        "                combined.append(cdf[[\"task\", \"prob_delta\"]])\n",
        "\n",
        "    if not combined: return\n",
        "    df_plot = pd.concat(combined)\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.violinplot(data=df_plot, x=\"task\", y=\"prob_delta\", inner=\"quartile\",\n",
        "                   palette=[COLOR_PALETTE[1], COLOR_PALETTE[2], COLOR_PALETTE[0]], ax=ax)\n",
        "    ax.axhline(0, color='black', linestyle=':', alpha=0.5)\n",
        "    ax.set_ylabel('$\\Delta$ Probability')\n",
        "    sns.despine(); ax.yaxis.grid(True)\n",
        "    plt.savefig('./results_acl_2026/prob_delta_distribution.pdf')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "T1E40DAMVQYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EXECUTE ACL VISUALIZATIONS\n",
        "# ============================================================================\n",
        "\n",
        "# 1. Horizon Plots\n",
        "print(\"Generating Horizon Plots...\")\n",
        "plot_reasoning_horizon_acl(\"./results_acl_2026/horizon_dyck.csv\", \"Dyck-n\", config)\n",
        "plot_reasoning_horizon_acl(\"./results_acl_2026/horizon_pronto_qa.csv\", \"ProntoQA\", config)\n",
        "plot_reasoning_horizon_acl(\"./results_acl_2026/horizon_gsm8k.csv\", \"GSM8K\", config)\n",
        "\n",
        "# 2. Accuracy Comparison\n",
        "print(\"Generating Accuracy Plots...\")\n",
        "plot_accuracy_comparison_acl(all_results)\n",
        "\n",
        "# 3. Probability Delta\n",
        "print(\"Generating Probability Delta Plots...\")\n",
        "plot_probability_delta_acl(all_results)"
      ],
      "metadata": {
        "id": "v84Ozw9v7LwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================================\n",
        "# ACL Robustness Analyzer (Statistical Significance Module)\n",
        "# ===========================================================================\n",
        "class ACLRobustnessAnalyzer:\n",
        "    \"\"\"\n",
        "    Generates the 'Full Stats Report' required for ACL submission.\n",
        "    Calculates statistical significance of the faithfulness decay (NLDD)\n",
        "    and the alignment drop (RSA) at the Reasoning Horizon (k*).\n",
        "    \"\"\"\n",
        "    def __init__(self, df: pd.DataFrame, task_name: str):\n",
        "        self.df = df\n",
        "        self.task_name = task_name\n",
        "\n",
        "    def generate_full_report(self, horizon_k: int) -> Dict:\n",
        "        \"\"\"\n",
        "        Compares metrics at step k* (Horizon) vs step 0 (Baseline).\n",
        "        \"\"\"\n",
        "        print(f\"\\n[Robustness Report] {self.task_name} (Horizon k*={horizon_k})\")\n",
        "\n",
        "        # 1. Get Data at Baseline (k=0 or k=1) and Horizon (k=k*)\n",
        "        # Adjust baseline k based on available steps (usually 1 for 1-indexed steps)\n",
        "        baseline_k = self.df['step_k'].min()\n",
        "\n",
        "        row_base = self.df[self.df['step_k'] == baseline_k].iloc[0]\n",
        "        row_horz = self.df[self.df['step_k'] == horizon_k].iloc[0]\n",
        "\n",
        "        stats_report = {}\n",
        "\n",
        "        # 2. Analyze Metrics (NLDD, RSA, TAS)\n",
        "        for metric, label in [('nldd', 'Faithfulness'), ('rsa', 'Alignment'), ('tas', 'Efficiency')]:\n",
        "            if f'{metric}_mean' not in self.df.columns: continue\n",
        "\n",
        "            val_base = row_base[f'{metric}_mean']\n",
        "            err_base = row_base.get(f'{metric}_sem', 0)\n",
        "            val_horz = row_horz[f'{metric}_mean']\n",
        "            err_horz = row_horz.get(f'{metric}_sem', 0)\n",
        "\n",
        "            # T-test approximation using SEM (assuming N provided in df or default 20)\n",
        "            n_samples = self.df.get('n_samples', pd.Series([20]*len(self.df))).iloc[0]\n",
        "            se_diff = np.sqrt(err_base**2 + err_horz**2)\n",
        "            t_stat = abs(val_base - val_horz) / (se_diff + 1e-9)\n",
        "\n",
        "            # Degrees of freedom approx (Welch-Satterthwaite would be better but N is equal)\n",
        "            df_stat = 2 * n_samples - 2\n",
        "            p_val = stats.t.sf(t_stat, df_stat) * 2  # two-sided\n",
        "\n",
        "            # Effect Size (Cohen's d approx)\n",
        "            # We treat SEM * sqrt(N) as SD\n",
        "            sd_pooled = np.sqrt(((err_base * np.sqrt(n_samples))**2 + (err_horz * np.sqrt(n_samples))**2) / 2)\n",
        "            cohens_d = abs(val_base - val_horz) / (sd_pooled + 1e-9)\n",
        "\n",
        "            stats_report[metric] = {\n",
        "                'baseline': val_base,\n",
        "                'horizon': val_horz,\n",
        "                'delta': val_horz - val_base,\n",
        "                'p_value': p_val,\n",
        "                'cohens_d': cohens_d,\n",
        "                'significant': p_val < 0.05\n",
        "            }\n",
        "\n",
        "            sig_mark = \"*\" if p_val < 0.05 else \"ns\"\n",
        "            if p_val < 0.001: sig_mark = \"***\"\n",
        "            elif p_val < 0.01: sig_mark = \"**\"\n",
        "\n",
        "            print(f\"   > {label}: {val_base:.1f} -> {val_horz:.1f} (d={cohens_d:.2f}, p={p_val:.1e} {sig_mark})\")\n",
        "\n",
        "        return stats_report"
      ],
      "metadata": {
        "id": "g0jukZAeGDp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# RUN ROBUSTNESS ANALYSIS ON ALL TASKS\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RUNNING ROBUSTNESS ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load horizon CSVs and run analysis\n",
        "tasks_info = {\n",
        "    \"Dyck-n\": \"./results_acl_2026/horizon_dyck.csv\",\n",
        "    \"ProntoQA\": \"./results_acl_2026/horizon_pronto_qa.csv\",\n",
        "    \"GSM8K\": \"./results_acl_2026/horizon_gsm8k.csv\"\n",
        "}\n",
        "\n",
        "robustness_results = {}\n",
        "\n",
        "for task_name, csv_path in tasks_info.items():\n",
        "    print(f\"\\n{'#'*60}\")\n",
        "    print(f\"# {task_name}\")\n",
        "    print(f\"{'#'*60}\")\n",
        "\n",
        "    # Load the horizon CSV\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Fix NLDD scale if needed\n",
        "    if df[\"nldd_mean\"].abs().max() < 2.0:\n",
        "        print(f\"  Converting NLDD to percentage scale...\")\n",
        "        cols = [c for c in df.columns if 'nldd' in c]\n",
        "        df[cols] *= 100\n",
        "\n",
        "    # Add n_samples if not present\n",
        "    if 'n_samples' not in df.columns:\n",
        "        df['n_samples'] = 20  # Your current N\n",
        "\n",
        "    # Calculate confidence intervals for the df\n",
        "    from scipy import stats\n",
        "    n = df['n_samples'].iloc[0]\n",
        "    t_crit = stats.t.ppf(0.975, df=n-1)\n",
        "\n",
        "    for m in ['nldd', 'rsa', 'tas']:\n",
        "        mean_col = f'{m}_mean'\n",
        "        sem_col = f'{m}_sem'\n",
        "\n",
        "        if mean_col in df.columns and sem_col in df.columns:\n",
        "            df[f'{m}_ci_lower'] = df[mean_col] - (t_crit * df[sem_col])\n",
        "            df[f'{m}_ci_upper'] = df[mean_col] + (t_crit * df[sem_col])\n",
        "\n",
        "    # Detect horizon (steepest drop)\n",
        "    nldd_diff = df[\"nldd_mean\"].diff().fillna(0)\n",
        "    horizon_idx = nldd_diff.idxmin()\n",
        "    horizon_k = int(df[\"step_k\"].iloc[horizon_idx])\n",
        "\n",
        "    print(f\"\\n  Detected horizon: k = {horizon_k}\")\n",
        "\n",
        "    # Create analyzer\n",
        "    analyzer = ACLRobustnessAnalyzer(df, task_name)\n",
        "\n",
        "    # Run full analysis\n",
        "    results = analyzer.generate_full_report(horizon_k)\n",
        "\n",
        "    # Store results\n",
        "    robustness_results[task_name] = {\n",
        "        'horizon': horizon_k,\n",
        "        'robustness': results\n",
        "    }\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… ROBUSTNESS ANALYSIS COMPLETE\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "-WmpkCfJwCQD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a8db5649d9fc415bab56532c364f10f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31cf858b21ea48d8887f6b365074e51a",
              "IPY_MODEL_8f4bd9504d214a2d877c907cab125efd",
              "IPY_MODEL_d7db9a9f3107410fa4fb3442b97d6fda"
            ],
            "layout": "IPY_MODEL_bd3ab0db27f841238e6a1e1e42a7fb2f"
          }
        },
        "31cf858b21ea48d8887f6b365074e51a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f898d7c2adba435a9a57e9a35ddcf2f1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_86b14207de8f40f6828291a3531ce3d6",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "8f4bd9504d214a2d877c907cab125efd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d9247211d7d473eb0a6ac92c90097f2",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5d1d3c1e35a4235b44ce7965c9cdcfa",
            "value": 4
          }
        },
        "d7db9a9f3107410fa4fb3442b97d6fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b97861049f894130a0f3d80920e469bd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d8aa232c31324cac92833f512dd158e6",
            "value": "â€‡4/4â€‡[00:05&lt;00:00,â€‡â€‡1.15s/it]"
          }
        },
        "bd3ab0db27f841238e6a1e1e42a7fb2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f898d7c2adba435a9a57e9a35ddcf2f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86b14207de8f40f6828291a3531ce3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d9247211d7d473eb0a6ac92c90097f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5d1d3c1e35a4235b44ce7965c9cdcfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b97861049f894130a0f3d80920e469bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8aa232c31324cac92833f512dd158e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78993bcc27c247f7a68ae2b5b0e28e26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a34aa5fb6819477f885af637e730e419",
              "IPY_MODEL_d41a5ca4c2274167b607ff76a3f8f5c1",
              "IPY_MODEL_b12a632879564bca901c1be0810cd689"
            ],
            "layout": "IPY_MODEL_be3731ce6d584c1390e32408d23069d3"
          }
        },
        "a34aa5fb6819477f885af637e730e419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_122a067f2f604aab881fedcc2905f3c0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d32e28a21a324eb8b6366cf5e4cd3518",
            "value": "Buildingâ€‡DYCK:â€‡100%"
          }
        },
        "d41a5ca4c2274167b607ff76a3f8f5c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd3073c0570c4c7eaa9ce203e181ca61",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61eab2b02739437090846562c9ae4aea",
            "value": 300
          }
        },
        "b12a632879564bca901c1be0810cd689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4402281476324e02812ffaa235e1688a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d95c3fa291724c3d87194f01604e2776",
            "value": "â€‡300/300â€‡[02:26&lt;00:00,â€‡â€‡2.14it/s]"
          }
        },
        "be3731ce6d584c1390e32408d23069d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "122a067f2f604aab881fedcc2905f3c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d32e28a21a324eb8b6366cf5e4cd3518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd3073c0570c4c7eaa9ce203e181ca61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61eab2b02739437090846562c9ae4aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4402281476324e02812ffaa235e1688a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d95c3fa291724c3d87194f01604e2776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fedcd960b3f45f9b35149417f5ff039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc420e92d0474446adf588448cf88321",
              "IPY_MODEL_c051664af01e466b94ea1f3b362648b0",
              "IPY_MODEL_6b08b25bee7d447bb9b9ac0c36f2518c"
            ],
            "layout": "IPY_MODEL_7b71f096e1e641abbac411558e1a0ad4"
          }
        },
        "fc420e92d0474446adf588448cf88321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae3cd4851b47480dbd7eea27f7885756",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_49a1486b01e341c3989ad0bb1bfe1dfa",
            "value": "Buildingâ€‡PRONTO_QA:â€‡100%"
          }
        },
        "c051664af01e466b94ea1f3b362648b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_259f4678ed7840c2b80465e26d67897b",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7137582a93fc4d0b92428a45e3981e15",
            "value": 300
          }
        },
        "6b08b25bee7d447bb9b9ac0c36f2518c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_228e9583da7d4caaa55d27319100e6e5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4c3790ccc41545c6bd248a268f2c26f8",
            "value": "â€‡300/300â€‡[02:19&lt;00:00,â€‡â€‡2.10it/s]"
          }
        },
        "7b71f096e1e641abbac411558e1a0ad4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae3cd4851b47480dbd7eea27f7885756": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49a1486b01e341c3989ad0bb1bfe1dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "259f4678ed7840c2b80465e26d67897b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7137582a93fc4d0b92428a45e3981e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "228e9583da7d4caaa55d27319100e6e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c3790ccc41545c6bd248a268f2c26f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dc211077c214dcf87996c87c062ca33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1bf1a34ac1d4de2a19c5006bb876125",
              "IPY_MODEL_5f9c18901ec4497dbec02c81b4d91df8",
              "IPY_MODEL_841e1c89b1aa4f97935ac094ff364dfb"
            ],
            "layout": "IPY_MODEL_bd868c38d61c45ffa2ca83ae123ee739"
          }
        },
        "d1bf1a34ac1d4de2a19c5006bb876125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_946ac250d2df4226b336322819c8ea76",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_896b86d5a777490b885de9e8e0178d6e",
            "value": "Buildingâ€‡GSM8K:â€‡100%"
          }
        },
        "5f9c18901ec4497dbec02c81b4d91df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e98f547771448878a48b512c1f4d29d",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a2673bc27b34557b841c7af63be6673",
            "value": 300
          }
        },
        "841e1c89b1aa4f97935ac094ff364dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e55d4f3c2fd41fc8bd345d1bd410b4d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_396ea17f770341b995e353efd9678cde",
            "value": "â€‡300/300â€‡[01:16&lt;00:00,â€‡â€‡3.87it/s]"
          }
        },
        "bd868c38d61c45ffa2ca83ae123ee739": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "946ac250d2df4226b336322819c8ea76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "896b86d5a777490b885de9e8e0178d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e98f547771448878a48b512c1f4d29d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a2673bc27b34557b841c7af63be6673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e55d4f3c2fd41fc8bd345d1bd410b4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "396ea17f770341b995e353efd9678cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d3fdf05e8d14101af07a0bf2dbb83d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0cd3bdb257a4e3c87672092bbd291b9",
              "IPY_MODEL_f83d0a4f264f41d6addf179f1afc3d31",
              "IPY_MODEL_f21bd98c505844159b4bf6c2af2f32bd"
            ],
            "layout": "IPY_MODEL_b8f5e1ca43694567b1b84d9493c82519"
          }
        },
        "b0cd3bdb257a4e3c87672092bbd291b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1875f5de8ab4270a4a32c801bde0e4c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_36b48f315fdd4ead8b65eda88330534a",
            "value": "Analyzing:â€‡100%"
          }
        },
        "f83d0a4f264f41d6addf179f1afc3d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6de24be45424e8c984a04b545dd64a4",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7db53ef0d301446881ce4721a970a51d",
            "value": 300
          }
        },
        "f21bd98c505844159b4bf6c2af2f32bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c613c927fd35429eab093e6f65e49e5f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_256ec2abc47442d3b0ec91e86d89b2ed",
            "value": "â€‡300/300â€‡[01:10&lt;00:00,â€‡â€‡4.21it/s]"
          }
        },
        "b8f5e1ca43694567b1b84d9493c82519": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1875f5de8ab4270a4a32c801bde0e4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36b48f315fdd4ead8b65eda88330534a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6de24be45424e8c984a04b545dd64a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7db53ef0d301446881ce4721a970a51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c613c927fd35429eab093e6f65e49e5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "256ec2abc47442d3b0ec91e86d89b2ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5810080b919848eb8d2fcd2e37b89568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0cc7c841ca7493d8b2c604af8af712c",
              "IPY_MODEL_9e2feb601dc34f58b60df9c04cd8d6c0",
              "IPY_MODEL_f8594ebdf642466983fa139e1f713db0"
            ],
            "layout": "IPY_MODEL_386fc4ec59ce49888e65254ef77b3307"
          }
        },
        "a0cc7c841ca7493d8b2c604af8af712c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8e1a36622f64216b1683b11ac1d1f2f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_69815720c4cd4fdcab15f30f3ce3ea16",
            "value": "Analyzingâ€‡dyck:â€‡100%"
          }
        },
        "9e2feb601dc34f58b60df9c04cd8d6c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04518e31d5824d9d83403e4b3fd34a5c",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8e8645255e645029d13553cc44ca591",
            "value": 300
          }
        },
        "f8594ebdf642466983fa139e1f713db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f593596fc204ce4a622b63e2c245151",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dd2341d8c9a64ada8580a0816ef8fac2",
            "value": "â€‡300/300â€‡[12:08&lt;00:00,â€‡â€‡2.39s/it]"
          }
        },
        "386fc4ec59ce49888e65254ef77b3307": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8e1a36622f64216b1683b11ac1d1f2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69815720c4cd4fdcab15f30f3ce3ea16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04518e31d5824d9d83403e4b3fd34a5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8e8645255e645029d13553cc44ca591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f593596fc204ce4a622b63e2c245151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd2341d8c9a64ada8580a0816ef8fac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdce1e291170447bad9c42edde2125b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd59352c1ad942988d219f631e28078e",
              "IPY_MODEL_58681ce54b764757a88269131f31115c",
              "IPY_MODEL_3ade69ce7ab34ea18a36a2fd6c51cc44"
            ],
            "layout": "IPY_MODEL_85643ab8c9c04f248aa1912045562edf"
          }
        },
        "dd59352c1ad942988d219f631e28078e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75c879e2e46b428d8e656ddbf21bc78e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e42e8075b04945498588eb30fa9ce446",
            "value": "Extractingâ€‡Dyckâ€‡activations:â€‡100%"
          }
        },
        "58681ce54b764757a88269131f31115c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb50ebc92b7f4712bf105ebe43926204",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8e9475db1cc4a6280874f22ed174535",
            "value": 1000
          }
        },
        "3ade69ce7ab34ea18a36a2fd6c51cc44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bd8492a0731402189b04c36a5ecf473",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c17faa73de424b0994ed29ec646c3cff",
            "value": "â€‡999/1000â€‡[01:36&lt;00:00,â€‡10.59it/s]"
          }
        },
        "85643ab8c9c04f248aa1912045562edf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "75c879e2e46b428d8e656ddbf21bc78e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e42e8075b04945498588eb30fa9ce446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb50ebc92b7f4712bf105ebe43926204": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8e9475db1cc4a6280874f22ed174535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bd8492a0731402189b04c36a5ecf473": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c17faa73de424b0994ed29ec646c3cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5eb321764e994af3a1bf2446e642d90a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb44293d2d5c4d3db2460085dfa961da",
              "IPY_MODEL_327ee672b9a84fdea885cd77b5471fae",
              "IPY_MODEL_24e60ce2f142450f92826a964f7f43d1"
            ],
            "layout": "IPY_MODEL_d4d759db0cf942b7bc6daba1af82b0d7"
          }
        },
        "cb44293d2d5c4d3db2460085dfa961da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a52213d68bb4fa09e3dbf3e33753cf9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_69686d3d0e0a4388be47ca54d5f7fa19",
            "value": "Trainingâ€‡stack_depthâ€‡probes:â€‡100%"
          }
        },
        "327ee672b9a84fdea885cd77b5471fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b110bf604a7c45dc9d1ed89147d7721a",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11cbd1e4045149d998270e0d3576a4b9",
            "value": 32
          }
        },
        "24e60ce2f142450f92826a964f7f43d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c7f819189e24b83a3d84927f44a5ebc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_af6627cb102b47238277662db474344e",
            "value": "â€‡32/32â€‡[1:37:30&lt;00:00,â€‡230.04s/it]"
          }
        },
        "d4d759db0cf942b7bc6daba1af82b0d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "6a52213d68bb4fa09e3dbf3e33753cf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69686d3d0e0a4388be47ca54d5f7fa19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b110bf604a7c45dc9d1ed89147d7721a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11cbd1e4045149d998270e0d3576a4b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c7f819189e24b83a3d84927f44a5ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af6627cb102b47238277662db474344e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f22d7fa7b2c644c799086affcf97da27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4627ca8b302b49499ebacac98042166d",
              "IPY_MODEL_40a777d7e5b94c3fa26ca060329939a8",
              "IPY_MODEL_ee5210af38ff4ca0886147c6568f1dde"
            ],
            "layout": "IPY_MODEL_ff588d52b76b4e658e5830278dc35478"
          }
        },
        "4627ca8b302b49499ebacac98042166d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f51169e564e4350a12998c082092baf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d6b598226bee4625bb129cddc8d98972",
            "value": "RSA:â€‡Extractingâ€‡activations:â€‡100%"
          }
        },
        "40a777d7e5b94c3fa26ca060329939a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b134130f01eb48e6b432ed0d977ea2b4",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc344002342b490abc5032b6fd0579f4",
            "value": 300
          }
        },
        "ee5210af38ff4ca0886147c6568f1dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a83e6bfd4f042fb91125ba3e173f9e6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a17d163b2e7f4a68a50afc3681702155",
            "value": "â€‡300/300â€‡[00:53&lt;00:00,â€‡â€‡5.84it/s]"
          }
        },
        "ff588d52b76b4e658e5830278dc35478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "7f51169e564e4350a12998c082092baf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6b598226bee4625bb129cddc8d98972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b134130f01eb48e6b432ed0d977ea2b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc344002342b490abc5032b6fd0579f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a83e6bfd4f042fb91125ba3e173f9e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a17d163b2e7f4a68a50afc3681702155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa23e15646834a788650c9609b7cb8b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7daa78097f60441ca352c87df689d21c",
              "IPY_MODEL_8262c9877c02429dbb4975621cdeb794",
              "IPY_MODEL_6f1072707ff74311b2c9d9ee92691716"
            ],
            "layout": "IPY_MODEL_34f23b1264ba42a4b6b34961d6cd593a"
          }
        },
        "7daa78097f60441ca352c87df689d21c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f91883b99d146ba82ae1d45733c8879",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a307349925d54a0e97851cdaead5b4d1",
            "value": "Geometryâ€‡Analysis:â€‡100%"
          }
        },
        "8262c9877c02429dbb4975621cdeb794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c58d537dbe8340bc9a87ed6a5f3aa946",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25828c306b2a4c4db773b55785523770",
            "value": 50
          }
        },
        "6f1072707ff74311b2c9d9ee92691716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_497dd1b547db446186d39e861fe61b74",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ff8ae0c7001f45d485528bde37dbe380",
            "value": "â€‡50/50â€‡[00:02&lt;00:00,â€‡23.18it/s]"
          }
        },
        "34f23b1264ba42a4b6b34961d6cd593a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "6f91883b99d146ba82ae1d45733c8879": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a307349925d54a0e97851cdaead5b4d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c58d537dbe8340bc9a87ed6a5f3aa946": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25828c306b2a4c4db773b55785523770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "497dd1b547db446186d39e861fe61b74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff8ae0c7001f45d485528bde37dbe380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b9dd013cd5e4d21bf3527fa794e9e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5614efe0815240c091276f620cd44f77",
              "IPY_MODEL_80c2023525164162942feb214f32d951",
              "IPY_MODEL_ae2ab21f23c34c98b0c20b229c7e1103"
            ],
            "layout": "IPY_MODEL_75f5ee5aa9344db283f885ef1652c4f3"
          }
        },
        "5614efe0815240c091276f620cd44f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f6a65e4f79445b8974868f6723c4e01",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_653ad43b6aee410fba649d2191aa75b5",
            "value": "RSA:â€‡Extractingâ€‡activations:â€‡100%"
          }
        },
        "80c2023525164162942feb214f32d951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4515a8623a6b4ff1905ee7e8369f422e",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96e9c4e13c6a42fb849d6107bb9879bd",
            "value": 300
          }
        },
        "ae2ab21f23c34c98b0c20b229c7e1103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27f573fb24d240a49e99e106eab6f83a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fd16f238bce34c949321685c586e450d",
            "value": "â€‡300/300â€‡[00:53&lt;00:00,â€‡â€‡5.60it/s]"
          }
        },
        "75f5ee5aa9344db283f885ef1652c4f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "1f6a65e4f79445b8974868f6723c4e01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "653ad43b6aee410fba649d2191aa75b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4515a8623a6b4ff1905ee7e8369f422e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96e9c4e13c6a42fb849d6107bb9879bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27f573fb24d240a49e99e106eab6f83a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd16f238bce34c949321685c586e450d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88ff3afb14984fd59ac5458984c06c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ebb0f444b95a4aa19809b68828655f9a",
              "IPY_MODEL_eacc833fae924bbd8fe80d1301293546",
              "IPY_MODEL_157ea26b79eb41a8bdfa0da7c2f88d7e"
            ],
            "layout": "IPY_MODEL_57123a3949bc4f6ab5b3085755279b4b"
          }
        },
        "ebb0f444b95a4aa19809b68828655f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34f83021ac3f4be3b511128405b5448f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_13a4485347974b7dbe7dbb0d74ea08c1",
            "value": "TASâ€‡byâ€‡depth:â€‡100%"
          }
        },
        "eacc833fae924bbd8fe80d1301293546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e848690324649e7bddf77d808e7ed30",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16149e5627e54d6a9f85c5c65d70113d",
            "value": 50
          }
        },
        "157ea26b79eb41a8bdfa0da7c2f88d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13e621df4813411895bb4ba5efc4d8cc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c33633a721874a98987cb05bc87a3e71",
            "value": "â€‡50/50â€‡[00:17&lt;00:00,â€‡â€‡2.49it/s]"
          }
        },
        "57123a3949bc4f6ab5b3085755279b4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "34f83021ac3f4be3b511128405b5448f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13a4485347974b7dbe7dbb0d74ea08c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e848690324649e7bddf77d808e7ed30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16149e5627e54d6a9f85c5c65d70113d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13e621df4813411895bb4ba5efc4d8cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c33633a721874a98987cb05bc87a3e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1348b4f20fb945478a513b7155b01305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88d7420fac584d99a7a512d6240e5d0c",
              "IPY_MODEL_5cd9a59fd2164c4f80132b7079f89acc",
              "IPY_MODEL_5e683a1c71f043cd9de78082e529160c"
            ],
            "layout": "IPY_MODEL_d15b2c3c84134f1b95f3cc39da60a8b6"
          }
        },
        "88d7420fac584d99a7a512d6240e5d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_315a25e5b2dd40999f9f66085607d87a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f54f086c9c444d29a7a2ed9867d91c19",
            "value": "Analyzing:â€‡100%"
          }
        },
        "5cd9a59fd2164c4f80132b7079f89acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a0a30b22ff147708aa449ce1bf7f10b",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48db09dd6ef44793b7a539504cf4dc0d",
            "value": 300
          }
        },
        "5e683a1c71f043cd9de78082e529160c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c134b2ea4d694395aba7ad1868040604",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_be1d7b97faa2461aaa7c06a96f10c8f5",
            "value": "â€‡300/300â€‡[01:16&lt;00:00,â€‡â€‡3.84it/s]"
          }
        },
        "d15b2c3c84134f1b95f3cc39da60a8b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "315a25e5b2dd40999f9f66085607d87a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f54f086c9c444d29a7a2ed9867d91c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a0a30b22ff147708aa449ce1bf7f10b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48db09dd6ef44793b7a539504cf4dc0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c134b2ea4d694395aba7ad1868040604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be1d7b97faa2461aaa7c06a96f10c8f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3910e0c7d6234bffadd21c77b032b561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba8c65e48f464fec961d44c0544e99b8",
              "IPY_MODEL_76d49bb9f0f44933ab83669b0e6f5f43",
              "IPY_MODEL_685a652c32bd480eaa267a0c1b659c0b"
            ],
            "layout": "IPY_MODEL_a4ef47504b854d519a924e6802e34ef6"
          }
        },
        "ba8c65e48f464fec961d44c0544e99b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56878df87fd446b58b68e9e5b8ca2fc6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1a58cf11fbf64cbdae5bdfcfaa35c95b",
            "value": "Analyzingâ€‡pronto_qa:â€‡100%"
          }
        },
        "76d49bb9f0f44933ab83669b0e6f5f43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dad05d9fa01b4b57a9b55379e6ed567c",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30762cdffca14c7183a322802ff3ae4b",
            "value": 300
          }
        },
        "685a652c32bd480eaa267a0c1b659c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8f94d6a4a604243b1f0a44fa8dd97e5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e4658be26f50444facc057de2c174e1c",
            "value": "â€‡300/300â€‡[12:41&lt;00:00,â€‡â€‡2.55s/it]"
          }
        },
        "a4ef47504b854d519a924e6802e34ef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56878df87fd446b58b68e9e5b8ca2fc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a58cf11fbf64cbdae5bdfcfaa35c95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dad05d9fa01b4b57a9b55379e6ed567c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30762cdffca14c7183a322802ff3ae4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8f94d6a4a604243b1f0a44fa8dd97e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4658be26f50444facc057de2c174e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "194ae1ce25754c17b79df96428b0d89b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82ded31f85fc4d428a4035377bfbcc95",
              "IPY_MODEL_05b68bcd2a904f91adbc37ddb84100e9",
              "IPY_MODEL_b77595d43b2240a8b9ec99e02067e719"
            ],
            "layout": "IPY_MODEL_ce54cccd399e46ed95be31a65b8f6c9a"
          }
        },
        "82ded31f85fc4d428a4035377bfbcc95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c31d89544def4a718432882106b72c8c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_801a17e33b3d4da8b51e170470d3a67e",
            "value": "ProntoQAâ€‡Probingâ€‡(Fullâ€‡CoT):â€‡â€‡18%"
          }
        },
        "05b68bcd2a904f91adbc37ddb84100e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f6e3f8275bc4e4baf121f88120981fa",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d53b11f88e74a71b605ff2b8984df74",
            "value": 176
          }
        },
        "b77595d43b2240a8b9ec99e02067e719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01997f5b95a8418a98dabc4117d842bb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_35d24577aeb7423fb5380cdef13f8c1a",
            "value": "â€‡176/1000â€‡[00:29&lt;02:33,â€‡â€‡5.36it/s]"
          }
        },
        "ce54cccd399e46ed95be31a65b8f6c9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c31d89544def4a718432882106b72c8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "801a17e33b3d4da8b51e170470d3a67e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f6e3f8275bc4e4baf121f88120981fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d53b11f88e74a71b605ff2b8984df74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01997f5b95a8418a98dabc4117d842bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d24577aeb7423fb5380cdef13f8c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}